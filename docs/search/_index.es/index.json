
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
        
        
        
        
        
        
        
        
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        
            
        
        
            
        [{"categories":[{"LinkTitle":"Code","RelPermalink":"/categories/code/"}],"content":"When I was demonstrating matrix operation acceleration before, I wanted to try AMD\u0026rsquo;s own ROCm. After compiling and running the program, I encountered an error:\nrocBLAS error: Cannot read D:\\example\\efficiency_v3\\rocm\\build\\Release\\/rocblas/library/TensileLibrary.dat: No such file or directory for GPU arch : gfx1150 List of available TensileLibrary Files : According to the official website, ROCm does not support Radeon 880M integrated graphics (AI H 365w processor)1. Unless you compile rocblas yourself, it will not work.\nI checked the tutorial articles online234, but there is no one that is compatible with this integrated graphics, so I have to do it myself.\n1. Download tools and source The tools you need are shown in the table below. If you don’t have them, go to the official website to download and install them.\nNo. Software/Toolkit Version 1 Visual Studio Community 2022 2 AMD HIP SDK for Windows5 6.2.4 3 Python 3.13 4 Git for Windows 2.7.4 5 Strawberry Perl 5.40.0.1 6 CMake 3.30.1 I have vcpkg installed on my machine. Except for the first two which are downloaded and installed from the official website, the rest are extracted from the downloads\\tools directory of vcpkg. Just add the path to the environment variable.\n# Configure toolkit path $Env:HIP_PATH=\u0026#34;C:\\Program Files\\AMD\\ROCm\\6.2\u0026#34; $Env:GIT_BIN_PATH = \u0026#34;D:\\vcpkg\\downloads\\tools\\git-2.7.4-windows\\bin\u0026#34; $Env:PERL_PATH = \u0026#34;D:\\vcpkg\\downloads\\tools\\perl\\5.40.0.1\u0026#34; $Env:CMAKE_BIN_PATH = \u0026#34;D:\\vcpkg\\downloads\\tools\\cmake-3.30.1-windows\\cmake-3.30.1-windows-i386\\bin\u0026#34; # Configure system environment variables $Env:PATH += \u0026#34;;$Env:HIP_PATH\\bin;$Env:GIT_BIN_PATH;$Env:PERL_PATH\\perl\\site\\bin;$Env:PERL_PATH\\perl\\bin;$Env:PERL_PATH\\c\\bin;$Env:CMAKE_BIN_PATH\u0026#34; Download the source code of rocBLAS and Tensile from Github, and make sure the version is consistent with the AMD HIP SDK version installed on your computer. You also need to download the patch file Tensile-fix-fallback-arch-build.patch to ensure that you can compile content for unsupported graphics cards.\n# Create a working directory New-Item -Path D:\\rocblas_build -ItemType Directory # Download rocBLAS git clone -b rocm-6.2.4 https://github.com/ROCm/rocBLAS # Download Tensile git clone -b rocm-6.2.4 https://github.com/ROCm/Tensile # Download Tensile patch Invoke-WebRequest -Uri https://raw.githubusercontent.com/ulyssesrr/docker-rocm-xtra/f25f12835c1d0a5efa80763b5381accf175b200e/rocm-xtra-rocblas-builder/patches/Tensile-fix-fallback-arch-build.patch -OutFile Tensile-fix-fallback-arch-build.patch It is best to package and back up the downloaded source code first to ensure that the changes can be rolled back if an error occurs.\n2. Modify source code Enter the Tensile folder and apply the patch:\n# Copy the patch file to the git folder Copy-Item -Path Tensile-fix-fallback-arch-build.patch -Destination $PWD/Tensile # Enter the Tensile directory cd Tensile # Apply the patch file git apply Tensile-fix-fallback-arch-build.patch The patch application here fails. It seems that the patch version is too old and needs to be modified manually. Below is the modified Tensile-fix-fallback-arch-build.patch file. Save it as and refer to the above command to re-patch.\ndiff --git a/Tensile/TensileCreateLibrary.py b/Tensile/TensileCreateLibrary.py index ac0486d8..d069949c 100644 --- a/Tensile/TensileCreateLibrary.py +++ b/Tensile/TensileCreateLibrary.py @@ -949,12 +949,11 @@ def generateLogicDataAndSolutions(logicFiles, args): for key, value in masterLibraries.items(): if key != \u0026#34;fallback\u0026#34;: value.insert(deepcopy(masterLibraries[\u0026#34;fallback\u0026#34;])) - for archName in archs: - archName = archName.split(\u0026#39;-\u0026#39;, 1)[0] - if archName not in masterLibraries: - print1(\u0026#34;Using fallback for arch: \u0026#34; + archName) - masterLibraries[archName] = deepcopy(masterLibraries[\u0026#34;fallback\u0026#34;]) - masterLibraries[archName].version = args.version + for architectureName in parseArchitecturesFromArgs(args.Architecture, True): + if architectureName not in masterLibraries: + print(\u0026#34;Using fallback for arch: \u0026#34;+architectureName) + masterLibraries[architectureName] = deepcopy(masterLibraries[\u0026#34;fallback\u0026#34;]) + masterLibraries[architectureName].version = args.version masterLibraries.pop(\u0026#34;fallback\u0026#34;) @@ -1027,6 +1027,17 @@ def WriteClientLibraryFromSolutions(solutionList, libraryWorkingPath, tensileSou return (codeObjectFiles, newLibrary) +def parseArchitecturesFromArgs(architectureArgValue, handleLiteralAllAsList): + if architectureArgValue == \u0026#39;all\u0026#39; and handleLiteralAllAsList: + archs = [gfxName(arch) for arch in globalParameters[\u0026#39;SupportedISA\u0026#39;]] + else: + if \u0026#34;;\u0026#34; in architectureArgValue: + archs = architectureArgValue.split(\u0026#34;;\u0026#34;) # user arg list format + else: + archs = architectureArgValue.split(\u0026#34;_\u0026#34;) # workaround for cmake list in list issue + + return archs + ################################################################################ # Write Master Solution Index CSV ################################################################################ @@ -1167,10 +1178,7 @@ def TensileCreateLibrary(): if not os.path.exists(logicPath): printExit(\u0026#34;LogicPath %s doesn\u0026#39;t exist\u0026#34; % logicPath) - if \u0026#34;;\u0026#34; in arguments[\u0026#34;Architecture\u0026#34;]: - archs = arguments[\u0026#34;Architecture\u0026#34;].split(\u0026#34;;\u0026#34;) # user arg list format - else: - archs = arguments[\u0026#34;Architecture\u0026#34;].split(\u0026#34;_\u0026#34;) # workaround for cmake list in list issue + archs = parseArchitecturesFromArgs(arguments[\u0026#34;Architecture\u0026#34;], False) logicArchs = set() for arch in archs: if arch in architectureMap: Next, we need to add the AMD Radeon 880M architecture to the Tensile source code. Use the hipinfo command to view the local graphics architecture. The gfx1150 corresponding to the gcnArchName: below is the architecture name of the graphics card.\n-------------------------------------------------------------------------------- device# 0 Name: AMD Radeon(TM) 880M Graphics ... gcnArchName: gfx1150 ... Then add the graphics card architecture name and parameters to the three files Tensile\\Tensile\\Source\\CMakeLists.txt, Tensile\\AsmCaps.py and Tensile\\Common.py, or apply the following patch:\ndiff --git a/Tensile/AsmCaps.py b/Tensile/AsmCaps.py index 783f9af8..27f29f5a 100644 --- a/Tensile/AsmCaps.py +++ b/Tensile/AsmCaps.py @@ -771,6 +771,50 @@ CACHED_ASM_CAPS = \\ \u0026#39;v_mov_b64\u0026#39;: False, \u0026#39;v_pk_fma_f16\u0026#39;: True, \u0026#39;v_pk_fmac_f16\u0026#39;: False}, + (11, 5, 0): {\u0026#39;HasAddLshl\u0026#39;: True, + \u0026#39;HasAtomicAdd\u0026#39;: True, + \u0026#39;HasDirectToLdsDest\u0026#39;: False, + \u0026#39;HasDirectToLdsNoDest\u0026#39;: False, + \u0026#39;HasExplicitCO\u0026#39;: True, + \u0026#39;HasExplicitNC\u0026#39;: True, + \u0026#39;HasGLCModifier\u0026#39;: True, + \u0026#39;HasNTModifier\u0026#39;: False, + \u0026#39;HasLshlOr\u0026#39;: True, + \u0026#39;HasMFMA\u0026#39;: False, + \u0026#39;HasMFMA_b8\u0026#39;: False, + \u0026#39;HasMFMA_bf16_1k\u0026#39;: False, + \u0026#39;HasMFMA_bf16_original\u0026#39;: False, + \u0026#39;HasMFMA_constSrc\u0026#39;: False, + \u0026#39;HasMFMA_f64\u0026#39;: False, + \u0026#39;HasMFMA_f8\u0026#39;: False, + \u0026#39;HasMFMA_i8_908\u0026#39;: False, + \u0026#39;HasMFMA_i8_940\u0026#39;: False, + \u0026#39;HasMFMA_vgpr\u0026#39;: False, + \u0026#39;HasMFMA_xf32\u0026#39;: False, + \u0026#39;HasSMulHi\u0026#39;: True, + \u0026#39;HasWMMA\u0026#39;: True, + \u0026#39;KernargPreloading\u0026#39;: False, + \u0026#39;MaxLgkmcnt\u0026#39;: 15, + \u0026#39;MaxVmcnt\u0026#39;: 63, + \u0026#39;SupportedISA\u0026#39;: True, + \u0026#39;SupportedSource\u0026#39;: True, + \u0026#39;VOP3v_dot4_i32_i8\u0026#39;: False, + \u0026#39;v_dot2_f32_f16\u0026#39;: True, + \u0026#39;v_dot2c_f32_f16\u0026#39;: True, + \u0026#39;v_dot4_i32_i8\u0026#39;: False, + \u0026#39;v_dot4c_i32_i8\u0026#39;: False, + \u0026#39;v_fma_f16\u0026#39;: True, + \u0026#39;v_fma_f32\u0026#39;: True, + \u0026#39;v_fma_f64\u0026#39;: True, + \u0026#39;v_fma_mix_f32\u0026#39;: True, + \u0026#39;v_fmac_f16\u0026#39;: False, + \u0026#39;v_fmac_f32\u0026#39;: True, + \u0026#39;v_mac_f16\u0026#39;: False, + \u0026#39;v_mac_f32\u0026#39;: False, + \u0026#39;v_mad_mix_f32\u0026#39;: False, + \u0026#39;v_mov_b64\u0026#39;: False, + \u0026#39;v_pk_fma_f16\u0026#39;: True, + \u0026#39;v_pk_fmac_f16\u0026#39;: False}, (11, 5, 1): {\u0026#39;HasAddLshl\u0026#39;: True, \u0026#39;HasAtomicAdd\u0026#39;: True, \u0026#39;HasDirectToLdsDest\u0026#39;: False, diff --git a/Tensile/Common.py b/Tensile/Common.py index e440e942..57813169 100644 --- a/Tensile/Common.py +++ b/Tensile/Common.py @@ -229,7 +229,7 @@ globalParameters[\u0026#34;SupportedISA\u0026#34;] = [(8,0,3), (9,4,0), (9,4,1), (9,4,2), (10,1,0), (10,1,1), (10,1,2), (10,3,0), (10,3,1), (11,0,0), (11,0,1), (11,0,2), - (11,5,1)] # assembly kernels writer supports these architectures + (11,5,0), (11,5,1)] # assembly kernels writer supports these architectures globalParameters[\u0026#34;CleanupBuildFiles\u0026#34;] = False # cleanup build files (e.g. kernel assembly) once no longer needed globalParameters[\u0026#34;GenerateManifestAndExit\u0026#34;] = False # Output manifest file with list of expected library objects and exit @@ -308,7 +308,7 @@ architectureMap = { \u0026#39;gfx1010\u0026#39;:\u0026#39;navi10\u0026#39;, \u0026#39;gfx1011\u0026#39;:\u0026#39;navi12\u0026#39;, \u0026#39;gfx1012\u0026#39;:\u0026#39;navi14\u0026#39;, \u0026#39;gfx1030\u0026#39;:\u0026#39;navi21\u0026#39;, \u0026#39;gfx1031\u0026#39;:\u0026#39;navi22\u0026#39;, \u0026#39;gfx1032\u0026#39;:\u0026#39;navi23\u0026#39;, \u0026#39;gfx1034\u0026#39;:\u0026#39;navi24\u0026#39;, \u0026#39;gfx1035\u0026#39;:\u0026#39;rembrandt\u0026#39;, \u0026#39;gfx1100\u0026#39;:\u0026#39;navi31\u0026#39;, \u0026#39;gfx1101\u0026#39;:\u0026#39;navi32\u0026#39;, \u0026#39;gfx1102\u0026#39;:\u0026#39;navi33\u0026#39;, - \u0026#39;gfx1151\u0026#39;:\u0026#39;gfx1151\u0026#39; + \u0026#39;gfx1150\u0026#39;:\u0026#39;gfx1150\u0026#39;, \u0026#39;gfx1151\u0026#39;:\u0026#39;gfx1151\u0026#39; } def getArchitectureName(gfxName): diff --git a/Tensile/Source/CMakeLists.txt b/Tensile/Source/CMakeLists.txt index e973a9ed..8904f4a7 100644 --- a/Tensile/Source/CMakeLists.txt +++ b/Tensile/Source/CMakeLists.txt @@ -51,9 +51,9 @@ if(NOT DEFINED CXX_VERSION_STRING) endif() if(CMAKE_CXX_COMPILER STREQUAL \u0026#34;hipcc\u0026#34;) - set(TENSILE_GPU_ARCHS gfx803 gfx900 gfx906:xnack- gfx908:xnack- gfx90a:xnack- gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1100 gfx1101 gfx1102 CACHE STRING \u0026#34;GPU architectures\u0026#34;) + set(TENSILE_GPU_ARCHS gfx803 gfx900 gfx906:xnack- gfx908:xnack- gfx90a:xnack- gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1100 gfx1101 gfx1102 gfx1150 CACHE STRING \u0026#34;GPU architectures\u0026#34;) else() - set(TENSILE_GPU_ARCHS gfx803 gfx900 gfx906 gfx908 gfx90a gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1100 gfx1101 gfx1102 CACHE STRING \u0026#34;GPU architectures\u0026#34;) + set(TENSILE_GPU_ARCHS gfx803 gfx900 gfx906 gfx908 gfx90a gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1100 gfx1101 gfx1102 gfx1150 CACHE STRING \u0026#34;GPU architectures\u0026#34;) endif() include(CMakeDependentOption) Then add the graphics card architecture name to the source code rocBLAS\\CMakeLists.txt file of rocBLAS. You can apply the following patch:\ndiff --git a/CMakeLists.txt b/CMakeLists.txt index dd521358..9c074139 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -111,7 +111,7 @@ list( APPEND CMAKE_PREFIX_PATH ${ROCM_PATH}/llvm ${ROCM_PATH} ${ROCM_PATH}/hip / set( TARGET_LIST_ROCM_5.6 \u0026#34;gfx803;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102\u0026#34;) set( TARGET_LIST_ROCM_5.7 \u0026#34;gfx803;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102\u0026#34;) set( TARGET_LIST_ROCM_6.0 \u0026#34;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102\u0026#34;) -set( TARGET_LIST_ROCM_6.2.4 \u0026#34;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102;gfx1151\u0026#34;) +set( TARGET_LIST_ROCM_6.2.4 \u0026#34;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102;gfx1150;gfx1151\u0026#34;) if(ROCM_PLATFORM_VERSION) if(${ROCM_PLATFORM_VERSION} VERSION_LESS 5.7.0) 3. Compile and install The following command initializes the x64 Native Tools Command Prompt for VS command line tool and enters the rocBLAS source directory, and executes the python command to download the vcpkg installation dependencies. Compile while downloading. For laptops, it is best to plug in the power supply and turn on the performance mode.\n# Load vc2022 compilation environment cmd /c \u0026#34;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\u0026#34; # Enter the rocBLAS source directory cd rocBLAS # Execute the following command to download a bunch of dependencies python rdeps.py Some of the dependent files will be stored in the temporary folder C:\\github and deleted after the compilation is successful.\nNext, execute the command to generate the library file, which takes a long time:\npython rmake.py -a gfx1150 --no-lazy-library-loading --no-merge-architectures -t $PWD\\..\\Tensile --cmake-arg=\u0026#34;-DROCM_PLATFORM_VERSION=6.2.4\u0026#34; After execution, the rocblas.dll file will be generated under the build\\release\\staging path of the rocBLAS source code directory.\nFinally, execute the following command to copy the generated library file and Tensile architecture file to a new folder:\n# Create a folder New-Item -Path $PWD\\..\\rocblas_dll -ItemType Directory # Copy the rocblas.dll file to the new folder Copy-Item -Path $PWD\\build\\release\\staging\\rocblas.dll -Destination $PWD\\..\\rocblas_dll # Copy the Tensile architecture file Copy-Item -Path $PWD\\build\\release\\Tensile\\library -Destination $PWD\\..\\rocblas_dll\\rocblas\\library -Recurse The generated dll file is much smaller than the dll that comes with the HIP SDK. There are two ways to use the newly generated rocblas:\nRelease the dll file and architecture file packaged in the rocblas_dll folder together with the compiled program. It is relatively simple and does not require the modification of the HIP SDK installation file;\nAdd the bin of the HIP SDK installation path to PATH, back up the original rocblas.dll and copy the newly generated rocblas.dll to the path, copy the newly generated gfx1150\u0026rsquo;s TensileLibrary_gfx1150.dat file and Kernels.so-000-gfx1150.hsaco to the bin\\rocblas\\library path of the HIP SDK installation path, and copy the generated TensileManifest.txt to the original TensileManifest.txt.\nFinally, don\u0026rsquo;t forget to delete the temporary folder: C:\\github\n4. Compile test Next, write a matrix multiplication program to test the performance of ROCm acceleration. The main program main.c is the same as 2.1 in Matrix multiplication operation (I) - using OpenMP to speed up loop calculation , and the blas.c file is as follows:\n// References: https://github.com/ROCm/rocBLAS-Examples/blob/develop/Languages/C/main.c #include \u0026lt;assert.h\u0026gt; #include \u0026lt;hip/hip_runtime_api.h\u0026gt; #include \u0026lt;math.h\u0026gt; #include \u0026lt;rocblas/rocblas.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #ifndef CHECK_HIP_ERROR #define CHECK_HIP_ERROR(error) \\ if (error != hipSuccess) \\ { \\ fprintf(stderr, \\ \u0026#34;hip error: \u0026#39;%s\u0026#39;(%d) at %s:%d\\n\u0026#34;, \\ hipGetErrorString(error), \\ error, \\ __FILE__, \\ __LINE__); \\ } #endif #ifndef CHECK_ROCBLAS_STATUS #define CHECK_ROCBLAS_STATUS(status) \\ if (status != rocblas_status_success) \\ { \\ fprintf(stderr, \u0026#34;rocBLAS error: \u0026#34;); \\ fprintf(stderr, \\ \u0026#34;rocBLAS error: \u0026#39;%s\u0026#39;(%d) at %s:%d\\n\u0026#34;, \\ rocblas_status_to_string(status), \\ status, \\ __FILE__, \\ __LINE__); \\ } #endif void matrix_multiply_float(int n, float A[], float B[], float C[]) { size_t rows, cols; rows = cols = n; typedef float data_type; rocblas_handle handle; rocblas_status rstatus = rocblas_create_handle(\u0026amp;handle); CHECK_ROCBLAS_STATUS(rstatus); hipStream_t test_stream; rstatus = rocblas_get_stream(handle, \u0026amp;test_stream); CHECK_ROCBLAS_STATUS(rstatus); data_type *da = 0; data_type *db = 0; data_type *dc = 0; CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;da, n * cols * sizeof(data_type))); CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;db, n * cols * sizeof(data_type))); CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;dc, n * cols * sizeof(data_type))); // upload asynchronously from pinned memory rstatus = rocblas_set_matrix_async(rows, cols, sizeof(data_type), A, n, da, n, test_stream); rstatus = rocblas_set_matrix_async(rows, cols, sizeof(data_type), B, n, db, n, test_stream); // scalar arguments will be from host memory rstatus = rocblas_set_pointer_mode(handle, rocblas_pointer_mode_host); CHECK_ROCBLAS_STATUS(rstatus); data_type alpha = 1.0; data_type beta = 2.0; // invoke asynchronous computation rstatus = rocblas_sgemm(handle, rocblas_operation_none, rocblas_operation_none, rows, cols, n, \u0026amp;alpha, da, n, db, n, \u0026amp;beta, dc, n); CHECK_ROCBLAS_STATUS(rstatus); // fetch results asynchronously to pinned memory rstatus = rocblas_get_matrix_async(rows, cols, sizeof(data_type), dc, n, C, n, test_stream); CHECK_ROCBLAS_STATUS(rstatus); // wait on transfer to be finished CHECK_HIP_ERROR(hipStreamSynchronize(test_stream)); CHECK_HIP_ERROR(hipFree(da)); CHECK_HIP_ERROR(hipFree(db)); CHECK_HIP_ERROR(hipFree(dc)); rstatus = rocblas_destroy_handle(handle); CHECK_ROCBLAS_STATUS(rstatus); } void matrix_multiply_double(int n, double A[], double B[], double C[]) { size_t rows, cols; rows = cols = n; typedef double data_type; rocblas_handle handle; rocblas_status rstatus = rocblas_create_handle(\u0026amp;handle); CHECK_ROCBLAS_STATUS(rstatus); hipStream_t test_stream; rstatus = rocblas_get_stream(handle, \u0026amp;test_stream); CHECK_ROCBLAS_STATUS(rstatus); data_type *da = 0; data_type *db = 0; data_type *dc = 0; CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;da, n * cols * sizeof(data_type))); CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;db, n * cols * sizeof(data_type))); CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;dc, n * cols * sizeof(data_type))); // upload asynchronously from pinned memory rstatus = rocblas_set_matrix_async(rows, cols, sizeof(data_type), A, n, da, n, test_stream); rstatus = rocblas_set_matrix_async(rows, cols, sizeof(data_type), B, n, db, n, test_stream); // scalar arguments will be from host memory rstatus = rocblas_set_pointer_mode(handle, rocblas_pointer_mode_host); CHECK_ROCBLAS_STATUS(rstatus); data_type alpha = 1.0; data_type beta = 2.0; // invoke asynchronous computation rstatus = rocblas_dgemm(handle, rocblas_operation_none, rocblas_operation_none, rows, cols, n, \u0026amp;alpha, da, n, db, n, \u0026amp;beta, dc, n); CHECK_ROCBLAS_STATUS(rstatus); // fetch results asynchronously to pinned memory rstatus = rocblas_get_matrix_async(rows, cols, sizeof(data_type), dc, n, C, n, test_stream); CHECK_ROCBLAS_STATUS(rstatus); // wait on transfer to be finished CHECK_HIP_ERROR(hipStreamSynchronize(test_stream)); CHECK_HIP_ERROR(hipFree(da)); CHECK_HIP_ERROR(hipFree(db)); CHECK_HIP_ERROR(hipFree(dc)); rstatus = rocblas_destroy_handle(handle); CHECK_ROCBLAS_STATUS(rstatus); } The cmake configuration file CMakeLists.txt is as follows. The HIP installation path is passed through the variable ROCM_DIR. The compilation here uses -DROCM_DIR=C:/Program Files/AMD/ROCm/6.2.\ncmake_minimum_required(VERSION 3.13) string(REGEX MATCHALL \u0026#34;[a-zA-Z]+\\ |[a-zA-Z]+$\u0026#34; DIRNAME \u0026#34;${CMAKE_CURRENT_SOURCE_DIR}\u0026#34;) project(${DIRNAME} LANGUAGES C) message(STATUS \u0026#34;PROJECT_NAME: ${PROJECT_NAME}\u0026#34;) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) message(STATUS \u0026#34;EXECUTE_FILE_NAME: ${EXECUTE_FILE_NAME}\u0026#34;) # setting aocl directory if(DEFINED ROCM_DIR) message(STATUS \u0026#34;ROCM_DIR is set to: ${ROCM_DIR}\u0026#34;) else() message(FATAL_ERROR \u0026#34;ROCM_DIR is not defined. Please set it to the AOCL installation directory.\u0026#34;) endif() set(CMAKE_C_FLAGS \u0026#34;${CMAKE_C_FLAGS} -D__HIP_PLATFORM_AMD__\u0026#34;) # Enable AOCL BLAS set(ROCM_INCLUDE_DIRS ${ROCM_DIR}/include) set(ROCM_LINK_DIR ${ROCM_DIR}/lib) set(ROCM_LIBS \u0026#34;rocblas;amdhip64\u0026#34;) set(SRC_LIST src/main.c src/blas.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${ROCM_INCLUDE_DIRS} ) target_link_directories(${EXECUTE_FILE_NAME} PRIVATE ${ROCM_LINK_DIR} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ${ROCM_LIBS} ) Compile using clang-cl of vs2022 on Windows platform, the running effect is as follows:\nPS D:\\example\\efficiency_v3\\rocm\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/rocm/build/Release/rocm_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.595000s(230.990Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.146214s(939.986Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.147548s(931.486Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.146811s(936.165Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.152827s(899.311Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.143523s(957.610Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.147841s(929.642Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.152921s(898.757Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.173812s(790.734Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.172323s(797.565Gflops) Average Gflops: 831.225, Max Gflops: 957.610 Average Time: 0.197882s, Min Time: 0.143523s PS D:\\example\\efficiency_v3\\rocm\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/rocm/build/Release/rocm_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 1.694739s(81.097Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 1.185994s(115.885Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 1.262083s(108.898Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 1.232974s(111.469Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 1.156926s(118.797Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 1.220483s(112.610Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 1.270335s(108.191Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 1.209922s(113.593Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 1.226809s(112.030Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 1.335011s(102.950Gflops) Average Gflops: 108.552, Max Gflops: 118.797 Average Time: 1.279528s, Min Time: 1.156926s The performance is similar to opencl, and we look forward to developing more features.\nSystem requirements (Windows) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAMD 680M显卡编译rocBLAS使用SD \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWindows编译AMD ROCm rocblas教程 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWindows下编译rocBLAS \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAMD HIP SDK for Windows \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-06-27-windows-compile-rocblas-rocm-6_2_4/","tags":[{"LinkTitle":"C++","RelPermalink":"/tags/c++/"}],"title":"Compile rocblas-rocm-6.2.4 under Windows"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/categories/code/"}],"content":"MPI is a parallel computing protocol and is currently the most commonly used interface program for high-performance computing clusters. MPI communicates through inter-process messages and can call multiple cores across nodes to perform parallel computing, which is not available in OpenMP. MPI is implemented on different platforms, such as MS-MPI and Intel MPI under Windows, OpenMPI and MPICH under Linux, etc.\n1. MPI parallel acceleration loop calculation 1.1 C Implementation MPI needs to initialize the main program interface and establish a message broadcast mechanism; at the same time, the array to be calculated is divided and broadcast to different processes. In the past, these operations were implemented internally by OpenMP or other parallel libraries, and programmers did not need to care about how the underlying implementation was implemented. However, using MPI requires programmers to manually allocate the global and local space of each process and control the broadcast of each message, which undoubtedly increases the additional learning cost.\nHere, the main.c file is modified as follows.\n#include \u0026#34;mpi.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;time.h\u0026gt; #include \u0026lt;math.h\u0026gt; #define MAX(a, b) ((a) \u0026gt; (b) ? (a) : (b)) #define MIN(a, b) ((a) \u0026lt; (b) ? (a) : (b)) extern void matrix_multiply_float(int n, int rank, int size, float local_A[], float B[], float local_C[]); extern void matrix_multiply_double(int n, int rank, int size, double local_A[], double B[], double local_C[]); // Initialize matrix void initialize_matrix_float(int n, float matrix[]) { srand((unsigned)time(NULL)); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { matrix[i * n + j] = rand() / (float)(RAND_MAX); } } } void initialize_matrix_double(int n, double matrix[]) { srand((unsigned)time(NULL)); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { matrix[i * n + j] = rand() / (double)(RAND_MAX); } } } // Execute matrix multiply and print results int mpi_float(int dim, int loop_num, double *ave_gflops, double *max_gflops, double *ave_time, double *min_time) { int rank, size; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;size); // Use volatile to prevent compiler optimizations volatile float *a, *b, *local_c; struct timespec start_ns, end_ns; double cpu_time, total_cpu_time; // Count the number of rows processed by each process int rows_per_process = dim / size; int remainder = dim % size; if (rank \u0026lt; remainder) { rows_per_process++; } for (int i = 0; i \u0026lt; loop_num; i++) { int check_indices[2]; float check_value; // The main process allocates memory for the complete matrix if (rank == 0) { a = (float *)malloc(dim * dim * sizeof(float)); b = (float *)malloc(dim * dim * sizeof(float)); if (a == NULL || b == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } initialize_matrix_float(dim, a); initialize_matrix_float(dim, b); // Generate checksum value int check_row = rand() % dim; int check_col = rand() % dim; check_value = 0.0; for (int k = 0; k \u0026lt; dim; k++) { check_value += a[check_row * dim + k] * b[k * dim + check_col]; } check_indices[0] = check_row; check_indices[1] = check_col; // Broadcast verification row and column index and verification value MPI_Bcast(check_indices, 2, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026amp;check_value, 1, MPI_FLOAT, 0, MPI_COMM_WORLD); } else { a = NULL; b = NULL; MPI_Bcast(check_indices, 2, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026amp;check_value, 1, MPI_FLOAT, 0, MPI_COMM_WORLD); } // Allocate local matrix space for each process float *local_A = (float *)malloc(rows_per_process * dim * sizeof(float)); local_c = (float *)calloc(rows_per_process * dim, sizeof(float)); if (local_A == NULL || local_c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } // Distribute the rows of matrix A to each process int *sendcounts = (int *)malloc(size * sizeof(int)); int *displs = (int *)malloc(size * sizeof(int)); int offset = 0; for (int p = 0; p \u0026lt; size; p++) { sendcounts[p] = (p \u0026lt; remainder) ? (rows_per_process * dim) : ((rows_per_process - 1) * dim); displs[p] = offset; offset += sendcounts[p]; } MPI_Scatterv(a, sendcounts, displs, MPI_FLOAT, local_A, rows_per_process * dim, MPI_FLOAT, 0, MPI_COMM_WORLD); // All processes require the complete matrix B float *full_B = (float *)malloc(dim * dim * sizeof(float)); if (full_B == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed for full_B\\n\u0026#34;); return 0; } if (rank == 0) { memcpy(full_B, b, dim * dim * sizeof(float)); } MPI_Bcast(full_B, dim * dim, MPI_FLOAT, 0, MPI_COMM_WORLD); timespec_get(\u0026amp;start_ns, TIME_UTC); matrix_multiply_float(dim, rank, size, local_A, full_B, local_c); timespec_get(\u0026amp;end_ns, TIME_UTC); cpu_time = (end_ns.tv_sec - start_ns.tv_sec) + (end_ns.tv_nsec - start_ns.tv_nsec) / 1e9; // Use MPI_Reduce to sum the cpu_time of all processes MPI_Reduce(\u0026amp;cpu_time, \u0026amp;total_cpu_time, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); if (rank == 0) { // Calculate the average cpu_time = total_cpu_time / size; } // Broadcast the average value to all processes MPI_Bcast(\u0026amp;cpu_time, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD); double gflops = 1e-9 * dim * dim * dim * 2 / cpu_time; if (rank == 0) { printf(\u0026#34;%d\\t: %d x %d Matrix multiply wall time : %.6fs(%.3fGflops)\\n\u0026#34;, i + 1, dim, dim, cpu_time, gflops); fflush(stdout); // Force flushing of standard output buffer } // Collect the local results of all processes to the main process float *c = NULL; if (rank == 0) { c = (float *)malloc(dim * dim * sizeof(float)); if (c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed for c matrix\\n\u0026#34;); return 0; } } MPI_Gatherv(local_c, rows_per_process * dim, MPI_FLOAT, c, sendcounts, displs, MPI_FLOAT, 0, MPI_COMM_WORLD); // Main process performs verification if (rank == 0) { int check_row = check_indices[0]; int check_col = check_indices[1]; float result_value = c[check_row * dim + check_col]; if (fabs(result_value - check_value) \u0026gt; 0.001) { fprintf(stderr, \u0026#34;Verification failed at iteration %d: expected %.6f, got %.6f\\n\u0026#34;, i + 1, check_value, result_value); } free(c); } // Free memory if (rank == 0) { free(a); free(b); } free(local_A); free(local_c); free(sendcounts); free(displs); free(full_B); *ave_gflops += gflops; *max_gflops = MAX(*max_gflops, gflops); *ave_time += cpu_time; *min_time = MIN(*min_time, cpu_time); } *ave_gflops /= loop_num; *ave_time /= loop_num; return 1; } int mpi_double(int dim, int loop_num, double *ave_gflops, double *max_gflops, double *ave_time, double *min_time) { int rank, size; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;size); // Use volatile to prevent compiler optimizations volatile double *a, *b, *local_c; struct timespec start_ns, end_ns; double cpu_time, total_cpu_time; // Count the number of rows processed by each process int rows_per_process = dim / size; int remainder = dim % size; if (rank \u0026lt; remainder) { rows_per_process++; } for (int i = 0; i \u0026lt; loop_num; i++) { int check_indices[2]; double check_value; // The main process allocates memory for the complete matrix if (rank == 0) { a = (double *)malloc(dim * dim * sizeof(double)); b = (double *)malloc(dim * dim * sizeof(double)); if (a == NULL || b == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } initialize_matrix_double(dim, a); initialize_matrix_double(dim, b); // Generate checksum value int check_row = rand() % dim; int check_col = rand() % dim; check_value = 0.0; for (int k = 0; k \u0026lt; dim; k++) { check_value += a[check_row * dim + k] * b[k * dim + check_col]; } check_indices[0] = check_row; check_indices[1] = check_col; // Broadcast verification row and column index and verification value MPI_Bcast(check_indices, 2, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026amp;check_value, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD); } else { a = NULL; b = NULL; MPI_Bcast(check_indices, 2, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026amp;check_value, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD); } // Allocate local matrix space for each process double *local_A = (double *)malloc(rows_per_process * dim * sizeof(double)); local_c = (double *)calloc(rows_per_process * dim, sizeof(double)); if (local_A == NULL || local_c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } // Distribute the rows of matrix A to each process int *sendcounts = (int *)malloc(size * sizeof(int)); int *displs = (int *)malloc(size * sizeof(int)); int offset = 0; for (int p = 0; p \u0026lt; size; p++) { sendcounts[p] = (p \u0026lt; remainder) ? (rows_per_process * dim) : ((rows_per_process - 1) * dim); displs[p] = offset; offset += sendcounts[p]; } MPI_Scatterv(a, sendcounts, displs, MPI_DOUBLE, local_A, rows_per_process * dim, MPI_DOUBLE, 0, MPI_COMM_WORLD); // All processes require the complete matrix B double *full_B = (double *)malloc(dim * dim * sizeof(double)); if (full_B == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed for full_B\\n\u0026#34;); return 0; } if (rank == 0) { memcpy(full_B, b, dim * dim * sizeof(double)); } MPI_Bcast(full_B, dim * dim, MPI_DOUBLE, 0, MPI_COMM_WORLD); timespec_get(\u0026amp;start_ns, TIME_UTC); matrix_multiply_double(dim, rank, size, local_A, full_B, local_c); timespec_get(\u0026amp;end_ns, TIME_UTC); cpu_time = (end_ns.tv_sec - start_ns.tv_sec) + (end_ns.tv_nsec - start_ns.tv_nsec) / 1e9; // Use MPI_Reduce to sum the cpu_time of all processes MPI_Reduce(\u0026amp;cpu_time, \u0026amp;total_cpu_time, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); if (rank == 0) { // Calculate the average cpu_time = total_cpu_time / size; } // Broadcast the average value to all processes MPI_Bcast(\u0026amp;cpu_time, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD); double gflops = 1e-9 * dim * dim * dim * 2 / cpu_time; if (rank == 0) { printf(\u0026#34;%d\\t: %d x %d Matrix multiply wall time : %.6fs(%.3fGflops)\\n\u0026#34;, i + 1, dim, dim, cpu_time, gflops); fflush(stdout); // Force flushing of standard output buffer } // Collect the local results of all processes to the main process double *c = NULL; if (rank == 0) { c = (double *)malloc(dim * dim * sizeof(double)); if (c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed for c matrix\\n\u0026#34;); return 0; } } MPI_Gatherv(local_c, rows_per_process * dim, MPI_DOUBLE, c, sendcounts, displs, MPI_DOUBLE, 0, MPI_COMM_WORLD); // Main process performs verification if (rank == 0) { int check_row = check_indices[0]; int check_col = check_indices[1]; double result_value = c[check_row * dim + check_col]; if (fabs(result_value - check_value) \u0026gt; 0.000001) { fprintf(stderr, \u0026#34;Verification failed at iteration %d: expected %.6f, got %.6f\\n\u0026#34;, i + 1, check_value, result_value); } free(c); } // Free memory if (rank == 0) { free(a); free(b); } free(local_A); free(local_c); free(sendcounts); free(displs); free(full_B); *ave_gflops += gflops; *max_gflops = MAX(*max_gflops, gflops); *ave_time += cpu_time; *min_time = MIN(*min_time, cpu_time); } *ave_gflops /= loop_num; *ave_time /= loop_num; return 1; } int main(int argc, char *argv[]) { int rank; MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); int n = 10; // Default matrix size exponent int loop_num = 5; // Number of iterations for averaging double ave_gflops = 0.0, max_gflops = 0.0; // Average and maximum Gflops double ave_time = 0.0, min_time = 1e9; // Average and minimum time int use_double = 0; // Default to float precision // Help message if (argc == 1 || (argc == 2 \u0026amp;\u0026amp; (strcmp(argv[1], \u0026#34;-h\u0026#34;) == 0 || strcmp(argv[1], \u0026#34;--help\u0026#34;) == 0))) { if (rank == 0) { printf(\u0026#34;Usage: mpiexec/mpirun [-n/-np $NUM_PROCS] %s [-n SIZE] [-l LOOP_NUM] [-float|-double]\\n\u0026#34;, argv[0]); printf(\u0026#34; -n SIZE Specify matrix size, like 2^SIZE (default: 10)\\n\u0026#34;); printf(\u0026#34; -l LOOP_NUM Specify number of iterations (default: 5)\\n\u0026#34;); printf(\u0026#34; -float Use float precision (default)\\n\u0026#34;); printf(\u0026#34; -double Use double precision\\n\u0026#34;); printf(\u0026#34; -h, --help Show this help message\\n\u0026#34;); } MPI_Finalize(); return 0; } // Parse -n, -l, -float, -double options int double_flag = 0, float_flag = 0; for (int argi = 1; argi \u0026lt; argc; ++argi) { if (strcmp(argv[argi], \u0026#34;-n\u0026#34;) == 0 \u0026amp;\u0026amp; argi + 1 \u0026lt; argc) { n = atoi(argv[argi + 1]); argi++; } else if (strcmp(argv[argi], \u0026#34;-l\u0026#34;) == 0 \u0026amp;\u0026amp; argi + 1 \u0026lt; argc) { loop_num = atoi(argv[argi + 1]); argi++; } else if (strcmp(argv[argi], \u0026#34;-double\u0026#34;) == 0) { double_flag = 1; } else if (strcmp(argv[argi], \u0026#34;-float\u0026#34;) == 0) { float_flag = 1; } } if (double_flag \u0026amp;\u0026amp; float_flag) { if (rank == 0) { fprintf(stderr, \u0026#34;Error: Cannot specify both -double and -float options.\\n\u0026#34;); } MPI_Finalize(); return 1; } use_double = double_flag ? 1 : 0; int dim = (int)pow(2, n); if (use_double) { if (rank == 0) { printf(\u0026#34;Using double precision for matrix multiplication.\\n\u0026#34;); } if (!mpi_double(dim, loop_num, \u0026amp;ave_gflops, \u0026amp;max_gflops, \u0026amp;ave_time, \u0026amp;min_time)) { MPI_Finalize(); return 1; } } else { if (rank == 0) { printf(\u0026#34;Using float precision for matrix multiplication.\\n\u0026#34;); } if (!mpi_float(dim, loop_num, \u0026amp;ave_gflops, \u0026amp;max_gflops, \u0026amp;ave_time, \u0026amp;min_time)) { MPI_Finalize(); return 1; } } if (rank == 0) { printf(\u0026#34;Average Gflops: %.3f, Max Gflops: %.3f\\n\u0026#34;, ave_gflops, max_gflops); printf(\u0026#34;Average Time: %.6fs, Min Time: %.6fs\\n\u0026#34;, ave_time, min_time); } MPI_Finalize(); return 0; } In order to ensure that there are no errors in the merged matrix, a random coordinate verification mechanism is added to the main program.\nThe mpi.c file is the specific implementation of the loop nested matrix operation.\nvoid matrix_multiply_float(int n, int rank, int size, float local_A[], float B[], float local_C[]) { // Count the number of rows processed by each process int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // Matrix multiplication operation for (int i = 0; i \u0026lt; rows_per_process; i++) { for (int j = 0; j \u0026lt; n; j++) { for (int k = 0; k \u0026lt; n; k++) { local_C[i * n + j] += local_A[i * n + k] * B[k * n + j]; } } } } // Parallel matrix multiply void matrix_multiply_double(int n, int rank, int size, double local_A[], double B[], double local_C[]) { // Count the number of rows processed by each process int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // Matrix multiplication operation for (int i = 0; i \u0026lt; rows_per_process; i++) { for (int j = 0; j \u0026lt; n; j++) { for (int k = 0; k \u0026lt; n; k++) { local_C[i * n + j] += local_A[i * n + k] * B[k * n + j]; } } } } The CMakeLists.txt configuration file sets how to include the mpi header file. The compilation here is done on the Windows platform, and the corresponding mpi library is ms-mpi.\ncmake_minimum_required(VERSION 3.13) project(mpi LANGUAGES C CXX) set(CMAKE_C_STANDARD 11) set(CMAKE_CXX_STANDARD 20) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable MPI support find_package(MPI REQUIRED) set(SRC_LIST src/main.c src/mpi.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${MPI_CXX_INCLUDE_DIRS} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ${MPI_LIBRARIES} ) Compile using MSYS2\u0026rsquo;s ucrt64 on the Windows platform and output the Release program execution effect as follows:\nPS D:\\example\\efficiency_v3\\c\\mpi\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/c/mpi/build/mpi_gnu_gnu_15.1.0.exe -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.235279s(9.127Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.245679s(8.741Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.252880s(8.492Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.253803s(8.461Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.235253s(9.128Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.261678s(8.207Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.261258s(8.220Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.256560s(8.370Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.254379s(8.442Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.271509s(7.909Gflops) Average Gflops: 8.510, Max Gflops: 9.128 Average Time: 0.252828s, Min Time: 0.235253s PS D:\\example\\efficiency_v3\\c\\mpi\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/c/mpi/build/mpi_gnu_gnu_15.1.0.exe -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.317128s(6.772Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.316111s(6.793Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.320794s(6.694Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.334887s(6.413Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.328439s(6.538Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.326553s(6.576Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.321168s(6.686Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.322008s(6.669Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.330364s(6.500Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.314038s(6.838Gflops) Average Gflops: 6.648, Max Gflops: 6.838 Average Time: 0.323149s, Min Time: 0.314038s The CPU of the computing machine is AMD AI 9 365w, with 10 cores and 20 threads. Hyperthreading is used here, and the overall performance is similar to that of OpenMP. The number of computing processes can be controlled through the command line. Since each process needs to allocate local storage space, the more computing processes there are, the greater the memory overhead.\n1.2 Fortran Implementation Fortran also provides MPI implementation, and both MS-MPI and OpenMPI provide Fortran interfaces. Due to space limitations, the main program source code here still uses the same main.c as in the previous section. The matrix multiplication part is implemented in Fortran, and the program functions are implemented by mixed programming of Fortran and C.\nThe matrix multiplication calculation related functions are defined in the Fortran source file mpi2c.f90:\nsubroutine matrix_multiply_float(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_float\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_float), intent(in) :: local_A(*) real(c_float), intent(in) :: B(*) real(c_float), intent(out) :: local_C(*) real(c_float), allocatable :: local_A_2d(:, :) real(c_float), allocatable :: B_2d(:, :) real(c_float), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j, k remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! Convert a one-dimensional array to a two-dimensional array do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! Initialize the result matrix local_C_2d = 0.0_c_float ! Matrix multiplication calculation do i = 1, rows_per_process do j = 1, n do k = 1, n local_C_2d(i, j) = local_C_2d(i, j) + local_A_2d(i, k) * B_2d(k, j) end do end do end do ! Convert a two-dimensional array to a one-dimensional array do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_double\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_double), intent(in) :: local_A(*) real(c_double), intent(in) :: B(*) real(c_double), intent(out) :: local_C(*) real(c_double), allocatable :: local_A_2d(:, :) real(c_double), allocatable :: B_2d(:, :) real(c_double), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j, k remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! Convert a one-dimensional array to a two-dimensional array do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! Initialize the result matrix local_C_2d = 0.0_c_double ! Matrix multiplication calculation do i = 1, rows_per_process do j = 1, n do k = 1, n local_C_2d(i, j) = local_C_2d(i, j) + local_A_2d(i, k) * B_2d(k, j) end do end do end do ! Convert a two-dimensional array to a one-dimensional array do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_double Since the C program interface stores the two-dimensional matrix in a one-dimensional array, in order for the Fortran program to read the matrix information stored in the one-dimensional array normally, it is necessary to define the conversion from the one-dimensional array to the two-dimensional array and open up new storage space. In order to facilitate interface reuse, the relevant content of the array conversion is defined in the calculation function, which will bring additional performance overhead.\nThe CMakeLists.txt configuration file itself has not changed much, and still contains the C mpi header file, but the settings for activating the Fortran language support are added.\ncmake_minimum_required(VERSION 3.13) project(mpi-fortran LANGUAGES C Fortran) set(CMAKE_C_STANDARD 11) set(CMAKE_Fortran_STANDARD 2008) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_Fortran_COMPILER_FRONTEND_VARIANT}_${CMAKE_Fortran_COMPILER_ID}_${CMAKE_Fortran_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable MPI support find_package(MPI REQUIRED) set(SRC_LIST src/main.c src/mpi2c.f90 ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${MPI_CXX_INCLUDE_DIRS} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ${MPI_LIBRARIES} ) Compiled using MSYS2\u0026rsquo;s ucrt64 toolchain under Windows, the Release program runs as follows:\nPS D:\\example\\efficiency_v3\\fortran\\mpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/mpi-fortran/build/mpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.078928s(27.208Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.089289s(24.051Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.082848s(25.921Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.080060s(26.823Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.084739s(25.342Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.083975s(25.573Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.085056s(25.248Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.083494s(25.720Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.082314s(26.089Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.082700s(25.967Gflops) Average Gflops: 25.794, Max Gflops: 27.208 Average Time: 0.083340s, Min Time: 0.078928s PS D:\\example\\efficiency_v3\\fortran\\mpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/mpi-fortran/build/mpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.126719s(16.947Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.127700s(16.817Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.126309s(17.002Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.124125s(17.301Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.121543s(17.668Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.129106s(16.634Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.129957s(16.525Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.128702s(16.686Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.127478s(16.846Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.125188s(17.154Gflops) Average Gflops: 16.958, Max Gflops: 17.668 Average Time: 0.126683s, Min Time: 0.121543s The above is the result of running on the same processor as 1.1. Fortran arrays use a column-first storage method that is different from C. The cache hit rate is improved, which is beneficial to improving matrix operation performance.\n2. MPI accelerates matrix block operations 2.1 C Implementation The above results reveal that cache performance has a great impact on matrix operations. In order to improve the cache hit rate, we try to use the matrix block algorithm to see if it can improve the computing performance of the C language program.\nThe main program main.c remains unchanged as before. Only mpi.c is changed here to add the block calculation function based on the original loop nested calculation:\n#include \u0026lt;math.h\u0026gt; //Block size, can be adjusted according to cache size #define BLOCK_SIZE 8 // Parallel matrix multiply void matrix_multiply_float(int n, int rank, int size, float local_A[], float B[], float local_C[]) { // Count the number of rows processed by each process int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // Initialize local_C to 0 for (int i = 0; i \u0026lt; rows_per_process; i++) { for (int j = 0; j \u0026lt; n; j++) { local_C[i * n + j] = 0.0f; } } // Matrix block multiplication for (int bi = 0; bi \u0026lt; rows_per_process; bi += BLOCK_SIZE) { for (int bj = 0; bj \u0026lt; n; bj += BLOCK_SIZE) { for (int bk = 0; bk \u0026lt; n; bk += BLOCK_SIZE) { // Calculation within the block int end_i = fmin(bi + BLOCK_SIZE, rows_per_process); int end_j = fmin(bj + BLOCK_SIZE, n); int end_k = fmin(bk + BLOCK_SIZE, n); for (int i = bi; i \u0026lt; end_i; i++) { for (int j = bj; j \u0026lt; end_j; j++) { float sum = local_C[i * n + j]; for (int k = bk; k \u0026lt; end_k; k++) { sum += local_A[i * n + k] * B[k * n + j]; } local_C[i * n + j] = sum; } } } } } } // Parallel matrix multiply void matrix_multiply_double(int n, int rank, int size, double local_A[], double B[], double local_C[]) { // Count the number of rows processed by each process int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // Initialize local_C to 0 for (int i = 0; i \u0026lt; rows_per_process; i++) { for (int j = 0; j \u0026lt; n; j++) { local_C[i * n + j] = 0.0; } } // Matrix block multiplication for (int bi = 0; bi \u0026lt; rows_per_process; bi += BLOCK_SIZE) { for (int bj = 0; bj \u0026lt; n; bj += BLOCK_SIZE) { for (int bk = 0; bk \u0026lt; n; bk += BLOCK_SIZE) { // Calculation within the block int end_i = fmin(bi + BLOCK_SIZE, rows_per_process); int end_j = fmin(bj + BLOCK_SIZE, n); int end_k = fmin(bk + BLOCK_SIZE, n); for (int i = bi; i \u0026lt; end_i; i++) { for (int j = bj; j \u0026lt; end_j; j++) { double sum = local_C[i * n + j]; for (int k = bk; k \u0026lt; end_k; k++) { sum += local_A[i * n + k] * B[k * n + j]; } local_C[i * n + j] = sum; } } } } } } The configuration file CMakeLists.txt is the same as in 1.1.\nCompiled using MSYS2\u0026rsquo;s ucrt64 toolchain under Windows, the program execution effect is as follows:\nPS D:\\example\\efficiency_v3\\c\\blockmpi\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/c/blockmpi/build/blockmpi_gnu_gnu_15.1.0.exe -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.039390s(54.519Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.041973s(51.164Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.044693s(48.050Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.042822s(50.149Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.044406s(48.361Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.041931s(51.215Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.042498s(50.531Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.041639s(51.573Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.042505s(50.523Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.043471s(49.401Gflops) Average Gflops: 50.548, Max Gflops: 54.519 Average Time: 0.042533s, Min Time: 0.039390s PS D:\\example\\efficiency_v3\\c\\blockmpi\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/c/blockmpi/build/blockmpi_gnu_gnu_15.1.0.exe -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.041856s(51.307Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.045559s(47.136Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.044370s(48.400Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.043212s(49.697Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.041711s(51.485Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.043633s(49.217Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.042332s(50.730Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.041305s(51.991Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.041940s(51.203Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.040975s(52.410Gflops) Average Gflops: 50.358, Max Gflops: 52.410 Average Time: 0.042689s, Min Time: 0.040975s The block size is set to 8 here, which is the optimal result obtained based on a large number of tests. When implementing on other platforms, it is necessary to test the hardware of the platform to determine the most appropriate block size.\n2.2 Fortran Implementation Next, use Fortran to implement the block matrix MPI acceleration function to see if the performance is improved.\nmain.c and CMakeLists.txt are basically the same as 1.2, only the mpi2c.f90 file is changed to add block content:\nsubroutine matrix_multiply_float(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_float\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_float), intent(in) :: local_A(*) real(c_float), intent(in) :: B(*) real(c_float), intent(out) :: local_C(*) real(c_float), allocatable :: local_A_2d(:, :) real(c_float), allocatable :: B_2d(:, :) real(c_float), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j, k, ii, jj, kk integer(c_int), parameter :: block_size = 8 ! Block size can be adjusted according to actual conditions remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! Convert a one-dimensional array to a two-dimensional array do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! Initialize the result matrix local_C_2d = 0.0_c_float ! 分块矩阵乘法计算 do ii = 1, rows_per_process, block_size do kk = 1, n, block_size do jj = 1, n, block_size do i = ii, min(ii + block_size - 1, rows_per_process) do k = kk, min(kk + block_size - 1, n) do j = jj, min(jj + block_size - 1, n) local_C_2d(i, j) = local_C_2d(i, j) + local_A_2d(i, k) * B_2d(k, j) end do end do end do end do end do end do ! Convert a two-dimensional array to a one-dimensional array do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_double\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_double), intent(in) :: local_A(*) real(c_double), intent(in) :: B(*) real(c_double), intent(out) :: local_C(*) real(c_double), allocatable :: local_A_2d(:, :) real(c_double), allocatable :: B_2d(:, :) real(c_double), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j, k, ii, jj, kk integer(c_int), parameter :: block_size = 8 ! Block size can be adjusted according to actual conditions remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! Convert a one-dimensional array to a two-dimensional array do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! Initialize the result matrix local_C_2d = 0.0_c_double ! 分块矩阵乘法计算 do ii = 1, rows_per_process, block_size do kk = 1, n, block_size do jj = 1, n, block_size do i = ii, min(ii + block_size - 1, rows_per_process) do k = kk, min(kk + block_size - 1, n) do j = jj, min(jj + block_size - 1, n) local_C_2d(i, j) = local_C_2d(i, j) + local_A_2d(i, k) * B_2d(k, j) end do end do end do end do end do end do ! Convert a two-dimensional array to a one-dimensional array do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_double Under Windows, use MSYS2\u0026rsquo;s ucrt64 tool chain to compile and output the Release program. The execution effect is as follows:\nPS D:\\example\\efficiency_v3\\fortran\\blockmpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/blockmpi-fortran/build/blockmpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.050200s(42.779Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.050326s(42.671Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.049548s(43.341Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.050283s(42.708Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.048627s(44.163Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.052763s(40.700Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.056050s(38.314Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.055364s(38.789Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.051921s(41.361Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.053460s(40.170Gflops) Average Gflops: 41.500, Max Gflops: 44.163 Average Time: 0.051854s, Min Time: 0.048627s PS D:\\example\\efficiency_v3\\fortran\\blockmpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/blockmpi-fortran/build/blockmpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.078406s(27.389Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.066016s(32.530Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.067229s(31.943Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.067948s(31.605Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.063610s(33.760Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.063506s(33.816Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.063981s(33.564Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.063603s(33.764Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.068714s(31.252Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.063268s(33.943Gflops) Average Gflops: 32.357, Max Gflops: 33.943 Average Time: 0.066628s, Min Time: 0.063268s It seems a bit slower than the C implementation, but in fact, the Fortran calculation function adds array conversion, storage space allocation and other functions, which increase the extra running time. Excluding this part of the overhead, the actual calculation performance should be better.\n3. MPI parallelism and underlying optimization acceleration 3.1 fortran compiler optimization Although the previous block matrix has been greatly improved, compared with the compiler\u0026rsquo;s underlying optimization and professional math library mentioned above, this performance is still a bit insufficient. Now try to combine MPI with the underlying optimization of the Fortran compiler to see if the performance can be further improved.\nmain.c and CMakeLists.txt are basically the same as 1.2. Here, the mpi2c.f90 file is modified to use the Fortran built-in function matmul to implement matrix operation functions:\nsubroutine matrix_multiply_float(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_float\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_float), intent(in) :: local_A(*) real(c_float), intent(in) :: B(*) real(c_float), intent(out) :: local_C(*) real(c_float), allocatable :: local_A_2d(:, :) real(c_float), allocatable :: B_2d(:, :) real(c_float), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! Convert a one-dimensional array to a two-dimensional array do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! Use matmul to perform matrix multiplication local_C_2d = matmul(local_A_2d, B_2d) ! Convert a two-dimensional array to a one-dimensional array do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_double\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_double), intent(in) :: local_A(*) real(c_double), intent(in) :: B(*) real(c_double), intent(out) :: local_C(*) real(c_double), allocatable :: local_A_2d(:, :) real(c_double), allocatable :: B_2d(:, :) real(c_double), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! Convert a one-dimensional array to a two-dimensional array do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! Use matmul to perform matrix multiplication local_C_2d = matmul(local_A_2d, B_2d) ! Convert a two-dimensional array to a one-dimensional array do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_double Compile using MSYS2\u0026rsquo;s ucrt64 toolchain and execute the Release program under Windows. The results are as follows:\nPS D:\\example\\efficiency_v3\\fortran\\matmul-mpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/matmul-mpi-fortran/build/matmul-mpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.797250s(172.391Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.551850s(249.051Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.560781s(245.085Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.555596s(247.372Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.564455s(243.490Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.552102s(248.938Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.588235s(233.646Gflops) Verification failed at iteration 7: expected 1032.420410, got 1032.418823 8 : 4096 x 4096 Matrix multiply wall time : 0.582861s(235.801Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.604666s(227.297Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.615205s(223.404Gflops) Average Gflops: 232.647, Max Gflops: 249.051 Average Time: 0.597300s, Min Time: 0.551850s PS D:\\example\\efficiency_v3\\fortran\\matmul-mpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/matmul-mpi-fortran/build/matmul-mpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 1.493319s(92.036Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 1.031578s(133.232Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 1.074232s(127.942Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 1.051967s(130.649Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 1.135295s(121.060Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 1.071376s(128.283Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 1.059316s(129.743Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 1.061684s(129.454Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 1.042263s(131.866Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 1.054022s(130.395Gflops) Average Gflops: 125.466, Max Gflops: 133.232 Average Time: 1.107505s, Min Time: 1.031578s Compared with the previous matrix block algorithm, it is a great improvement. Notice that a check error occurs in single-precision floating-point operations. The single-precision floating-point check error in the main program is 0.001. Here, the numerical error increases to 0.0016, exceeding the error value set previously. As the matrix size increases, the error of floating-point operations will accumulate. A more reasonable solution is to use higher-precision floating-point operations or increase the check error. The former means that the amount of calculation increases and the performance decreases, and the latter may lead to a decrease in the credibility of the calculation results. Which price to accept needs to be determined based on the calculation occasion.\n3.2 BLAS acceleration The blas library demonstrated previously uses openmp to achieve parallel acceleration at the bottom layer, but the openmp library is designed to be only suitable for single-node multi-core acceleration, not for multi-node multi-core operating environments on supercomputers. Is there a way to use the blas library with mpi to achieve cross-node multi-core acceleration at the bottom layer? The implementation method is demonstrated below.\nFirst, the main program file main.c is directly reused as before, and mpi.c is modified to add a call to the openblas function to implement matrix operation functions. Since openblas uses openmp acceleration by default, you need to manually set openblas to run as a single thread.\n#include \u0026lt;cblas.h\u0026gt; #include \u0026lt;openblas_config.h\u0026gt; #include \u0026lt;mpi.h\u0026gt; void matrix_multiply_float(int n, int rank, int size, float local_A[], float B[], float local_C[]) { // Set OpenBLAS to use single thread openblas_set_num_threads(1); // Count the number of rows processed by each process int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // Local matrix multiplication using OpenBLAS // C = α * A * B + β * C，which α = 1.0，β = 1.0 cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, rows_per_process, n, n, 1.0, local_A, n, B, n, 1.0, local_C, n); } void matrix_multiply_double(int n, int rank, int size, double local_A[], double B[], double local_C[]) { // Set OpenBLAS to use single thread openblas_set_num_threads(1); // Count the number of rows processed by each process int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // Local matrix multiplication using OpenBLAS // C = α * A * B + β * C，which α = 1.0，β = 1.0 cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, rows_per_process, n, n, 1.0, local_A, n, B, n, 1.0, local_C, n); } The CMakeLists.txt configuration file also needs to link to openblas and openmp, otherwise the compilation will report an error.\ncmake_minimum_required(VERSION 3.13) project(openblas-mpi LANGUAGES C CXX) set(CMAKE_C_STANDARD 11) set(CMAKE_CXX_STANDARD 20) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable MPI support find_package(MPI REQUIRED) # Find OpenBLAS find_package(OpenMP REQUIRED) find_package(OpenBLAS CONFIG REQUIRED) set(SRC_LIST src/main.c src/mpi.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${MPI_CXX_INCLUDE_DIRS} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE OpenBLAS::OpenBLAS OpenMP::OpenMP_C ${MPI_LIBRARIES} ) Compile under Windows, use MSYS2\u0026rsquo;s ucrt64 toolchain and its own openblas, compile and output the Release program, the running effect is as follows:\nPS D:\\example\\efficiency_v3\\c\\openblas-mpi\\build\u0026gt; mpiexec -n 10 D:/example/efficiency_v3/c/openblas-mpi/build/openblas-mpi_gnu_gnu_15.1.0.exe -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.162794s(844.252Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.178354s(770.595Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.201812s(681.023Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.191400s(718.073Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.186770s(735.872Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.200100s(686.850Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.182761s(752.013Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.181107s(758.882Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.182459s(753.258Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.190692s(720.739Gflops) Average Gflops: 742.156, Max Gflops: 844.252 Average Time: 0.185825s, Min Time: 0.162794s PS D:\\example\\efficiency_v3\\c\\openblas-mpi\\build\u0026gt; mpiexec -n 10 D:/example/efficiency_v3/c/openblas-mpi/build/openblas-mpi_gnu_gnu_15.1.0.exe -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.267384s(514.013Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.361675s(380.007Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.338016s(406.605Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.331837s(414.176Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.352789s(389.578Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.346735s(396.381Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.343613s(399.982Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.349749s(392.965Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.340508s(403.629Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.347642s(395.347Gflops) Average Gflops: 409.268, Max Gflops: 514.013 Average Time: 0.337995s, Min Time: 0.267384s It can be seen that the calculation speed has been greatly improved compared with the previous method. Hyperthreading is not used here, but the result is very close to the effect of using OpenBLAS alone to accelerate the calculation, and the double-precision floating-point calculation performance is even better. Using hyperthreading during the test did not achieve better results, and there is a high probability that the program will crash, which may be caused by the mixing of OpenMP and MPI.\n4. Summarize Running on a single machine, writing an MPI program requires separate control of memory and message broadcasting, which makes coding and debugging more troublesome and brings additional performance overhead. Compared with OpenMP, it has no performance advantage, but OpenMP has more advantages in coding.\nThe significance of MPI lies in cross-node multi-core calls on machines such as supercomputing clusters. High-performance computing programs developed using MPI are particularly suitable for acceleration through calls on large clusters and multiple nodes in such occasions.\nFor high-performance computing programs, using MPI alone does not make much sense for loop acceleration. Computing acceleration should be achieved in conjunction with the optimization of compilers and professional mathematical libraries.\n","permalink":"https://andrewmoa.site/post/2025-06-25-matrix_multiplication-3/","tags":[{"LinkTitle":"C++","RelPermalink":"/tags/c++/"},{"LinkTitle":"Fortran","RelPermalink":"/tags/fortran/"}],"title":"Matrix multiplication operation (Ⅲ) - using MPI parallel acceleration"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/categories/code/"}],"content":"BLAS was originally developed as a linear algebra library using Fortran, and was later ported to C/C++. As a core component of modern high-performance computing, it has formed a set of standards. There are open source implementations such as Netlib BLAS, GotoBLAS and its successor OpenBLAS. Commercially, each manufacturer has corresponding implementations for its own platform, such as Intel\u0026rsquo;s MKL, NVIDIA\u0026rsquo;s CUDA, AMD\u0026rsquo;s AOCL and ROCm. Some of them are optimized for CPU platforms, and some use GPU parallel acceleration. This article uses different BLAS libraries to implement matrix operations and analyzes the performance differences between different implementations.\n1. CPU parallel accelerated BLAS library 1.1 Intel MKL The main.c file is the same as 2.1 in Matrix multiplication operation (I) - using OpenMP to speed up loop calculation . blas.c imports the BLAS library of mkl and uses the GEMM function to perform matrix multiplication operations.\n#include \u0026lt;mkl_cblas.h\u0026gt; void matrix_multiply_float(int n, float A[], float B[], float C[]) { cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, n, n, n, 1.0, A, n, B, n, 0.0, C, n); } void matrix_multiply_double(int n, double A[], double B[], double C[]) { cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, n, n, n, 1.0, A, n, B, n, 0.0, C, n); } The CMakeLists.txt configuration file includes the mkl and openmp library files. The mkl underlying layer uses openmp for parallelization by default, so it is necessary to link to the openmp library.\ncmake_minimum_required(VERSION 3.13) project(mkl LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) set(MKL_LINK static) # Enable OpenMP find_package(OpenMP REQUIRED) # Enable MKL find_package(MKL CONFIG REQUIRED) set(SRC_LIST src/main.c src/blas.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_compile_options(${EXECUTE_FILE_NAME} PUBLIC $\u0026lt;TARGET_PROPERTY:MKL::MKL,INTERFACE_COMPILE_OPTIONS\u0026gt; ) target_include_directories(${EXECUTE_FILE_NAME} PUBLIC $\u0026lt;TARGET_PROPERTY:MKL::MKL,INTERFACE_INCLUDE_DIRECTORIES\u0026gt; ) target_link_libraries(${EXECUTE_FILE_NAME} PUBLIC OpenMP::OpenMP_C $\u0026lt;LINK_ONLY:MKL::MKL\u0026gt; ) The compilation machine used here is an AMD laptop with an AI 9 365w processor. It is compiled and run using clang-cl of vs2022 under Windows. The execution effect of the Release program is as follows:\nPS D:\\example\\efficiency_v3\\c\\mkl\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/mkl/build/Release/mkl_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.218935s(627.761Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.211711s(649.183Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.215178s(638.722Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.223452s(615.072Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.202687s(678.085Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.203175s(676.455Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.225790s(608.702Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.204435s(672.287Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.217666s(631.421Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.217374s(632.270Gflops) Average Gflops: 642.996, Max Gflops: 678.085 Average Time: 0.214040s, Min Time: 0.202687s PS D:\\example\\efficiency_v3\\c\\mkl\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/mkl/build/Release/mkl_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.400238s(343.393Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.365257s(376.280Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.375613s(365.906Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.353108s(389.226Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.380444s(361.260Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.381736s(360.036Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.392378s(350.272Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.382949s(358.897Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.401440s(342.365Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.413794s(332.143Gflops) Average Gflops: 357.978, Max Gflops: 389.226 Average Time: 0.384696s, Min Time: 0.353108s To run Intel mkl programs properly on AMD machines, it is recommended to add MKL_DEBUG_CPU_TYPE=5 to the environment variables.\nThe results of running on an Intel workstation (Xeon Gold 6226R) are as follows:\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./mkl_msvc_clang_19.1.5.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 4.505398s(1952.345Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 4.513220s(1948.962Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 4.472770s(1966.587Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 4.478043s(1964.272Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 4.487709s(1960.041Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 4.467270s(1969.009Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 4.399587s(1999.299Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 4.511694s(1949.621Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 4.477332s(1964.584Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 4.459314s(1972.521Gflops) Average Gflops: 1964.724, Max Gflops: 1999.299 Average Time: 4.477234s, Min Time: 4.399587s PS E:\\example\\efficiency_v3\\run\u0026gt; ./mkl_msvc_clang_19.1.5.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 10.416302s(844.455Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 10.291459s(854.698Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 10.229627s(859.865Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 10.319844s(852.347Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 10.277702s(855.842Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 10.134144s(867.966Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 10.602970s(829.588Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 10.421812s(844.008Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 10.096355s(871.215Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 10.168106s(865.067Gflops) Average Gflops: 854.505, Max Gflops: 871.215 Average Time: 10.295832s, Min Time: 10.096355s For BLAS, the smaller the matrix size, the less obvious the acceleration effect. However, the larger the matrix size, the better. There is always an upper limit. The appropriate matrix size depends on the performance of the CPU and cache. Different platforms are suitable for different matrix operation scales.\n1.2 OpenBLAS Considering that Intel\u0026rsquo;s library may have a lower execution efficiency on AMD platforms, we will try to replace mkl with openblas. Here we only change the header file referenced by blas.c, and keep the rest unchanged:\n#include \u0026lt;cblas.h\u0026gt; void matrix_multiply_float(int n, float A[], float B[], float C[]) { cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, n, n, n, 1.0, A, n, B, n, 0.0, C, n); } void matrix_multiply_double(int n, double A[], double B[], double C[]) { cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, n, n, n, 1.0, A, n, B, n, 0.0, C, n); } The CMakeLists.txt configuration file contains openblas and openmp. Yes, the underlying layer of openblas is also parallelized through openmp.\ncmake_minimum_required(VERSION 3.13) project(openblas LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable OpenBLAS and OpenMP find_package(OpenMP REQUIRED) find_package(OpenBLAS CONFIG REQUIRED) set(SRC_LIST src/main.c src/blas.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE OpenBLAS::OpenBLAS OpenMP::OpenMP_C ) Compile and run on an AMD machine, and use MSYS2\u0026rsquo;s ucrt64 toolchain to compile and output the Release program under Windows. The execution effect is as follows:\nPS D:\\example\\efficiency_v3\\c\\openblas\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openblas/build/openblas_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.155257s(885.234Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.154143s(891.633Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.159396s(862.251Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.159194s(863.341Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.154769s(888.027Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.160298s(857.398Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.158431s(867.502Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.153623s(894.649Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.164457s(835.712Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.145844s(942.366Gflops) Average Gflops: 878.811, Max Gflops: 942.366 Average Time: 0.156541s, Min Time: 0.145844s PS D:\\example\\efficiency_v3\\c\\openblas\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openblas/build/openblas_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.342961s(400.743Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.366820s(374.677Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.331864s(414.143Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.324773s(423.185Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.370822s(370.634Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.387845s(354.365Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.353893s(388.363Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.344588s(398.851Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.359606s(382.193Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.379676s(361.990Gflops) Average Gflops: 386.914, Max Gflops: 423.185 Average Time: 0.356285s, Min Time: 0.324773s The performance of openblas on the AMD platform is indeed much better than that of Intel mkl.\nThe results of running on an Intel workstation are as follows:\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./openblas_gnu_gnu_15.1.0.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 5.334670s(1648.854Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 5.288424s(1663.273Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 5.288376s(1663.288Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 5.280766s(1665.685Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 5.203108s(1690.546Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 5.283861s(1664.709Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 5.313183s(1655.522Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 5.247752s(1676.164Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 5.229815s(1681.913Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 5.205896s(1689.641Gflops) Average Gflops: 1669.960, Max Gflops: 1690.546 Average Time: 5.267585s, Min Time: 5.203108s PS E:\\example\\efficiency_v3\\run\u0026gt; ./openblas_gnu_gnu_15.1.0.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 12.076276s(728.378Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 12.136422s(724.768Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 12.155159s(723.651Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 12.246918s(718.229Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 12.240157s(718.626Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 12.205895s(720.643Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 12.001869s(732.894Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 12.127655s(725.292Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 12.195832s(721.238Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 12.124491s(725.481Gflops) Average Gflops: 723.920, Max Gflops: 732.894 Average Time: 12.151067s, Min Time: 12.001869s Openblas performs worse than mkl on Intel platform.\n1.3 AMD AOCL Next, let\u0026rsquo;s see how AMD\u0026rsquo;s own blas library performs. AOCL1 is a CPU optimization library launched by AMD for its own platform. It contains library files related to mathematical operations such as blas, lapack, and fftw, which are used to accelerate mathematical calculation tasks on AMD CPUs. As in 1.2, the header file is also cblas.h, and this time even blas.c does not need to be changed. CMakeLists.txt is as follows. When configuring cmake, you need to pass the AOCL installation path AOCL_DIR parameter, and the data model AOCL_BLAS_DATA uses LP64 by default. The AOCL used for demonstration is version 5.1 under Windows. Other versions may need to be adjusted according to the installation package file.\ncmake_minimum_required(VERSION 3.13) project(aocl LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # setting aocl directory if(DEFINED AOCL_DIR) message(STATUS \u0026#34;AOCL_DIR is set to: ${AOCL_DIR}\u0026#34;) else() message(FATAL_ERROR \u0026#34;AOCL_DIR is not defined. Please set it to the AOCL installation directory.\u0026#34;) endif() if(NOT DEFINED AOCL_BLAS_DATA) set(AOCL_BLAS_DATA LP64) message(STATUS \u0026#34;AOCL_BLAS_DATA is not defined. Use default value: ${AOCL_BLAS_DATA}\u0026#34;) else() message(STATUS \u0026#34;AOCL_BLAS_DATA is set to: ${AOCL_BLAS_DATA}\u0026#34;) endif() # Enable AOCL BLAS set(AOCL_BLIS_INCLUDE_DIRS ${AOCL_DIR}/amd-blis/include/${AOCL_BLAS_DATA}) set(AOCL_BLIS_LINK_DIR ${AOCL_DIR}/amd-blis/lib/${AOCL_BLAS_DATA}) set(AOCL_BLIS_LIBS AOCL-LibBlis-Win-MT.lib) # find OpenMP find_package(OpenMP REQUIRED) set(SRC_LIST src/main.c src/blas.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${AOCL_BLIS_INCLUDE_DIRS} ) target_link_directories(${EXECUTE_FILE_NAME} PRIVATE ${AOCL_BLIS_LINK_DIR} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ${AOCL_BLIS_LIBS} OpenMP::OpenMP_C ) Compile using clang-cl of vs2022 on an AMD machine. The parameters passed here are:\nPS D:\\example\\efficiency_v3\\c\\aocl\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/aocl/build/Release/aocl_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.174192s(789.010Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.173520s(792.065Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.207043s(663.819Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.174526s(787.497Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.179918s(763.899Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.154489s(889.637Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.157877s(870.547Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.165565s(830.119Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.156284s(879.416Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.207975s(660.843Gflops) Average Gflops: 792.685, Max Gflops: 889.637 Average Time: 0.175139s, Min Time: 0.154489s PS D:\\example\\efficiency_v3\\c\\aocl\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/aocl/build/Release/aocl_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.307589s(446.827Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.348955s(393.859Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.292173s(470.403Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.296039s(464.259Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.291551s(471.407Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.316478s(434.277Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.306831s(447.931Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.302522s(454.311Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.291020s(472.266Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.301170s(456.351Gflops) Average Gflops: 451.189, Max Gflops: 472.266 Average Time: 0.305433s, Min Time: 0.291020s On AMD machines, the single-precision floating point performance is slightly inferior to OpenBLAS, the double-precision floating point performance is better than OpenBLAS, and the overall performance is stronger than Intel MKL.\nThe running results on Intel workstations are as follows:\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./aocl_msvc_clang_19.1.5.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 5.475668s(1606.396Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 5.130221s(1714.564Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 5.183950s(1696.794Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 5.265062s(1670.653Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 5.232872s(1680.930Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 4.979762s(1766.368Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 5.069269s(1735.180Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 5.059971s(1738.368Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 5.063930s(1737.009Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 5.029378s(1748.942Gflops) Average Gflops: 1709.521, Max Gflops: 1766.368 Average Time: 5.149008s, Min Time: 4.979762s PS E:\\example\\efficiency_v3\\run\u0026gt; ./aocl_msvc_clang_19.1.5.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 10.912167s(806.081Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 10.452268s(841.549Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 10.402997s(845.535Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 10.937584s(804.208Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 10.529708s(835.360Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 10.834290s(811.875Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 10.291204s(854.720Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 10.675223s(823.973Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 10.651213s(825.830Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 10.510095s(836.918Gflops) Average Gflops: 828.605, Max Gflops: 854.720 Average Time: 10.619675s, Min Time: 10.291204s The performance of aocl on Intel platform is even better than openblas and second only to mkl.\n2. GPU parallel accelerated BLAS library 2.1 CUDA cuda is a high-performance computing toolkit developed by Nvidia specifically for its own graphics cards, which includes cublas to implement blas functions. The main.c main program file remains unchanged, and a new cuda.cu file is created to implement cuda accelerated matrix multiplication operations.\n#include \u0026lt;cublas_v2.h\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; extern \u0026#34;C\u0026#34; void matrix_multiply_float(int n, float A[], float B[], float C[]) { cublasStatus_t status; cublasHandle_t handle; float *d_A = 0; float *d_B = 0; float *d_C = 0; float alpha = 1.0; float beta = 0.0; cudaMalloc((void **)\u0026amp;d_A, n * n * sizeof(d_A[0])); cudaMalloc((void **)\u0026amp;d_B, n * n * sizeof(d_B[0])); cudaMalloc((void **)\u0026amp;d_C, n * n * sizeof(d_C[0])); cublasSetVector(n * n, sizeof(*A), A, 1, d_A, 1); cublasSetVector(n * n, sizeof(*B), B, 1, d_B, 1); cublasSetVector(n * n, sizeof(*C), C, 1, d_C, 1); status = cublasCreate(\u0026amp;handle); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! CUBLAS initialization error\\n\u0026#34;); return; } status = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, n, n, n, \u0026amp;alpha, d_A, n, d_B, n, \u0026amp;beta, d_C, n); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! CUBLAS Sgemm error\\n\u0026#34;); return; } cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); status = cublasDestroy(handle); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! shutdown error (A)\\n\u0026#34;); return; } } extern \u0026#34;C\u0026#34; void matrix_multiply_double(int n, double A[], double B[], double C[]) { cublasStatus_t status; cublasHandle_t handle; double *d_A = 0; double *d_B = 0; double *d_C = 0; double alpha = 1.0; double beta = 0.0; cudaMalloc((void **)\u0026amp;d_A, n * n * sizeof(d_A[0])); cudaMalloc((void **)\u0026amp;d_B, n * n * sizeof(d_B[0])); cudaMalloc((void **)\u0026amp;d_C, n * n * sizeof(d_C[0])); cublasSetVector(n * n, sizeof(*A), A, 1, d_A, 1); cublasSetVector(n * n, sizeof(*B), B, 1, d_B, 1); cublasSetVector(n * n, sizeof(*C), C, 1, d_C, 1); status = cublasCreate(\u0026amp;handle); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! CUBLAS initialization error\\n\u0026#34;); return; } status = cublasDgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, n, n, n, \u0026amp;alpha, d_A, n, d_B, n, \u0026amp;beta, d_C, n); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! CUBLAS Dgemm error\\n\u0026#34;); return; } cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); status = cublasDestroy(handle); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! shutdown error (A)\\n\u0026#34;); return; } } The CMakeLists.txt file is as follows. CMake will automatically call nvcc to compile the cuda source file, which needs to be linked to the cublas library and the cuda runtime.\ncmake_minimum_required(VERSION 3.13) project(cuda LANGUAGES C) set(CMAKE_C_STANDARD 11) enable_language(CUDA) set(CMAKE_CUDA_ARCHITECTURES OFF) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) set(SRC_LIST src/main.c src/cuda.cu ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE cublas cudart ) The cuda toolkit version 12.9 is used for compilation here, and the compiler is msvc of vs2022. The running effect on the NVIDIA RTX A4000 single-card workstation is as follows:\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./cuda_msvc_msvc_19.44.35209.0.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 2.473336s(3556.369Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 2.124057s(4141.175Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 2.141353s(4107.727Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 2.164315s(4064.146Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 2.146332s(4098.198Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 2.083143s(4222.510Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 2.092037s(4204.559Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 2.200659s(3997.026Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 2.104675s(4179.311Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 2.177780s(4039.018Gflops) Average Gflops: 4061.004, Max Gflops: 4222.510 Average Time: 2.170769s, Min Time: 2.083143s PS E:\\example\\efficiency_v3\\run\u0026gt; ./cuda_msvc_msvc_19.44.35209.0.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 31.435958s(279.810Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 32.104813s(273.981Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 33.338183s(263.844Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 33.624088s(261.601Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 33.429806s(263.121Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 33.508910s(262.500Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 33.479590s(262.730Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 33.880561s(259.621Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 33.770107s(260.470Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 33.821256s(260.076Gflops) Average Gflops: 264.775, Max Gflops: 279.810 Average Time: 33.239327s, Min Time: 31.435958s It can only be said that the single-precision floating-point computing performance is indeed very powerful, and the double-precision floating-point computing performance is not as good as CPU acceleration.\nIn order to ensure the universality of the main.c file, cuda.cu writes the video memory allocation program inside the calculation function. In fact, after deducting this part of the overhead, the actual computing performance should be higher. However, this is also in line with programming habits. Generally, high-performance computing programs package different mathematical libraries into separate modules, separate from the main program, and switch the appropriate module according to the runtime. Therefore, the running time mainly counts the computing time of the main program calling different modules. The main program writer does not need to care about how each module is executed, but only needs to care about the output results and performance.\n2.2 OpenCL Opencl not only supports Nvidia, but also AMD and Intel products. Although graphics card manufacturers all claim to support opencl, Nvidia has cuda as a moat. Before the release of rocm, AMD lacked a suite with the same functions for a long time and could only rely on opencl to achieve a similar ecosystem. And at this stage, rocm only supports a few high-end graphics cards from AMD, unlike cuda which has relatively wide support.\nThere are two main blas libraries implemented using opencl: one is clblas, which has been stopped for a long time; the other is clblast, which can be used as a substitute for clblas at this stage.\nAs before, main.c is universal, and a new clblast.c file is created to call clblast to implement matrix multiplication operations.\n// Reference : https://github.com/CNugteren/CLBlast/blob/master/samples/sgemm.c #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;clblast_c.h\u0026gt; #define CL_TARGET_OPENCL_VERSION 120 #define CL_USE_DEPRECATED_OPENCL_1_2_APIS void matrix_multiply_float(int n, float A[], float B[], float C[]) { const size_t platform_id = 0; const size_t device_id = 0; const float alpha = 0.7f; const float beta = 1.0f; cl_uint num_platforms; clGetPlatformIDs(0, NULL, \u0026amp;num_platforms); cl_platform_id *platforms = (cl_platform_id *)malloc(num_platforms * sizeof(cl_platform_id)); clGetPlatformIDs(num_platforms, platforms, NULL); cl_platform_id platform = platforms[platform_id]; cl_uint num_devices; clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, 0, NULL, \u0026amp;num_devices); cl_device_id *devices = (cl_device_id *)malloc(num_devices * sizeof(cl_device_id)); clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, num_devices, devices, NULL); cl_device_id device = devices[device_id]; cl_context context = clCreateContext(NULL, 1, \u0026amp;device, NULL, NULL, NULL); cl_command_queue queue = clCreateCommandQueue(context, device, 0, NULL); cl_event event = NULL; cl_mem device_a = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(float), NULL, NULL); cl_mem device_b = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(float), NULL, NULL); cl_mem device_c = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(float), NULL, NULL); clEnqueueWriteBuffer(queue, device_a, CL_TRUE, 0, n * n * sizeof(float), A, 0, NULL, NULL); clEnqueueWriteBuffer(queue, device_b, CL_TRUE, 0, n * n * sizeof(float), B, 0, NULL, NULL); clEnqueueWriteBuffer(queue, device_c, CL_TRUE, 0, n * n * sizeof(float), C, 0, NULL, NULL); CLBlastStatusCode status = CLBlastSgemm(CLBlastLayoutRowMajor, CLBlastTransposeNo, CLBlastTransposeNo, n, n, n, alpha, device_a, 0, n, device_b, 0, n, beta, device_c, 0, n, \u0026amp;queue, \u0026amp;event); if (status == CLBlastSuccess) { clWaitForEvents(1, \u0026amp;event); clReleaseEvent(event); } else { fprintf(stderr, \u0026#34;Error! CLBlast failed with status %d\\n\u0026#34;, status); } free(platforms); free(devices); clReleaseMemObject(device_a); clReleaseMemObject(device_b); clReleaseMemObject(device_c); clReleaseCommandQueue(queue); clReleaseContext(context); } void matrix_multiply_double(int n, double A[], double B[], double C[]) { const size_t platform_id = 0; const size_t device_id = 0; const double alpha = 0.7f; const double beta = 1.0f; cl_uint num_platforms; clGetPlatformIDs(0, NULL, \u0026amp;num_platforms); cl_platform_id *platforms = (cl_platform_id *)malloc(num_platforms * sizeof(cl_platform_id)); clGetPlatformIDs(num_platforms, platforms, NULL); cl_platform_id platform = platforms[platform_id]; cl_uint num_devices; clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, 0, NULL, \u0026amp;num_devices); cl_device_id *devices = (cl_device_id *)malloc(num_devices * sizeof(cl_device_id)); clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, num_devices, devices, NULL); cl_device_id device = devices[device_id]; cl_context context = clCreateContext(NULL, 1, \u0026amp;device, NULL, NULL, NULL); cl_command_queue queue = clCreateCommandQueue(context, device, 0, NULL); cl_event event = NULL; cl_mem device_a = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(double), NULL, NULL); cl_mem device_b = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(double), NULL, NULL); cl_mem device_c = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(double), NULL, NULL); clEnqueueWriteBuffer(queue, device_a, CL_TRUE, 0, n * n * sizeof(double), A, 0, NULL, NULL); clEnqueueWriteBuffer(queue, device_b, CL_TRUE, 0, n * n * sizeof(double), B, 0, NULL, NULL); clEnqueueWriteBuffer(queue, device_c, CL_TRUE, 0, n * n * sizeof(double), C, 0, NULL, NULL); CLBlastStatusCode status = CLBlastDgemm(CLBlastLayoutRowMajor, CLBlastTransposeNo, CLBlastTransposeNo, n, n, n, alpha, device_a, 0, n, device_b, 0, n, beta, device_c, 0, n, \u0026amp;queue, \u0026amp;event); if (status == CLBlastSuccess) { clWaitForEvents(1, \u0026amp;event); clReleaseEvent(event); } else { fprintf(stderr, \u0026#34;Error! CLBlast failed with status %d\\n\u0026#34;, status); } free(platforms); free(devices); clReleaseMemObject(device_a); clReleaseMemObject(device_b); clReleaseMemObject(device_c); clReleaseCommandQueue(queue); clReleaseContext(context); } Just like cuda, minus the overhead of video memory allocation and preheating, the actual computing time will only be shorter.\nThe contents of the CMakeLists.txt file are as follows, and need to be linked to opencl:\ncmake_minimum_required(VERSION 3.13) project(opencl LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable clBlast find_package(clBlast REQUIRED) set(SRC_LIST src/main.c src/clblast.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE clBlast opencl ) The compilation toolchain uses MSYS2\u0026rsquo;s ucrt64, and the compilation machine is configured with AMD Radeon 880M core graphics. The running results on the compilation machine are as follows:\nPS D:\\example\\efficiency_v3\\run\u0026gt; ./opencl_gnu_gnu_15.1.0.exe -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 1.117597s(122.977Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.186813s(735.703Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.162170s(847.499Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.168798s(814.219Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.159344s(862.531Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.162423s(846.179Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.180195s(762.723Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.156903s(875.949Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.170804s(804.659Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.150039s(916.018Gflops) Average Gflops: 758.846, Max Gflops: 916.018 Average Time: 0.261509s, Min Time: 0.150039s PS D:\\example\\efficiency_v3\\run\u0026gt; ./opencl_gnu_gnu_15.1.0.exe -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 2.102548s(65.368Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 1.159928s(118.489Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 1.173824s(117.087Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 1.161465s(118.332Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 1.159717s(118.511Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 1.172199s(117.249Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 1.157075s(118.781Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 1.178716s(116.601Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 1.162563s(118.221Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 1.175523s(116.917Gflops) Average Gflops: 112.556, Max Gflops: 118.781 Average Time: 1.260356s, Min Time: 1.157075s The single-precision floating-point computing performance is equivalent to the CPU acceleration performance, and the double-precision floating-point computing performance is only about 1/4 of the CPU acceleration performance.\nThe running results on the NVIDIA RTX A4000 single-card workstation are as follows:\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./opencl_gnu_gnu_15.1.0.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 6.913470s(1272.312Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 6.030729s(1458.545Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 6.100249s(1441.924Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 6.059375s(1451.650Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 6.045835s(1454.901Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 6.013252s(1462.785Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 6.010330s(1463.496Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 6.043099s(1455.560Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 6.087911s(1444.846Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 6.010101s(1463.552Gflops) Average Gflops: 1436.957, Max Gflops: 1463.552 Average Time: 6.131435s, Min Time: 6.010101s PS E:\\example\\efficiency_v3\\run\u0026gt; ./opencl_gnu_gnu_15.1.0.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 30.209699s(291.168Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 32.432424s(271.213Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 32.288050s(272.426Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 32.304304s(272.289Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 32.392134s(271.550Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 32.370801s(271.729Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 32.440887s(271.142Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 32.515872s(270.517Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 32.635461s(269.526Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 32.786553s(268.284Gflops) Average Gflops: 272.984, Max Gflops: 291.168 Average Time: 32.237619s, Min Time: 30.209699s The single-precision floating-point operation is much worse than that of CUDA, only about 1/3 of CUDA, and even worse than MKL; the double-precision is slightly better than CUDA.\n3. Summarize The main advantage of GPU acceleration lies in large-scale single-precision floating-point calculations. Due to the long preheating time of the video memory, the advantage of GPU acceleration is not obvious for small-scale calculations. For ultra-large-scale calculations, it is necessary to consider the limitations of video memory size and memory exchange capacity. For double-precision floating-point calculations, the GPU acceleration effect is not obvious, and is not even as significant as the CPU acceleration performance improvement.\nIf you only consider the CPU acceleration library, it is best to choose the math library of the corresponding manufacturer according to the CPU platform. For example, Intel platform is suitable for mkl, AMD platform AOCL is the first choice, and of course openblas can also be used as a cross-platform alternative for general devices. A more reasonable approach is to develop specific computing modules for different platforms, and then flexibly schedule them at runtime to ensure optimal performance on each platform.\nAMD Optimizing CPU Libraries (AOCL) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-06-24-matrix_multiplication-2/","tags":[{"LinkTitle":"C++","RelPermalink":"/tags/c++/"}],"title":"Matrix multiplication operation (Ⅱ) - Accelerated operation based on BLAS library"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/categories/code/"}],"content":"Speaking of matrices, anyone who studies science and engineering will think of the fear of being dominated by linear algebra classes. Matrix multiplication operations are indispensable for various industrial and scientific research numerical calculations, and are also used in various benchmarking software. The time consumption of matrix multiplication operations is also an important indicator for judging the floating-point operation performance of computers. The purpose of this article is to verify the performance differences of various implementation methods through matrix multiplication operations, and compare the performance differences of different computing platforms to provide a reference for high-performance computing development.\n1. Matrix multiplication algorithm Matrix multiplication is also called matrix dot multiplication, which can be represented by the following figure 1. It is usually formed by multiplying and accumulating the elements of the corresponding rows of matrix A and the corresponding columns of matrix B to form the values of the corresponding rows and columns of matrix C. The number of columns of matrix A must be equal to the number of rows of matrix B, and the size of matrix C is equal to the number of rows of matrix B × the number of columns of matrix A. Using C language to represent an M×N matrix dot multiplication with an N×T matrix, it is generally written in the form of 3 layers of nested loops.\n... for (int i = 0; i \u0026lt; M; i++) { for (int j = 0; j \u0026lt; T; j++) { C[i,j] = 0.0; for (int k = 0; k \u0026lt; N; k++) { C[i,j] += A[i,k] * B[k,j]; } } } ... It is generally believed that the number of multiplications of an N×N dimensional matrix is ​​N to the power of 3, and the computational complexity is expressed as O(n3). There are also some algorithms that convert some of the multiplication operations into additions, reducing the number of multiplication operations. For example, the Strassen algorithm2 has a theoretical computational complexity of only O(n2.807), which reduces the time spent on multiplying large matrices. Newer algorithms such as the Coppersmith-Winograd method are said to have a computational complexity of only O(n2.3727). This difference in algorithms is not discussed in this article.\n2. Parallelization of nested loops Modern computer processors are generally multi-core. To fully utilize the processor performance, it is a good option to use parallel libraries such as OpenMP to parallelize the calculation program. It is meaningless to use a single core to solve the nested loop. I will not demonstrate it here. If you are interested, you can remove the parallel library from the following code and compile and calculate it.\n2.1 C Implementation The C implementation is very simple, using three layers of nested loops to implement the matrix multiplication algorithm. Here, openmp is used to merge the first two layers of loops and assign them to different threads for calculation 3, so as to fully utilize the performance of multi-core processors. The following is the sample code of openmp.c.\n#include \u0026lt;omp.h\u0026gt; void matrix_multiply_float(int n, float A[], float B[], float C[]) { #pragma omp parallel for collapse(2) shared(A, B, C) for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { C[i * n + j] = 0; for (int k = 0; k \u0026lt; n; k++) { C[i * n + j] += A[i * n + k] * B[k * n + j]; } } } } void matrix_multiply_double(int n, double A[], double B[], double C[]) { #pragma omp parallel for collapse(2) shared(A, B, C) for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { C[i * n + j] = 0; for (int k = 0; k \u0026lt; n; k++) { C[i * n + j] += A[i * n + k] * B[k * n + j]; } } } } In main.c, some command line switches are defined, and users can customize matrix size, number of loops, calculation accuracy, etc. A performance parameter, GFLOPs, is defined here to measure the ability to perform floating-point operations per unit time. 1 GFLOPs is equivalent to performing 109 floating-point operations in 1 second.\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;time.h\u0026gt; #include \u0026lt;math.h\u0026gt; #define MAX(a, b) ((a) \u0026gt; (b) ? (a) : (b)) #define MIN(a, b) ((a) \u0026lt; (b) ? (a) : (b)) extern void matrix_multiply_float(int n, float A[], float B[], float C[]); extern void matrix_multiply_double(int n, double A[], double B[], double C[]); // Initialize matrix void initialize_matrix_float(int n, float matrix[]) { srand((unsigned)time(NULL)); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { matrix[i * n + j] = rand() / (float)(RAND_MAX); } } } void initialize_matrix_double(int n, double matrix[]) { srand((unsigned)time(NULL)); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { matrix[i * n + j] = rand() / (double)(RAND_MAX); } } } // Execute matrix multiply and print results int execute_float(int dim, int loop_num, double *ave_gflops, double *max_gflops, double *ave_time, double *min_time) { // Use volatile to prevent compiler optimizations volatile float *a, *b, *c; struct timespec start_ns, end_ns; double cpu_time; for (int i = 0; i \u0026lt; loop_num; i++) { a = (float *)malloc(dim * dim * sizeof(float)); b = (float *)malloc(dim * dim * sizeof(float)); c = (float *)malloc(dim * dim * sizeof(float)); if (a == NULL || b == NULL || c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } initialize_matrix_float(dim, a); initialize_matrix_float(dim, b); timespec_get(\u0026amp;start_ns, TIME_UTC); matrix_multiply_float(dim, a, b, c); timespec_get(\u0026amp;end_ns, TIME_UTC); cpu_time = (end_ns.tv_sec - start_ns.tv_sec) + (end_ns.tv_nsec - start_ns.tv_nsec) / 1e9; double gflops = 1e-9 * dim * dim * dim * 2 / cpu_time; printf(\u0026#34;%d\\t: %d x %d Matrix multiply wall time : %.6fs(%.3fGflops)\\n\u0026#34;, i + 1, dim, dim, cpu_time, gflops); free(a); free(b); free(c); *ave_gflops += gflops; *max_gflops = MAX(*max_gflops, gflops); *ave_time += cpu_time; *min_time = MIN(*min_time, cpu_time); } *ave_gflops /= loop_num; *ave_time /= loop_num; return 1; } int execute_double(int dim, int loop_num, double *ave_gflops, double *max_gflops, double *ave_time, double *min_time) { // Use volatile to prevent compiler optimizations volatile double *a, *b, *c; struct timespec start_ns, end_ns; double cpu_time; for (int i = 0; i \u0026lt; loop_num; i++) { a = (double *)malloc(dim * dim * sizeof(double)); b = (double *)malloc(dim * dim * sizeof(double)); c = (double *)malloc(dim * dim * sizeof(double)); if (a == NULL || b == NULL || c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } initialize_matrix_double(dim, a); initialize_matrix_double(dim, b); timespec_get(\u0026amp;start_ns, TIME_UTC); matrix_multiply_double(dim, a, b, c); timespec_get(\u0026amp;end_ns, TIME_UTC); cpu_time = (end_ns.tv_sec - start_ns.tv_sec) + (end_ns.tv_nsec - start_ns.tv_nsec) / 1e9; double gflops = 1e-9 * dim * dim * dim * 2 / cpu_time; printf(\u0026#34;%d\\t: %d x %d Matrix multiply wall time : %.6fs(%.3fGflops)\\n\u0026#34;, i + 1, dim, dim, cpu_time, gflops); free(a); free(b); free(c); *ave_gflops += gflops; *max_gflops = MAX(*max_gflops, gflops); *ave_time += cpu_time; *min_time = MIN(*min_time, cpu_time); } *ave_gflops /= loop_num; *ave_time /= loop_num; return 1; } int main(int argc, char *argv[]) { int n = 10;\t// Default matrix size exponent int loop_num = 5;\t// Number of iterations for averaging double ave_gflops = 0.0, max_gflops = 0.0; // Average and maximum Gflops double ave_time = 0.0, min_time = 1e9;\t// Average and minimum time int use_double = 0;\t// Default to float precision // Help message if (argc == 1 || (argc == 2 \u0026amp;\u0026amp; (strcmp(argv[1], \u0026#34;-h\u0026#34;) == 0 || strcmp(argv[1], \u0026#34;--help\u0026#34;) == 0))) { printf(\u0026#34;Usage: %s [-n SIZE] [-l LOOP_NUM] [-float|-double]\\n\u0026#34;, argv[0]); printf(\u0026#34; -n SIZE Specify matrix size, like 2^SIZE (default: 10)\\n\u0026#34;); printf(\u0026#34; -l LOOP_NUM Specify number of iterations (default: 5)\\n\u0026#34;); printf(\u0026#34; -float Use float precision (default)\\n\u0026#34;); printf(\u0026#34; -double Use double precision\\n\u0026#34;); printf(\u0026#34; -h, --help Show this help message\\n\u0026#34;); return 0; } // Parse -n, -l, -float, -double options int double_flag = 0, float_flag = 0; for (int argi = 1; argi \u0026lt; argc; ++argi) { if (strcmp(argv[argi], \u0026#34;-n\u0026#34;) == 0 \u0026amp;\u0026amp; argi + 1 \u0026lt; argc) { n = atoi(argv[argi + 1]); argi++; } else if (strcmp(argv[argi], \u0026#34;-l\u0026#34;) == 0 \u0026amp;\u0026amp; argi + 1 \u0026lt; argc) { loop_num = atoi(argv[argi + 1]); argi++; } else if (strcmp(argv[argi], \u0026#34;-double\u0026#34;) == 0) { double_flag = 1; } else if (strcmp(argv[argi], \u0026#34;-float\u0026#34;) == 0) { float_flag = 1; } } if (double_flag \u0026amp;\u0026amp; float_flag) { fprintf(stderr, \u0026#34;Error: Cannot specify both -double and -float options.\\n\u0026#34;); return 1; } use_double = double_flag ? 1 : 0; int dim = (int)pow(2, n); if (use_double) { printf(\u0026#34;Using double precision for matrix multiplication.\\n\u0026#34;); execute_double(dim, loop_num, \u0026amp;ave_gflops, \u0026amp;max_gflops, \u0026amp;ave_time, \u0026amp;min_time); } else { printf(\u0026#34;Using float precision for matrix multiplication.\\n\u0026#34;); execute_float(dim, loop_num, \u0026amp;ave_gflops, \u0026amp;max_gflops, \u0026amp;ave_time, \u0026amp;min_time); } printf(\u0026#34;Average Gflops: %.3f, Max Gflops: %.3f\\n\u0026#34;, ave_gflops, max_gflops); printf(\u0026#34;Average Time: %.6fs, Min Time: %.6fs\\n\u0026#34;, ave_time, min_time); return 0; } CMake is used here, and CMakeLists.txt tells the compiler how to include the openmp header file and link to it.\ncmake_minimum_required(VERSION 3.13) project(openmp LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) find_package(OpenMP REQUIRED) set(SRC_LIST src/main.c src/openmp.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE OpenMP::OpenMP_C ) The Release program compiled by clang-cl of vs2022 on Windows platform has the following execution effect:\nPS D:\\example\\efficiency_v3\\c\\openmp\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/Release/openmp_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.362688s(5.921Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.304758s(7.047Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.314348s(6.832Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.294248s(7.298Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.294496s(7.292Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.306986s(6.995Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.320405s(6.702Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.278521s(7.710Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.294080s(7.302Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.338626s(6.342Gflops) Average Gflops: 6.944, Max Gflops: 7.710 Average Time: 0.310916s, Min Time: 0.278521s PS D:\\example\\efficiency_v3\\c\\openmp\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/Release/openmp_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.353909s(6.068Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.319001s(6.732Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.329514s(6.517Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.381114s(5.635Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.349447s(6.145Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.370087s(5.803Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.349626s(6.142Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.370023s(5.804Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.364005s(5.900Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.344441s(6.235Gflops) Average Gflops: 6.098, Max Gflops: 6.732 Average Time: 0.353117s, Min Time: 0.319001s Compiled using MSYS2\u0026rsquo;s clang64 toolchain, the Release program execution results are as follows:\nPS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_clang_20.1.7.exe\u0026#34; -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.344437s(6.235Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.327010s(6.567Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.367141s(5.849Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.358603s(5.988Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.362182s(5.929Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.338947s(6.336Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.334758s(6.415Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.331219s(6.484Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.343316s(6.255Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.324451s(6.619Gflops) Average Gflops: 6.268, Max Gflops: 6.619 Average Time: 0.343206s, Min Time: 0.324451s PS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_clang_20.1.7.exe\u0026#34; -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.447118s(4.803Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.423630s(5.069Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.365229s(5.880Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.355163s(6.046Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.443296s(4.844Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.310319s(6.920Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.366405s(5.861Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.369704s(5.809Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.382898s(5.608Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.367266s(5.847Gflops) Average Gflops: 5.669, Max Gflops: 6.920 Average Time: 0.383103s, Min Time: 0.310319s Switch to MSYS2\u0026rsquo;s ucrt64 toolchain to compile, and the Release program execution effect is as follows:\nPS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.308967s(6.951Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.267709s(8.022Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.278587s(7.708Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.263047s(8.164Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.262595s(8.178Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.264196s(8.128Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.273148s(7.862Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.262910s(8.168Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.277290s(7.745Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.274961s(7.810Gflops) Average Gflops: 7.874, Max Gflops: 8.178 Average Time: 0.273341s, Min Time: 0.262595s PS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.314685s(6.824Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.290665s(7.388Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.304162s(7.060Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.315041s(6.817Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.306290s(7.011Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.356495s(6.024Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.310081s(6.926Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.301022s(7.134Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.317229s(6.769Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.307106s(6.993Gflops) Average Gflops: 6.895, Max Gflops: 7.388 Average Time: 0.312278s, Min Time: 0.290665s The performance of the gcc compiler on the Windows platform is slightly better. The computing performance of the three is basically at the same level, and the difference is small and can be ignored.\nThe above are all calculated results on the AMD AI 9 365w processor, and all cores and hyperthreads are basically fully utilized during the calculation process. The calculation results shown in this article, unless otherwise specified, are the results output by running on this processor.\n2.2 Fortran Implementation Fortran has a built-in matrix operation function matmul. We will not consider the algorithm optimization of built-in functions here. The three-layer loop nesting implemented in C is implemented in Fortran, and then accelerated by openmp.\nThe function implemented by main.f90 is the same as that of main.c implemented in C.\nprogram main implicit none external matrix_multiply_float, matrix_multiply_double integer :: n = 10 integer :: loop_num = 5 real :: ave_gflops = 0.0, max_gflops = 0.0 real :: ave_time = 0.0, min_time = 1e9 logical :: use_double = .false. integer :: argi, i, dim character(len=100) :: arg logical :: double_set = .false., float_set = .false. if (command_argument_count() == 0) then call print_help() stop end if argi = 1 do while (argi \u0026lt;= command_argument_count()) call get_command_argument(argi, arg) if (trim(arg) == \u0026#39;-n\u0026#39; .and. argi + 1 \u0026lt;= command_argument_count()) then argi = argi + 1 call get_command_argument(argi, arg) read(arg, *) n else if (trim(arg) == \u0026#39;-l\u0026#39; .and. argi + 1 \u0026lt;= command_argument_count()) then argi = argi + 1 call get_command_argument(argi, arg) read(arg, *) loop_num else if (trim(arg) == \u0026#39;-double\u0026#39;) then double_set = .true. use_double = .true. else if (trim(arg) == \u0026#39;-float\u0026#39;) then float_set = .true. use_double = .false. else if (trim(arg) == \u0026#39;-h\u0026#39; .or. trim(arg) == \u0026#39;--help\u0026#39;) then call print_help() stop end if if (double_set .and. float_set) then print *, \u0026#34;Error: Cannot specify both -double and -float options.\u0026#34; stop end if argi = argi + 1 end do dim = 2**n if (use_double) then call perform_double(dim, loop_num, ave_gflops, max_gflops, ave_time, min_time) else call perform_float(dim, loop_num, ave_gflops, max_gflops, ave_time, min_time) end if ave_gflops = ave_gflops / loop_num ave_time = ave_time / loop_num print \u0026#39;(A, F8.3, A, F8.3)\u0026#39;, \u0026#39;Average Gflops: \u0026#39;, ave_gflops, \u0026#39;, Max Gflops: \u0026#39;, max_gflops print \u0026#39;(A, F10.6, A, F10.6, A)\u0026#39;, \u0026#39;Average Time: \u0026#39;, ave_time, \u0026#39;s, Min Time: \u0026#39;, min_time, \u0026#39;s\u0026#39; contains subroutine print_help() print *, \u0026#39;Usage: program_name [-n SIZE] [-l LOOP_NUM] [-float|-double]\u0026#39; print *, \u0026#39; -n SIZE Specify matrix size, like 2^SIZE (default: 10)\u0026#39; print *, \u0026#39; -l LOOP_NUM Specify number of iterations (default: 5)\u0026#39; print *, \u0026#39; -float Use real*32 precision (default)\u0026#39; print *, \u0026#39; -double Use real*64 precision\u0026#39; print *, \u0026#39; -h, --help Show this help message\u0026#39; end subroutine print_help subroutine initialize_matrix_float(n, matrix) integer, intent(in) :: n real, intent(out) :: matrix(n, n) integer :: i, j real :: rand call random_seed() do i = 1, n do j = 1, n call random_number(rand) matrix(i, j) = rand end do end do end subroutine initialize_matrix_float subroutine initialize_matrix_double(n, matrix) integer, intent(in) :: n real*8, intent(out) :: matrix(n, n) integer :: i, j real*8 :: rand call random_seed() do i = 1, n do j = 1, n call random_number(rand) matrix(i, j) = rand end do end do end subroutine initialize_matrix_double subroutine perform_double(dim, loop_num, ave_gflops, max_gflops, ave_time, min_time) integer, intent(in) :: dim, loop_num real, intent(inout) :: ave_gflops, max_gflops, ave_time real, intent(inout) :: min_time real*8, allocatable :: a_double(:, :), b_double(:, :), c_double(:, :) real :: gflops integer*8 :: i, start_count(1), end_count(1), count_rate(1) real :: elapsed_time print *, \u0026#39;Using real*64 precision for matrix multiplication.\u0026#39; allocate(a_double(dim, dim), b_double(dim, dim), c_double(dim, dim)) do i = 1, loop_num call initialize_matrix_double(dim, a_double) call initialize_matrix_double(dim, b_double) call system_clock(count=start_count(1), count_rate=count_rate(1)) call matrix_multiply_double(dim, a_double, b_double, c_double) call system_clock(count=end_count(1)) elapsed_time = real(end_count(1) - start_count(1)) / real(count_rate(1)) gflops = 1e-9 * dim * dim * dim * 2 / elapsed_time print \u0026#39;(I8, A, I0, A, I0, A, F10.6, A, F8.3, A)\u0026#39;, i, \u0026#39; : \u0026#39;, dim, \u0026#39; x \u0026#39;, dim, \u0026#39; Matrix multiply wall time : \u0026#39;, elapsed_time, \u0026#39;s(\u0026#39;, gflops, \u0026#39;Gflops)\u0026#39; ave_gflops = ave_gflops + gflops max_gflops = max(max_gflops, gflops) ave_time = ave_time + elapsed_time min_time = min(min_time, elapsed_time) end do deallocate(a_double, b_double, c_double) end subroutine perform_double subroutine perform_float(dim, loop_num, ave_gflops, max_gflops, ave_time, min_time) integer, intent(in) :: dim, loop_num real, intent(inout) :: ave_gflops, max_gflops, ave_time real, intent(inout) :: min_time real*4, allocatable :: a_float(:, :), b_float(:, :), c_float(:, :) real :: gflops integer*8 :: i, start_count(1), end_count(1), count_rate(1) real :: elapsed_time print *, \u0026#39;Using real*32 precision for matrix multiplication.\u0026#39; allocate(a_float(dim, dim), b_float(dim, dim), c_float(dim, dim)) do i = 1, loop_num call initialize_matrix_float(dim, a_float) call initialize_matrix_float(dim, b_float) call system_clock(count=start_count(1), count_rate=count_rate(1)) call matrix_multiply_float(dim, a_float, b_float, c_float) call system_clock(count=end_count(1)) elapsed_time = real(end_count(1) - start_count(1)) / real(count_rate(1)) gflops = 1e-9 * dim * dim * dim * 2 / elapsed_time print \u0026#39;(I8, A, I0, A, I0, A, F10.6, A, F8.3, A)\u0026#39;, i, \u0026#39; : \u0026#39;, dim, \u0026#39; x \u0026#39;, dim, \u0026#39; Matrix multiply wall time : \u0026#39;, elapsed_time, \u0026#39;s(\u0026#39;, gflops, \u0026#39;Gflops)\u0026#39; ave_gflops = ave_gflops + gflops max_gflops = max(max_gflops, gflops) ave_time = ave_time + elapsed_time min_time = min(min_time, elapsed_time) end do deallocate(a_float, b_float, c_float) end subroutine perform_float end program main omp.f90 is a concrete implementation of the three-layer loop algorithm, and also uses OpenMP to expand the first two layers of loops to achieve parallelization.\nsubroutine matrix_multiply_float(n, a, b, c) implicit none integer, intent(in) :: n real*4, intent(in) :: a(n, n), b(n, n) real*4, intent(out) :: c(n, n) integer :: i, j, k c = 0.0 !$omp parallel do private(i, j, k) shared(a, b, c, n) collapse(2) do i = 1, n do j = 1, n do k = 1, n c(i, j) = c(i, j) + a(i, k) * b(k, j) end do end do end do !$omp end parallel do end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, a, b, c) implicit none integer, intent(in) :: n real*8, intent(in) :: a(n, n), b(n, n) real*8, intent(out) :: c(n, n) integer :: i, j, k c = 0.0d0 !$omp parallel do private(i, j, k) shared(a, b, c, n) collapse(2) do i = 1, n do j = 1, n do k = 1, n c(i, j) = c(i, j) + a(i, k) * b(k, j) end do end do end do !$omp end parallel do end subroutine matrix_multiply_double The contents of the CMakeLists.txt file are shown below:\ncmake_minimum_required(VERSION 3.13) project(openmp-fortran LANGUAGES Fortran) set(CMAKE_Fortran_STANDARD 2008) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_Fortran_COMPILER_FRONTEND_VARIANT}_${CMAKE_Fortran_COMPILER_ID}_${CMAKE_Fortran_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable OpenMP find_package(OpenMP REQUIRED) set(SRC_LIST src/main.f90 src/omp.f90 ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE OpenMP::OpenMP_Fortran ) Compile using Intel oneAPI ifx, and the Release program execution effect is as follows:\nPS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/Release/openmp-fortran_msvc_intelllvm_2025.1.1.exe\u0026#34; -l 10 -n 10 Using real*32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.299000s( 7.182Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.284000s( 7.562Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.337000s( 6.372Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.310000s( 6.927Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.321000s( 6.690Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.358000s( 5.999Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.290000s( 7.405Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.312000s( 6.883Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.301000s( 7.134Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.290000s( 7.405Gflops) Average Gflops: 6.956, Max Gflops: 7.562 Average Time: 0.310200s, Min Time: 0.284000s PS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/Release/openmp-fortran_msvc_intelllvm_2025.1.1.exe\u0026#34; -l 10 -n 10 -double Using real*64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.391000s( 5.492Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.362000s( 5.932Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.334000s( 6.430Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.393000s( 5.464Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.331000s( 6.488Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.357000s( 6.015Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.366000s( 5.867Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.345000s( 6.225Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.341000s( 6.298Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.359000s( 5.982Gflops) Average Gflops: 6.019, Max Gflops: 6.488 Average Time: 0.357900s, Min Time: 0.331000s Compiled using MSYS2\u0026rsquo;s ucrt64 toolchain, the Release program execution results are as follows:\nPS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/openmp-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using real*32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.224624s( 9.560Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.224959s( 9.546Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.228968s( 9.379Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.238853s( 8.991Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.216879s( 9.902Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.212407s( 10.110Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.233809s( 9.185Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.240426s( 8.932Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.241291s( 8.900Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.222414s( 9.655Gflops) Average Gflops: 9.416, Max Gflops: 10.110 Average Time: 0.228463s, Min Time: 0.212407s PS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/openmp-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using real*64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.258885s( 8.295Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.299699s( 7.165Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.298025s( 7.206Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.291631s( 7.364Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.271050s( 7.923Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.330989s( 6.488Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.278846s( 7.701Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.281121s( 7.639Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.288999s( 7.431Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.318669s( 6.739Gflops) Average Gflops: 7.395, Max Gflops: 8.295 Average Time: 0.291792s, Min Time: 0.258885s The result of running on AMD processor shows that Intel ifx version is slower than gcc version, and this is also true for multiple attempts.\nThe generated program is copied to Intel workstation (Xeon Gold 6226R) and the running time of both versions is basically the same. It is doubtful whether Intel compiler has made some negative optimization for AMD platform. 2.3 Rust implementation There is no native implementation of openmp in rust, but there is a parallel library rayon with similar functions. Here, rust+rayon is used to implement a three-layer loop nesting algorithm.\nmain.rs implements the same function as main.c.\nmod matmul; use matmul::{matrix_multiply_double, matrix_multiply_float}; use clap::{Arg, ArgAction, Command}; use rand::prelude::*; use std::time::Instant; fn initialize_matrix_float(n: usize, matrix: \u0026amp;mut [f32]) { let mut rng = rand::rng(); for i in 0..n * n { matrix[i] = rng.random::\u0026lt;f32\u0026gt;(); } } fn initialize_matrix_double(n: usize, matrix: \u0026amp;mut [f64]) { let mut rng = rand::rng(); for i in 0..n * n { matrix[i] = rng.random::\u0026lt;f64\u0026gt;(); } } fn execute_float(dim: usize, loop_num: usize) -\u0026gt; (f64, f64, f64, f64) { let mut ave_gflops: f64 = 0.0; let mut max_gflops: f64 = 0.0; let mut ave_time: f64 = 0.0; let mut min_time = f64::MAX; for i in 0..loop_num { let mut a = vec![0.0; dim * dim]; let mut b = vec![0.0; dim * dim]; let mut c = vec![0.0; dim * dim]; initialize_matrix_float(dim, \u0026amp;mut a); initialize_matrix_float(dim, \u0026amp;mut b); let start = Instant::now(); matrix_multiply_float(dim, \u0026amp;a, \u0026amp;b, \u0026amp;mut c); let cpu_time = start.elapsed().as_secs_f64(); let gflops = 2.0 * (dim * dim * dim) as f64 / cpu_time / 1e9; println!( \u0026#34;{}\\t: {} x {} Matrix multiply wall time : {:.6}s({:.3}Gflops)\u0026#34;, i + 1, dim, dim, cpu_time, gflops ); ave_gflops += gflops; max_gflops = max_gflops.max(gflops); ave_time += cpu_time; min_time = min_time.min(cpu_time); } ave_gflops /= loop_num as f64; ave_time /= loop_num as f64; (ave_gflops, max_gflops, ave_time, min_time) } fn execute_double(dim: usize, loop_num: usize) -\u0026gt; (f64, f64, f64, f64) { let mut ave_gflops: f64 = 0.0; let mut max_gflops: f64 = 0.0; let mut ave_time: f64 = 0.0; let mut min_time = f64::MAX; for i in 0..loop_num { let mut a = vec![0.0; dim * dim]; let mut b = vec![0.0; dim * dim]; let mut c = vec![0.0; dim * dim]; initialize_matrix_double(dim, \u0026amp;mut a); initialize_matrix_double(dim, \u0026amp;mut b); let start = Instant::now(); matrix_multiply_double(dim, \u0026amp;a, \u0026amp;b, \u0026amp;mut c); let cpu_time = start.elapsed().as_secs_f64(); let gflops = 2.0 * (dim * dim * dim) as f64 / cpu_time / 1e9; println!( \u0026#34;{}\\t: {} x {} Matrix multiply wall time : {:.6}s({:.3}Gflops)\u0026#34;, i + 1, dim, dim, cpu_time, gflops ); ave_gflops += gflops; max_gflops = max_gflops.max(gflops); ave_time += cpu_time; min_time = min_time.min(cpu_time); } ave_gflops /= loop_num as f64; ave_time /= loop_num as f64; (ave_gflops, max_gflops, ave_time, min_time) } fn main() { let matches = Command::new(\u0026#34;rayon-rs\u0026#34;) .version(\u0026#34;0.1.0\u0026#34;) .author(\u0026#34;AndrewMoa\u0026#34;) .about(\u0026#34;Matrix multiplication benchmark\u0026#34;) .arg( Arg::new(\u0026#34;size\u0026#34;) .short(\u0026#39;n\u0026#39;) .long(\u0026#34;size\u0026#34;) .help(\u0026#34;Matrix size exponent (size = 2^n)\u0026#34;) .default_value(\u0026#34;10\u0026#34;), ) .arg( Arg::new(\u0026#34;loops\u0026#34;) .short(\u0026#39;l\u0026#39;) .long(\u0026#34;loops\u0026#34;) .help(\u0026#34;Number of iterations\u0026#34;) .default_value(\u0026#34;5\u0026#34;), ) .arg( Arg::new(\u0026#34;f64\u0026#34;) .short(\u0026#39;d\u0026#39;) .long(\u0026#34;f64\u0026#34;) .help(\u0026#34;Use float64 precision\u0026#34;) .action(ArgAction::SetTrue), ) .arg( Arg::new(\u0026#34;f32\u0026#34;) .short(\u0026#39;f\u0026#39;) .long(\u0026#34;f32\u0026#34;) .help(\u0026#34;Use float32 precision (default)\u0026#34;) .action(ArgAction::SetTrue), ) .get_matches(); let n: usize = matches .get_one::\u0026lt;String\u0026gt;(\u0026#34;size\u0026#34;) .unwrap() .parse() .expect(\u0026#34;Invalid size exponent\u0026#34;); let loop_num: usize = matches .get_one::\u0026lt;String\u0026gt;(\u0026#34;loops\u0026#34;) .unwrap() .parse() .expect(\u0026#34;Invalid loop count\u0026#34;); let use_double = matches.get_flag(\u0026#34;f64\u0026#34;); let use_float = matches.get_flag(\u0026#34;f32\u0026#34;); if use_double \u0026amp;\u0026amp; use_float { eprintln!(\u0026#34;Error: Cannot specify both --f64 and --f32\u0026#34;); std::process::exit(1); } let dim = 2usize.pow(n as u32); if use_double { println!(\u0026#34;Using f64 precision for matrix multiplication.\u0026#34;); let (ave_gflops, max_gflops, ave_time, min_time) = execute_double(dim, loop_num); println!( \u0026#34;Average Gflops: {:.3}, Max Gflops: {:.3}\u0026#34;, ave_gflops, max_gflops ); println!(\u0026#34;Average Time: {:.6}s, Min Time: {:.6}s\u0026#34;, ave_time, min_time); } else { println!(\u0026#34;Using f32 precision for matrix multiplication.\u0026#34;); let (ave_gflops, max_gflops, ave_time, min_time) = execute_float(dim, loop_num); println!( \u0026#34;Average Gflops: {:.3}, Max Gflops: {:.3}\u0026#34;, ave_gflops, max_gflops ); println!(\u0026#34;Average Time: {:.6}s, Min Time: {:.6}s\u0026#34;, ave_time, min_time); } } The specific algorithm is implemented in the matmul.rs file. Here, a two-layer loop is used instead of a three-layer loop, and then the outermost loop is expanded through rayon to achieve parallelization. The actual effect is similar to openmp.\nuse rayon::prelude::*; pub fn matrix_multiply_float(n: usize, a: \u0026amp;[f32], b: \u0026amp;[f32], c: \u0026amp;mut [f32]) { c.par_iter_mut().enumerate().for_each(|(idx, c_ij)| { let i = idx / n; let j = idx % n; *c_ij = 0.0; for k in 0..n { *c_ij += a[i * n + k] * b[k * n + j]; } }); } pub fn matrix_multiply_double(n: usize, a: \u0026amp;[f64], b: \u0026amp;[f64], c: \u0026amp;mut [f64]) { c.par_iter_mut().enumerate().for_each(|(idx, c_ij)| { let i = idx / n; let j = idx % n; *c_ij = 0.0; for k in 0..n { *c_ij += a[i * n + k] * b[k * n + j]; } }); } The configuration file Cargo.toml file is as follows:\n[package] name = \u0026#34;rayon-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] clap = { version = \u0026#34;4.5.40\u0026#34;, features = [\u0026#34;derive\u0026#34;] } rand = \u0026#34;0.9.1\u0026#34; rayon = \u0026#34;1.10.0\u0026#34; Compiled using the msvc toolchain on the Windows platform, the Release program has the following effects:\nPS D:\\example\\efficiency_v3\\rust\\rayon-rs\u0026gt; cargo run --release -- -l 10 -n 10 Finished `release` profile [optimized] target(s) in 0.04s Running `target\\release\\rayon-rs.exe -l 10 -n 10` Using f32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.183645s(11.694Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.189942s(11.306Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.186613s(11.508Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.190823s(11.254Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.197830s(10.855Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.194282s(11.053Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.199380s(10.771Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.198576s(10.814Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.190201s(11.291Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.195518s(10.984Gflops) Average Gflops: 11.153, Max Gflops: 11.694 Average Time: 0.192681s, Min Time: 0.183645s PS D:\\example\\efficiency_v3\\rust\\rayon-rs\u0026gt; cargo run --release -- -l 10 -n 10 -d Finished `release` profile [optimized] target(s) in 0.04s Running `target\\release\\rayon-rs.exe -l 10 -n 10 -d` Using f64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.188721s(11.379Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.186753s(11.499Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.206919s(10.378Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.197388s(10.879Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.196899s(10.907Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.206537s(10.398Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.200781s(10.696Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.191448s(11.217Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.192770s(11.140Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.178019s(12.063Gflops) Average Gflops: 11.056, Max Gflops: 12.063 Average Time: 0.194624s, Min Time: 0.178019s 3. 3. Matrix Block Parallelization When directly solving a large matrix, in order to improve the solving efficiency, the large matrix can be divided into small blocks for solving. This can improve the cache hit rate and improve the computing performance to a certain extent.\n3.1 Matrix block C implementation Based on the implementation in 2.1, change the contents of the openmp.c file as follows, and keep the contents of other files unchanged.\n#include \u0026lt;omp.h\u0026gt; #include \u0026lt;math.h\u0026gt; //Block size, adjusted according to cache size #define BLOCK_SIZE 8 void matrix_multiply_float(int n, float A[], float B[], float C[]) { #pragma omp parallel for collapse(2) shared(A, B, C) for (int i_block = 0; i_block \u0026lt; n; i_block += BLOCK_SIZE) { for (int j_block = 0; j_block \u0026lt; n; j_block += BLOCK_SIZE) { for (int k_block = 0; k_block \u0026lt; n; k_block += BLOCK_SIZE) { int i_end = fmin(i_block + BLOCK_SIZE, n); int j_end = fmin(j_block + BLOCK_SIZE, n); int k_end = fmin(k_block + BLOCK_SIZE, n); for (int i = i_block; i \u0026lt; i_end; i++) { for (int j = j_block; j \u0026lt; j_end; j++) { for (int k = k_block; k \u0026lt; k_end; k++) { C[i * n + j] += A[i * n + k] * B[k * n + j]; } } } } } } } void matrix_multiply_double(int n, double A[], double B[], double C[]) { #pragma omp parallel for collapse(2) shared(A, B, C) for (int i_block = 0; i_block \u0026lt; n; i_block += BLOCK_SIZE) { for (int j_block = 0; j_block \u0026lt; n; j_block += BLOCK_SIZE) { for (int k_block = 0; k_block \u0026lt; n; k_block += BLOCK_SIZE) { int i_end = fmin(i_block + BLOCK_SIZE, n); int j_end = fmin(j_block + BLOCK_SIZE, n); int k_end = fmin(k_block + BLOCK_SIZE, n); for (int i = i_block; i \u0026lt; i_end; i++) { for (int j = j_block; j \u0026lt; j_end; j++) { for (int k = k_block; k \u0026lt; k_end; k++) { C[i * n + j] += A[i * n + k] * B[k * n + j]; } } } } } } } Compiled using MSYS2\u0026rsquo;s ucrt64 toolchain on Windows, the Release program execution results are as follows:\nPS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.058133s(36.941Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.060944s(35.237Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.060845s(35.294Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.060216s(35.663Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.061135s(35.127Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.060427s(35.539Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.059523s(36.078Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.060110s(35.726Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.062199s(34.526Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.059708s(35.966Gflops) Average Gflops: 35.610, Max Gflops: 36.941 Average Time: 0.060324s, Min Time: 0.058133s PS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.065988s(32.544Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.056852s(37.773Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.059504s(36.090Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.053153s(40.402Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.057459s(37.374Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.053974s(39.787Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.055030s(39.024Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.053222s(40.349Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.054025s(39.750Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.053405s(40.211Gflops) Average Gflops: 38.330, Max Gflops: 40.402 Average Time: 0.056261s, Min Time: 0.053153s Compared with the loop nesting algorithm, the performance is greatly improved after the block division. The block size is related to the hardware platform, which directly affects the cache hit rate and thus affects the computing performance. A lot of tests are needed to determine the most appropriate block size.\n3.2 Matrix block fortran implementation Based on the content of 2.2, only the omp.f90 file is changed and the matrix blocking algorithm is added.\nsubroutine matrix_multiply_float(n, a, b, c) implicit none integer, intent(in) :: n real*4, intent(in) :: a(n, n), b(n, n) real*4, intent(out) :: c(n, n) integer :: i, j, k, bi, bj, bk, block_size real*4 :: temp ! Define block size, adjust according to cache situation block_size = 8 c = 0.0 !$omp parallel do private(bi, bj, bk, i, j, k, temp) shared(a, b, c, n, block_size) do bi = 1, n, block_size do bj = 1, n, block_size do bk = 1, n, block_size do i = bi, min(bi + block_size - 1, n) do j = bj, min(bj + block_size - 1, n) temp = c(i, j) do k = bk, min(bk + block_size - 1, n) temp = temp + a(i, k) * b(k, j) end do c(i, j) = temp end do end do end do end do end do !$omp end parallel do end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, a, b, c) implicit none integer, intent(in) :: n real*8, intent(in) :: a(n, n), b(n, n) real*8, intent(out) :: c(n, n) integer :: i, j, k, bi, bj, bk, block_size real*8 :: temp block_size = 8 c = 0.0d0 !$omp parallel do private(bi, bj, bk, i, j, k, temp) shared(a, b, c, n, block_size) do bi = 1, n, block_size do bj = 1, n, block_size do bk = 1, n, block_size do i = bi, min(bi + block_size - 1, n) do j = bj, min(bj + block_size - 1, n) temp = c(i, j) do k = bk, min(bk + block_size - 1, n) temp = temp + a(i, k) * b(k, j) end do c(i, j) = temp end do end do end do end do end do !$omp end parallel do end subroutine matrix_multiply_double Compiled using MSYS2\u0026rsquo;s ucrt64 toolchain on Windows, the Release program execution results are as follows:\nPS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/openmp-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using real*32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.091248s( 23.535Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.082953s( 25.888Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.080934s( 26.534Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.078231s( 27.451Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.076461s( 28.086Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.078698s( 27.288Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.077900s( 27.567Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.079131s( 27.138Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.083838s( 25.615Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.078336s( 27.414Gflops) Average Gflops: 26.651, Max Gflops: 28.086 Average Time: 0.080773s, Min Time: 0.076461s PS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/openmp-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using real*64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.087275s( 24.606Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.080483s( 26.682Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.079304s( 27.079Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.080373s( 26.719Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.076651s( 28.016Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.080497s( 26.678Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.097153s( 22.104Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.085253s( 25.190Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.085751s( 25.043Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.098205s( 21.867Gflops) Average Gflops: 25.399, Max Gflops: 28.016 Average Time: 0.085094s, Min Time: 0.076651s The performance improvement depends on the block size. You can adjust the block size to slowly find the most suitable parameters.\n3.3 Matrix block rust implementation Similarly, based on 2.3, only the matmul.rs file is changed here.\nuse rayon::prelude::*; // Defining the block size const BLOCK_SIZE: usize = 8; pub fn matrix_multiply_float(n: usize, a: \u0026amp;[f32], b: \u0026amp;[f32], c: \u0026amp;mut [f32]) { c.par_chunks_mut(n).enumerate().for_each(|(i, c_row)| { for bj in (0..n).step_by(BLOCK_SIZE) { for bk in (0..n).step_by(BLOCK_SIZE) { for j in bj..(bj + BLOCK_SIZE).min(n) { for k in bk..(bk + BLOCK_SIZE).min(n) { c_row[j] += a[i * n + k] * b[k * n + j]; } } } } }); } pub fn matrix_multiply_double(n: usize, a: \u0026amp;[f64], b: \u0026amp;[f64], c: \u0026amp;mut [f64]) { c.par_chunks_mut(n).enumerate().for_each(|(i, c_row)| { for bj in (0..n).step_by(BLOCK_SIZE) { for bk in (0..n).step_by(BLOCK_SIZE) { for j in bj..(bj + BLOCK_SIZE).min(n) { for k in bk..(bk + BLOCK_SIZE).min(n) { c_row[j] += a[i * n + k] * b[k * n + j]; } } } } }); } Compiled using the msvc toolchain under Windows, the Release program has the following effects:\nPS D:\\example\\efficiency_v3\\rust\\rayon-rs\u0026gt; cargo run --release -- -l 10 -n 10 Finished `release` profile [optimized] target(s) in 0.03s Running `target\\release\\rayon-rs.exe -l 10 -n 10` Using f32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.113868s(18.859Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.116698s(18.402Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.125060s(17.172Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.125669s(17.088Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.116527s(18.429Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.114784s(18.709Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.117369s(18.297Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.114227s(18.800Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.118140s(18.177Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.115756s(18.552Gflops) Average Gflops: 18.249, Max Gflops: 18.859 Average Time: 0.117810s, Min Time: 0.113868s PS D:\\example\\efficiency_v3\\rust\\rayon-rs\u0026gt; cargo run --release -- -l 10 -n 10 -d Finished `release` profile [optimized] target(s) in 0.03s Running `target\\release\\rayon-rs.exe -l 10 -n 10 -d` Using f64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.120614s(17.805Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.128292s(16.739Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.122600s(17.516Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.118252s(18.160Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.127342s(16.864Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.121794s(17.632Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.130288s(16.483Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.137106s(15.663Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.130548s(16.450Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.138826s(15.469Gflops) Average Gflops: 16.878, Max Gflops: 18.160 Average Time: 0.127566s, Min Time: 0.118252s The performance improvement is limited compared to C and Fortran, and it takes time to adjust the block size and try out the most appropriate parameters.\n4. The Black Magic of Compilers In the previous section, we have discussed the algorithm implementation using a simple loop and used some parallel libraries to accelerate the calculation. Now, we will not use parallel libraries but use the Fortran matmul function and the Rust ndarray and mathru libraries to implement matrix multiplication operations, to illustrate the importance of compiler optimization.\n4.1 Fortran built-in function matmul The content of main.f90 is consistent with that in 2.2. The content of the specific implementation file matmul.f90 is as follows.\nsubroutine matrix_multiply_float(n, a, b, c) implicit none integer :: n real*4, intent(in) :: a(n, n), b(n, n) real*4, intent(out) :: c(n, n) c = matmul(a, b) end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, a, b, c) implicit none integer :: n real*8, intent(in) :: a(n, n), b(n, n) real*8, intent(out) :: c(n, n) c = matmul(a, b) end subroutine matrix_multiply_double CMakeLists.txt removed openmp related code.\ncmake_minimum_required(VERSION 3.13) project(matmul-fortran LANGUAGES Fortran) set(CMAKE_Fortran_STANDARD 2008) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_Fortran_COMPILER_FRONTEND_VARIANT}_${CMAKE_Fortran_COMPILER_ID}_${CMAKE_Fortran_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) set(SRC_LIST src/main.f90 src/matmul.f90 ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ) Compiled using MSYS2\u0026rsquo;s ucrt64 toolchain under Windows, the Release program execution results are as follows:\nPS D:\\example\\efficiency_v3\\fortran\\matmul-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/matmul-fortran/build/matmul-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using real*32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.049251s( 43.603Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.048811s( 43.996Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.048778s( 44.026Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.048698s( 44.098Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.046176s( 46.506Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.046269s( 46.413Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.046297s( 46.385Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.046634s( 46.049Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.046198s( 46.484Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.046255s( 46.428Gflops) Average Gflops: 45.399, Max Gflops: 46.506 Average Time: 0.047337s, Min Time: 0.046176s PS D:\\example\\efficiency_v3\\fortran\\matmul-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/matmul-fortran/build/matmul-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using real*64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.094482s( 22.729Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.093445s( 22.981Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.090847s( 23.638Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.089272s( 24.055Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.088472s( 24.273Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.088759s( 24.195Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.088489s( 24.268Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.088787s( 24.187Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.089572s( 23.975Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.088757s( 24.195Gflops) Average Gflops: 23.850, Max Gflops: 24.273 Average Time: 0.090088s, Min Time: 0.088472s Hyperthreading is not used, but it beats the combination of previous loop nesting and openmp methods. This may explain why C has been popular for decades but still cannot replace Fortran.\n4.2 Rust implemented through the ndarray library ndarray is a library used by Rust to process arrays, with a built-in matrix transposition function. Unlike the matrix stored in a one-dimensional array implemented in 2.3, using the ndarray library requires converting the one-dimensional array into a two-dimensional one.\nThe main.rs file is the same as in 2.3, and the matmul.rs file is as follows.\nuse ndarray::{Array2, ArrayView2}; pub fn matrix_multiply_float(n: usize, a: \u0026amp;[f32], b: \u0026amp;[f32], c: \u0026amp;mut [f32]) { let a = ArrayView2::from_shape((n, n), a).unwrap(); let b = ArrayView2::from_shape((n, n), b).unwrap(); let mut c_mat = Array2::\u0026lt;f32\u0026gt;::zeros((n, n)); c_mat.assign(\u0026amp;a.dot(\u0026amp;b)); c.copy_from_slice(c_mat.as_slice().unwrap()); } pub fn matrix_multiply_double(n: usize, a: \u0026amp;[f64], b: \u0026amp;[f64], c: \u0026amp;mut [f64]) { let a = ArrayView2::from_shape((n, n), a).unwrap(); let b = ArrayView2::from_shape((n, n), b).unwrap(); let mut c_mat = Array2::\u0026lt;f64\u0026gt;::zeros((n, n)); c_mat.assign(\u0026amp;a.dot(\u0026amp;b)); c.copy_from_slice(c_mat.as_slice().unwrap()); } The content of the Cargo.toml file is as follows.\n[package] name = \u0026#34;ndarray-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] clap = { version = \u0026#34;4.5.40\u0026#34;, features = [\u0026#34;derive\u0026#34;] } ndarray = \u0026#34;0.16.1\u0026#34; rand = \u0026#34;0.9.1\u0026#34; Compiled using the msvc toolchain under Windows, the Release program has the following effects:\nPS D:\\example\\efficiency_v3\\rust\\ndarray-rs\u0026gt; cargo run --release -- -l 10 -n 10 Finished `release` profile [optimized] target(s) in 0.04s Running `target\\release\\ndarray-rs.exe -l 10 -n 10` Using f32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.019414s(110.617Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.020280s(105.891Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.019639s(109.346Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.020076s(106.966Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.019702s(109.000Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.019438s(110.479Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.019336s(111.061Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.018605s(115.426Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.018340s(117.096Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.019040s(112.787Gflops) Average Gflops: 110.867, Max Gflops: 117.096 Average Time: 0.019387s, Min Time: 0.018340s PS D:\\example\\efficiency_v3\\rust\\ndarray-rs\u0026gt; cargo run --release -- -l 10 -n 10 -d Finished `release` profile [optimized] target(s) in 0.04s Running `target\\release\\ndarray-rs.exe -l 10 -n 10 -d` Using f64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.041449s(51.811Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.039612s(54.213Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.039634s(54.183Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.040022s(53.657Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.038563s(55.687Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.037992s(56.524Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.038128s(56.324Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.038157s(56.280Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.038750s(55.419Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.038102s(56.361Gflops) Average Gflops: 55.046, Max Gflops: 56.524 Average Time: 0.039041s, Min Time: 0.037992s It also does not use hyperthreading, and it beats the previous loop nesting and rayon implementation combination. If the time difference caused by the conversion between one-dimensional arrays and two-dimensional arrays is not taken into account, the actual computing performance will actually be higher.\n4.3 Rust implemented through the mathru library Mathru is another Rust math library that provides many commonly used linear algebra calculation functions. The underlying algorithm can be implemented using Intel MKL or OpenBLAS.\nSimilarly, main.rs is not shown, and only the matmul.rs file is changed, as shown below.\nuse mathru::algebra::linear::matrix::General; pub fn matrix_multiply_float(n: usize, a: \u0026amp;[f32], b: \u0026amp;[f32], c: \u0026amp;mut [f32]) { let a_mat = General::new(n, n, a.to_vec()); let b_mat = General::new(n, n, b.to_vec()); let c_mat = \u0026amp;a_mat * \u0026amp;b_mat; c.copy_from_slice(\u0026amp;c_mat.convert_to_vec()); } pub fn matrix_multiply_double(n: usize, a: \u0026amp;[f64], b: \u0026amp;[f64], c: \u0026amp;mut [f64]) { let a_mat = General::new(n, n, a.to_vec()); let b_mat = General::new(n, n, b.to_vec()); let c_mat = a_mat * b_mat; c.copy_from_slice(\u0026amp;c_mat.convert_to_vec()); } The Cargo.toml file is as follows. The default implementation of mathru is used here, and Intel mkl or openblas is not used.\n[package] name = \u0026#34;mathru-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] clap = { version = \u0026#34;4.5.40\u0026#34;, features = [\u0026#34;derive\u0026#34;] } mathru = \u0026#34;0.15.5\u0026#34; rand = \u0026#34;0.9.1\u0026#34; Compiled using the msvc toolchain under Windows, the Release program execution effect is as follows:\nPS D:\\example\\efficiency_v3\\rust\\mathru-rs\u0026gt; cargo run --release -- -l 10 -n 10 Finished `release` profile [optimized] target(s) in 0.05s Running `target\\release\\mathru-rs.exe -l 10 -n 10` Using f32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.009277s(231.472Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.007753s(277.002Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.009030s(237.817Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.007585s(283.111Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.008559s(250.904Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.009235s(232.530Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.008425s(254.900Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.007451s(288.214Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.007609s(282.237Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.008545s(251.318Gflops) Average Gflops: 258.950, Max Gflops: 288.214 Average Time: 0.008347s, Min Time: 0.007451s PS D:\\example\\efficiency_v3\\rust\\mathru-rs\u0026gt; cargo run --release -- -l 10 -n 10 -d Finished `release` profile [optimized] target(s) in 0.05s Running `target\\release\\mathru-rs.exe -l 10 -n 10 -d` Using f64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.016261s(132.063Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.016281s(131.904Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.015308s(140.282Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.016065s(133.672Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.016702s(128.578Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.014126s(152.019Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.017731s(121.116Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.020181s(106.409Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.015631s(137.388Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.015181s(141.459Gflops) Average Gflops: 132.489, Max Gflops: 152.019 Average Time: 0.016347s, Min Time: 0.014126s Without using multithreading, it is much faster than ndarray, which seems to indicate that some optimizations are done at the bottom layer. Similarly, if we do not consider the conversion between one-dimensional and two-dimensional arrays, the actual computing performance should be higher.\n5. Summarize C, Fortran and Rust matrix multiplication programs accelerated by parallel libraries such as OpenMP and Rayon use the same algorithm. The performance of matrix multiplication will be slightly different according to the implementation of different platforms and compilers, but generally they are in the same order of magnitude. This simple loop parallel acceleration calculation method is suitable for occasions with small matrix size. As the matrix size increases, the overall computing performance shows a decreasing trend.\nEven if the block matrix + parallel library method is used to improve the cache hit rate, the performance improvement is still limited. Moreover, the matrix block size is related to the cache size of the computing platform, and a lot of tests are required to call out the appropriate block parameters.\nThe matrix multiplication function implemented using Fortran\u0026rsquo;s Matmul and Rust\u0026rsquo;s mathematical library, even without parallel acceleration, still leads the implementation of simple loop parallel acceleration in matrix multiplication performance. This just shows the importance of optimizing the underlying algorithm.\nIn the field of high-performance computing, in addition to the OpenMP parallel acceleration method, there are also BLAS, CUDA, MPI, etc., which will be further demonstrated in future articles.\nMatrix multiplication \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n矩阵乘法的Strassen算法详解 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n矩阵乘法并行化（使用OpenMP） \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-06-23-matrix_multiplication-1/","tags":[{"LinkTitle":"C++","RelPermalink":"/tags/c++/"},{"LinkTitle":"Fortran","RelPermalink":"/tags/fortran/"},{"LinkTitle":"Rust","RelPermalink":"/tags/rust/"}],"title":"Matrix multiplication operation (I) - using OpenMP to speed up loop calculation"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/categories/code/"}],"content":"Recently, I plan to refactor the code I wrote previously using Rust, which involves the question of how to choose a GUI interface. Rust has only been officially released for ten years, and is not as good as the old C/C++ in GUI development. There are many well-known and time-tested GUI interface libraries such as wxWidgets, qt, gtk+, etc. This article selects several Rust GUI libraries and simply implements a boundary layer calculator for horizontal comparison.\n1. slint slint has been promoting itself recently, claiming to be the next generation of GUI toolkit, which seems to be quite ambitious. Slint defines the UI interface through a custom declarative language, and can be previewed through the plugin in vscode or through the official slintpad website.\nThe cargo configuration is as follows. slint-build is used to translate the .slint file into a .rs file.\n[package] name = \u0026#34;sLayers-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] slint = \u0026#34;1.8.0\u0026#34; [build-dependencies] slint-build = \u0026#34;1.8.0\u0026#34; [profile.release] strip = true opt-level = \u0026#34;z\u0026#34; lto = true codegen-units = 1 panic = \u0026#34;abort\u0026#34; The UI interface file dialog.slint. Slint can define the parameters to be called in the UI interface, and automatically and implicitly generate set_ and get_ methods to set the values ​​of these parameters in the RS file. Similarly, the callback function defined in the interface will also automatically and implicitly generate the on_ method for calling in the RS file. It should be noted that when the parameters in the UI interface are updated in RS, the associated controls will not automatically update the display, and the control display needs to be updated manually.\nimport { Button, LineEdit, SpinBox, CheckBox, GridBox, VerticalBox } from \u0026#34;std-widgets.slint\u0026#34;; export component Dialog inherits Window { title: \u0026#34;Layers\u0026#34;; in-out property \u0026lt;string\u0026gt; tt: \u0026#34;3.0\u0026#34;; in-out property \u0026lt;int\u0026gt; nl: 5; in-out property \u0026lt;string\u0026gt; ft: \u0026#34;0.3\u0026#34;; in-out property \u0026lt;string\u0026gt; gr: \u0026#34;1.5\u0026#34;; callback calculate_first_thickness(); callback calculate_num_layers(); callback calculate_growth_rate(); callback calculate_total_thickness(); callback calculate_value(); VerticalBox { Text { text: \u0026#34;Calculate fluid boundary layer parameters.\\nCalculate the selected parameters based on the others.\u0026#34;; } GridBox { Row { b_t := CheckBox { text: \u0026#34;Total thickness (mm)\u0026#34;; checked: true; enabled: !self.checked; toggled() =\u0026gt; { if self.checked { b_n.checked = false; b_f.checked = false; b_g.checked = false; } } } e_t := LineEdit { text: root.tt; input-type: decimal; read-only: b_t.checked; } } Row { b_n := CheckBox { text: \u0026#34;Number of layers\u0026#34;; checked: false; enabled: !self.checked; toggled() =\u0026gt; { if self.checked { b_t.checked = false; b_f.checked = false; b_g.checked = false; } } } e_n := SpinBox { value: root.nl; minimum: 1; } } Row { b_f := CheckBox { text: \u0026#34;First thickness (mm)\u0026#34;; checked: false; enabled: !self.checked; toggled() =\u0026gt; { if self.checked { b_n.checked = false; b_t.checked = false; b_g.checked = false; } } } e_f := LineEdit { text: root.ft; input-type: decimal; read-only: b_f.checked; } } Row { b_g := CheckBox { text: \u0026#34;Growth rate\u0026#34;; checked: false; enabled: !self.checked; toggled() =\u0026gt; { if self.checked { b_n.checked = false; b_f.checked = false; b_t.checked = false; } } } e_g := LineEdit { text: root.gr; input-type: decimal; read-only: b_g.checked; } } } Button { text: \u0026#34;Calculate\u0026#34;; clicked =\u0026gt; { root.tt = e_t.text; root.nl = e_n.value; root.ft = e_f.text; root.gr = e_g.text; if b_t.checked { root.calculate_total_thickness(); } else if b_n.checked { root.calculate_num_layers(); } else if b_f.checked { root.calculate_first_thickness(); } else if b_g.checked { root.calculate_growth_rate(); } e_t.text = root.tt; e_g.text = root.gr; e_f.text = root.ft; e_n.value = root.nl; } } } } The build.rs build script calls slint-build to translate the .slint files into .rs files.\nfn main() { slint_build::compile(\u0026#34;ui/dialog.slint\u0026#34;).expect(\u0026#34;Slint build failed\u0026#34;); } The main.rs file contains the algorithm implementation of boundary layer calculation and updates the UI interface parameters through callback functions.\n// Prevent console window in addition to Slint window in Windows release builds when, e.g., starting the app via file manager. Ignored on other platforms. #![cfg_attr(not(debug_assertions), windows_subsystem = \u0026#34;windows\u0026#34;)] use slint::SharedString; use std::error::Error; slint::include_modules!(); fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt; { let ui = Dialog::new()?; ui.on_calculate_total_thickness({ let ui_handle = ui.as_weak(); move || { let ui = ui_handle.unwrap(); let n = ui.get_nl(); let f = ui.get_ft().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let g = ui.get_gr().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let mut v = 0.0; for i in 0..n { v = v + f * g.powi(i); } ui.set_tt(SharedString::from(v.to_string())); } }); ui.on_calculate_first_thickness({ let ui_handle = ui.as_weak(); move || { let ui = ui_handle.unwrap(); let n = ui.get_nl(); let t = ui.get_tt().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let g = ui.get_gr().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let mut fi = t / (n as f64); loop { let mut v: f64 = 0.0; for i in 0..n { v = v + fi * g.powi(i); } if (v - t).abs() \u0026lt; 1e-6 { break; } else { fi = fi - (v - t) * 0.01; } } ui.set_ft(SharedString::from(fi.to_string())); } }); ui.on_calculate_growth_rate({ let ui_handle = ui.as_weak(); move || { let ui = ui_handle.unwrap(); let n = ui.get_nl(); let t = ui.get_tt().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let f = ui.get_ft().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let mut gi = f / t; let mut v = 0.0; while (v - t).abs() \u0026gt; 1e-6 { gi = gi - (v - t) * 0.01; v = 0.0; for i in 0..n { v = v + f * gi.powi(i); } } ui.set_gr(SharedString::from(gi.to_string())); } }); ui.on_calculate_num_layers({ let ui_handle = ui.as_weak(); move || { let ui = ui_handle.unwrap(); let f: f64 = ui.get_ft().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let t = ui.get_tt().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let g = ui.get_gr().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let mut i = 1; let mut v = 0.0; loop { v = v + f * g.powi(i); if v \u0026gt; t { break; } i = i + 1; } ui.set_nl(i + 1); } }); ui.run()?; Ok(()) } The final interface effect is as follows. The display interface color will change depending on the system theme color. The size of the generated release binary package is about 3MB. The running memory is around 110MB. Advantages:\nImplement UI interface through declarative language, and even include some simple calculation logic in .slint file. Rust native interface, no unsafe operation. Support cross-platform and cross-device compilation, and also support mobile phone and embedded development. Disadvantages:\nIt does not support multiple windows and file drag-and-drop functions, nor does it support file dialog boxes and message dialog boxes. It cannot call window handles, and does not provide related operation methods. It does not support event control, and lacks more fine-grained support for window and control operations. 2. egui egui is a part of eframe , which aims to create a simple, fast and easy-to-use GUI library. eframe is a cross-platform application development framework that supports multi-platform application development such as Windows, Linus, MacOS and Android. egui not only supports local application development, but can even compile code into wasm and run in the browser.\nThe cargo configuration is as follows, where the main dependency is eframe.\n[package] name = \u0026#34;eLayers-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] eframe = \u0026#34;0.31.1\u0026#34; [profile.release] strip = true opt-level = \u0026#34;z\u0026#34; lto = true codegen-units = 1 panic = \u0026#34;abort\u0026#34; main.rs file, the UI entry is included in the app structure of eframe. The control assembly and button callback functions are implemented through closures. The advantage is that the control and parameter association are updated in real time, and there is no need to manually update the control display.\n#![cfg_attr(not(debug_assertions), windows_subsystem = \u0026#34;windows\u0026#34;)] // hide console window on Windows in release use eframe::egui; fn main() -\u0026gt; eframe::Result { let options = eframe::NativeOptions { viewport: egui::ViewportBuilder::default().with_inner_size([320.0, 240.0]), ..Default::default() }; eframe::run_native( \u0026#34;Layers\u0026#34;, options, Box::new(|_| { // This gives us image support: Ok(Box::\u0026lt;LayersApp\u0026gt;::default()) }), ) } #[derive(PartialEq)] enum Flags { Total, First, Number, Growth, } struct LayersApp { tt: f64, ft: f64, gr: f64, nl: i32, checked: Flags, } impl Default for LayersApp { fn default() -\u0026gt; Self { Self { tt: 3.0, ft: 0.3, gr: 1.5, nl: 5, checked: Flags::Total, } } } impl eframe::App for LayersApp { fn update(\u0026amp;mut self, ctx: \u0026amp;egui::Context, _frame: \u0026amp;mut eframe::Frame) { egui::CentralPanel::default().show(ctx, |ui| { ui.heading(\u0026#34;Layers\u0026#34;); ui.label(\u0026#34;Calculate fluid boundary layer parameters.\\nCalculate the selected parameters based on the others.\u0026#34;); ui.vertical(|ui| { egui::Grid::new(\u0026#34;\u0026#34;) .num_columns(2) .striped(true) .show(ui, |ui| { ui.radio_value(\u0026amp;mut self.checked, Flags::Total, \u0026#34;Total thickness (mm)\u0026#34;); ui.add(egui::DragValue::new(\u0026amp;mut self.tt).max_decimals(6).speed(0.1)); ui.end_row(); ui.radio_value(\u0026amp;mut self.checked, Flags::Number, \u0026#34;Number of layers\u0026#34;); ui.add(egui::DragValue::new(\u0026amp;mut self.nl).speed(1)); ui.end_row(); ui.radio_value(\u0026amp;mut self.checked, Flags::First, \u0026#34;First thickness (mm)\u0026#34;); ui.add(egui::DragValue::new(\u0026amp;mut self.ft).max_decimals(6).speed(0.1)); ui.end_row(); ui.radio_value(\u0026amp;mut self.checked, Flags::Growth, \u0026#34;Growth rate\u0026#34;); ui.add(egui::DragValue::new(\u0026amp;mut self.gr).max_decimals(6).speed(0.1)); ui.end_row(); }); }); if ui.button(\u0026#34;Calculate\u0026#34;).clicked() { match self.checked { Flags::Total =\u0026gt; { let mut v = 0.0; for i in 0..self.nl { v = v + self.ft * self.gr.powi(i); } self.tt = v; } Flags::Number =\u0026gt; { let mut i = 1; let mut v = 0.0; loop { v = v + self.ft * self.gr.powi(i); if v \u0026gt; self.tt { break; } i = i + 1; } self.nl = i + 1; } Flags::First =\u0026gt; { let mut f = self.tt / (self.nl as f64); loop { let mut v: f64 = 0.0; for i in 0..self.nl { v = v + f * self.gr.powi(i); } if (v - self.tt).abs() \u0026lt; 1e-6 { break; } else { f = f - (v - self.tt) * 0.01; } } self.ft = f; } Flags::Growth =\u0026gt; { let mut g = self.ft / self.tt; let mut v = 0.0; while (v - self.tt).abs() \u0026gt; 1e-6 { g = g - (v - self.tt) * 0.01; v = 0.0; for i in 0..self.nl { v = v + self.ft * g.powi(i); } } self.gr = g; } } } }); } } The interface effect is shown in the figure below. Compared with the traditional UI interface, the style presented by EGUI is more similar to the web front end. The screenshot here shows that the system is a light theme. If the system is a dark theme, the interface background color will be displayed in dark. The interface occasionally flickers because the control display is related to the parameters, which may be related to the high refresh frequency of the interface. The generated release binary package size is less than 3MB. The running memory is also around 110MB. Advantages:\nSupports Web and Native application development. Rust native, no unsafe operations. Supports cross-platform. Disadvantages:\nNo UI tools, no quick preview of UI interface, no separation of interface and implementation methods. Cannot call window handles, and does not support event control. Under development, the API is not stable yet, and major adjustments may occur at any time. 3. fltk-rs fltk was originally a lightweight GUI library developed in C++. fltk-rs is the rust binding of fltk, and the underlying implementation is still implemented in C++. The interface implementation of fltk is very simple, and the assembly of controls is operated through relatively primitive coordinates, and it does not support automatic arrangement and combination. Although it provides a wealth of controls, the native theme effects are similar to the minimalist style of Motif , which is far from the modern UI interface. Fortunately, fltk-theme provides a wealth of interface themes for beautification, and users can customize their own theme styles based on this.\nThe cargo configuration file is as follows. Here, fltk downloads the compiled lib file through the \u0026quot;fltk-bundled\u0026quot; feature to avoid recompiling fltk and reporting errors. fltk-rs can use the interface tool fluid provided by fltk for rapid development and preview of the UI interface. In rust, fl2rust is used to translate the .fl file generated by fluid into a .rs file.\n[package] name = \u0026#34;fLayers-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] fltk = { version = \u0026#34;1.5.9\u0026#34;, features = [\u0026#34;fltk-bundled\u0026#34;] } [build-dependencies] fl2rust = \u0026#34;0.7.0\u0026#34; [profile.release] strip = true opt-level = \u0026#34;z\u0026#34; lto = true codegen-units = 1 panic = \u0026#34;abort\u0026#34; The ui file widget.fl is generated by fluid, so try to avoid manual editing.\n# data file for the Fltk User Interface Designer (fluid) version 1.0403 header_name {.h} code_name {.cxx} class Widget {open } { Function {make_window()} {open } { Fl_Window window { label Layers open xywh {668 341 360 200} type Double visible } { Fl_Group {} { label {Calculate fluid boundary layer parameters.} open xywh {0 20 360 5} align 5 } {} Fl_Group {} { label {Calculate the selected parameters based on the others.} open xywh {0 40 360 5} align 5 } {} Fl_Round_Button rb_tt { label {Total thickness (mm)} xywh {0 50 170 25} down_box ROUND_DOWN_BOX value 1 } Fl_Value_Input vi_tt { xywh {180 50 170 25} maximum 1e+06 step 0.1 value 3 } Fl_Round_Button rb_nl { label {Number of layers} xywh {0 80 170 25} down_box ROUND_DOWN_BOX } Fl_Value_Input vi_nl { xywh {180 80 170 25} maximum 1e+06 step 1 value 5 } Fl_Round_Button rb_ft { label {First thickness (mm)} xywh {0 110 170 25} down_box ROUND_DOWN_BOX } Fl_Value_Input vi_ft { xywh {180 110 170 25} maximum 1e+06 step 0.1 value 0.3 } Fl_Round_Button rb_gr { label {Growth rate} xywh {0 140 170 25} down_box ROUND_DOWN_BOX } Fl_Value_Input vi_gr {selected xywh {180 140 170 25} maximum 1e+06 step 0.1 value 1.5 } Fl_Button bn_calc { label Calculate xywh {10 170 340 22} } } } } build.rs build script tells fl2rust how to translate the UI interface into an RS file.\n// build.rs fn main() { use std::env; use std::path::PathBuf; println!(\u0026#34;cargo:rerun-if-changed=ui/widget.fl\u0026#34;); let g = fl2rust::Generator::default(); let out_path = PathBuf::from(env::var(\u0026#34;OUT_DIR\u0026#34;).unwrap()); g.in_out(\u0026#34;ui/widget.fl\u0026#34;, out_path.join(\u0026#34;widget.rs\u0026#34;).to_str().unwrap()) .expect(\u0026#34;Failed to generate rust from fl file!\u0026#34;); } main.rs file, which implements control refresh and button functions through closures.\n// src/main.rs // Prevent console window #![cfg_attr(not(debug_assertions), windows_subsystem = \u0026#34;windows\u0026#34;)] #![allow(unused_variables)] #![allow(unused_mut)] #![allow(unused_imports)] #![allow(clippy::needless_update)] use fltk::{prelude::*, *}; include!(concat!(env!(\u0026#34;OUT_DIR\u0026#34;), \u0026#34;/widget.rs\u0026#34;)); fn main() { let app = app::App::default(); show_window(); app.run().unwrap(); } fn show_window() { let mut win = Widget::make_window(); win.rb_tt.deactivate(); let mut ui = win.clone(); win.rb_tt.set_callback(move |_| { if ui.rb_tt.value() { ui.rb_tt.deactivate(); ui.rb_ft.activate(); ui.rb_ft.set_value(false); ui.rb_nl.activate(); ui.rb_nl.set_value(false); ui.rb_gr.activate(); ui.rb_gr.set_value(false); } }); let mut ui = win.clone(); win.rb_ft.set_callback(move |_| { if ui.rb_ft.value() { ui.rb_ft.deactivate(); ui.rb_tt.activate(); ui.rb_tt.set_value(false); ui.rb_nl.activate(); ui.rb_nl.set_value(false); ui.rb_gr.activate(); ui.rb_gr.set_value(false); } }); let mut ui = win.clone(); win.rb_nl.set_callback(move |_| { if ui.rb_nl.value() { ui.rb_nl.deactivate(); ui.rb_tt.activate(); ui.rb_tt.set_value(false); ui.rb_ft.activate(); ui.rb_ft.set_value(false); ui.rb_gr.activate(); ui.rb_gr.set_value(false); } }); let mut ui = win.clone(); win.rb_gr.set_callback(move |_| { if ui.rb_gr.value() { ui.rb_gr.deactivate(); ui.rb_tt.activate(); ui.rb_tt.set_value(false); ui.rb_ft.activate(); ui.rb_ft.set_value(false); ui.rb_nl.activate(); ui.rb_nl.set_value(false); } }); win.vi_tt.set_precision(6); win.vi_ft.set_precision(6); win.vi_nl.set_precision(0); win.vi_gr.set_precision(6); let mut ui = win.clone(); win.bn_calc.set_callback(move |_| { let tt = ui.vi_tt.value(); let ft = ui.vi_ft.value(); let nl = ui.vi_nl.value() as i32; let gr = ui.vi_gr.value(); if ui.rb_tt.value() { let mut v = 0.0; for i in 0..nl { v = v + ft * gr.powi(i); } ui.vi_tt.set_value(v); } else if ui.rb_ft.value() { let mut f = tt / (nl as f64); loop { let mut v: f64 = 0.0; for i in 0..nl { v = v + f * gr.powi(i); } if (v - tt).abs() \u0026lt; 1e-6 { break; } else { f = f - (v - tt) * 0.01; } } ui.vi_ft.set_value(f); } else if ui.rb_nl.value() { let mut i = 1; let mut v = 0.0; loop { v = v + ft * gr.powi(i); if v \u0026gt; tt { break; } i = i + 1; } ui.vi_nl.set_value(i as f64 + 1.0); } else { let mut gi = ft / tt; let mut v = 0.0; while (v - tt).abs() \u0026gt; 1e-6 { gi = gi - (v - tt) * 0.01; v = 0.0; for i in 0..nl { v = v + ft * gi.powi(i); } } ui.vi_gr.set_value(gi); } }) } The interface effect is as follows. The interface color is fixed and will not change with the system theme. The generated release binary package is only over 600 KB in size. The running memory is less than 20MB. Advantages:\nThe API is relatively mature, provides simple event control functions, and supports extracting window handles. It provides UI tools that can quickly develop and preview interfaces, and supports the separation of interfaces and implementations. It supports cross-platform. Disadvantages:\nThe native interface theme style is too old and not suitable for modern UI styles. The underlying layer is implemented in C++, which is not native to Rust and contains many unsafe operations. The control arrangement and combination method is relatively primitive, and there is no assembler that automatically arranges controls. 4. Comparison and Conclusion Slint and egui can basically be regarded as rust native GUIs. The memory consumption and binary package size of the two are basically at the same level, but the UI implementation methods and interface effects of the two are very different. In terms of ecology, slint is more complete than egui, with UI tools and support for separation of interface and implementation. The cost is that slint has more dependencies and the build process takes longer. Compared with traditional C++ interface libraries, the common problem of both is that there are too few new functions and no support for fine-grained control of displayed content. For example, window handles and file drag and drop, although there are many ways to bypass the functional limitations of UI libraries to achieve them, it also increases the learning cost and adds an extra burden to developers.\nfltk-rs is better than the previous two in memory consumption and binary release package size, but the underlying layer depends on C++, and it is inevitable that many unsafe operations will be used. The development history of fltk is much earlier than the previous two, so the API function is more complete, which is a good choice for developers who have just switched from C/C++ to rust. It is precisely because fltk has a long history and the native interface style is relatively old, which is not in line with the modern aesthetic style. Moreover, the arrangement and combination of fltk\u0026rsquo;s controls are relatively primitive, but fortunately, with the support of ui tools, the development efficiency is not too bad.\nIn short:\nIf you want to refactor from C/C++ code to rust and do not pursue gorgeous and modern interface effects, fltk-rs is suitable If you have more requirements for application window and event control and are sensitive to application resource control, fltk-rs is suitable If you need to take into account both Web and Native application development, egui is suitable If you want to develop completely from scratch with rust and pursue cross-platform and embedded functions, slint is suitable As for other rust interface libraries, such as Tauri, gtk-rs, CXX-qt, etc., I can\u0026rsquo;t think of any special advantages for the time being, so I can only say that we will meet again if we are destined to.\n","permalink":"https://andrewmoa.site/post/2025-06-11-rust_gui/","tags":[{"LinkTitle":"Rust","RelPermalink":"/tags/rust/"}],"title":"Exploring Rust Graphical Interface Libraries"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/categories/cfd/"}],"content":"Although the official STAR-CCM+ documentation specifically states that FORTRAN is not supported under Windows1. But in fact, as long as the compiler supports it, user library compiled using Fortran under Windows can be loaded and run normally in STAR-CCM+.\n1. Build CMake Project First, we refer to the tutorial case in the official documentation 2 and build a CMake project. The project structure is as follows:\nSTARCCM_FORTRAN_SAMPLE │ CMakeLists.txt\t# CMake Configuration File │ README.md\t# Description document, not required ├───.vscode │ launch.json\t# Automatically generated file when starting debug mode, not required │ settings.json\t# Define CMake related variables └───src initVelocity.f StarReal.f.in sutherlandViscosity.f uflib.f zeroGradT.f The main content of CMake configuration file CMakeLists.txt is as follows:\ncmake_minimum_required(VERSION 3.10) # Project name project(UserFortran LANGUAGES Fortran) set(CMAKE_Fortran_STANDARD 2008) # Check for STARCCM_USER_LIB_DIR if(NOT DEFINED STARCCM_USER_LIB_DIR) message(FATAL_ERROR \u0026#34;STARCCM_USER_LIB_DIR is not defined. Please specify the path to the STAR-CCM+ UserFunctions library directory.\u0026#34;) # For example, in Windows : C:/Program Files/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/win64/clang17.0vc14.2-r8/lib # In Linux : /opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/gnu11.4-r8/lib\u0026#34; else() message(STATUS \u0026#34;STARCCM_USER_LIB_DIR location : \u0026#34; ${STARCCM_USER_LIB_DIR}) endif() # Check for STARCCM_STD_LIB_DIR if(NOT DEFINED STARCCM_STD_LIB_DIR) message(STATUS \u0026#34;STARCCM_STD_LIB_DIR undefined. using system standard library. \u0026#34;) # For example, in Linux : /opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64\u0026#34; else() message(STATUS \u0026#34;STARCCM_STD_LIB_DIR location : \u0026#34; ${STARCCM_STD_LIB_DIR}) endif() # STAR-CCM+ output precision if(USE_DOUBLE_PRECISION) message(STATUS \u0026#34;Using double precision for STAR-CCM+\u0026#34;) set(STAR_REAL \u0026#34;1D0\u0026#34;) else() message(STATUS \u0026#34;Using float precision for STAR-CCM+\u0026#34;) set(STAR_REAL \u0026#34;1.0\u0026#34;) endif() # generate the StarReal.f file configure_file(src/StarReal.f.in ${CMAKE_BINARY_DIR}/StarReal.f @ONLY) # Include directories include_directories(${PROJECT_SOURCE_DIR}/include ) # Link with STARCCM LIB directory link_directories(${STARCCM_USER_LIB_DIR} ${STARCCM_STD_LIB_DIR} ) # Specify the source files set(SOURCES ${CMAKE_BINARY_DIR}/StarReal.f src/initVelocity.f src/sutherlandViscosity.f src/zeroGradT.f src/uflib.f ) # Add library add_library(${CMAKE_PROJECT_NAME} SHARED ${SOURCES} ) # Link library target_link_libraries(${CMAKE_PROJECT_NAME} UserFunctions) # Install target install(TARGETS ${CMAKE_PROJECT_NAME} RUNTIME DESTINATION bin LIBRARY DESTINATION lib ARCHIVE DESTINATION lib/static ) There are two key settings:\nSpecify the search path for the STAR-CCM+ library files through STARCCM_USER_LIB_DIR and STARCCM_STD_LIB_DIR. STARCCM_STD_LIB_DIR is optional and is used to specify the link to the STAR-CCM+ standard library files under Linux. Specify the precision through USE_DOUBLE_PRECISION, generate StarReal.f based on the configuration file StarReal.f.in, and specify the StarReal type precision to the compiler StarReal.f.in is the same as the official example\u0026rsquo;s StarReal.f, except that the original value is changed to the @STAR_REAL@ marker to tell CMake how to replace the character:\nmodule StarRealMod integer, parameter :: StarInt = kind(1) integer, parameter :: StarReal = kind(@STAR_REAL@) integer, parameter :: CoordReal = kind(1D0) integer, parameter :: StarIntSize = StarInt integer, parameter :: StarRealSize = StarReal integer, parameter :: CoordRealSize = CoordReal end module StarRealMod zeroGradT.f defines a boundary configuration function:\nC Set boundary temperature equal to cell temperature subroutine zeroGradT(result,size,fc,T) use StarRealMod implicit none integer, intent(in) :: size real(StarReal), intent(out) :: result(size) integer, intent(in) :: fc(2,*) real(StarReal), intent(in) :: T(*) integer i C Loop through all entities applying T_boundary = T_cell C fc(1,i) is the cell next to i do i = 1,size result(i) = T(fc(1,i)) end do return end initVelocity.f defines a region configuration function:\nC Initial velocity based on uniform swirl subroutine initVelocity(result,size,centroid) use StarRealMod implicit none integer, intent(in) :: size real(StarReal), intent(out) :: result(3,size) real(CoordReal), intent(in) :: centroid(3,*) integer i real(CoordReal) dr(3) C Angular velocity and origin of rotation real(CoordReal), parameter :: omega(3) = (/0.0,0.0,100.0/) real(CoordReal), parameter :: origin(3) = (/0.0,0.0,0.0/) C Loop through all entities applying u = omega x (centroid - origin) do i = 1,size dr(1) = centroid(1,i) - origin(1) dr(2) = centroid(2,i) - origin(2) dr(3) = centroid(3,i) - origin(3) result(1,i) = omega(2)*dr(3) - omega(3)*dr(2) result(2,i) = omega(3)*dr(1) - omega(1)*dr(3) result(3,i) = omega(1)*dr(2) - omega(2)*dr(1) end do return end sutherlandViscosity.f defines a field function:\nC Dynamic viscosity based on Sutherland\u0026#39;s law subroutine sutherlandViscosity(result,size,T) use StarRealMod implicit none integer, intent(in) :: size real(StarReal), intent(out) :: result(size) real(StarReal), intent(in) :: T(*) integer i C Reference viscosity, Sutherland constant and reference temperature real(StarReal), parameter :: v0 = 1.716E-5 real(StarReal), parameter :: Cs = 110.0 real(StarReal), parameter :: T0 = 273.15 C Loop through all entities applying Sutherland\u0026#39;s law do i = 1,size result(i) = v0 * (T(i)/T0)**1.5 * (T0 + Cs)/(T(i) + Cs) end do return end uflib.f is used as an import function to register the user-defined function above:\nsubroutine uflib() use StarRealMod implicit none C Register user functions here external zeroGradT,initVelocity,sutherlandViscosity call uffunc(zeroGradT, \u0026#34;BoundaryProfile\u0026#34;, \u0026amp; \u0026#34;Zero Gradient Temperature\u0026#34;) call ufarg (zeroGradT, \u0026#34;Face\u0026#34;, \u0026amp; \u0026#34;FaceCellIndex\u0026#34;, 2*StarIntSize) call ufarg (zeroGradT, \u0026#34;Cell\u0026#34;, \u0026amp; \u0026#34;Temperature\u0026#34;, StarRealSize) call uffunc(initVelocity, \u0026#34;RegionProfile\u0026#34;, \u0026amp; \u0026#34;Initial Velocity\u0026#34;) call ufarg(initVelocity, \u0026#34;Cell\u0026#34;, \u0026amp; \u0026#34;Centroid\u0026#34;, 3*CoordRealSize) call uffunc(sutherlandViscosity, \u0026#34;ScalarFieldFunction\u0026#34;, \u0026amp; \u0026#34;Sutherland Viscosity\u0026#34;) call ufarg(sutherlandViscosity, \u0026#34;Cell\u0026#34;, \u0026amp; \u0026#34;Temperature\u0026#34;, StarRealSize) return end The Fortran codes that come with the official documents are all in a fixed format. I don’t know if it’s due to the formatting, but the first 6 spaces in the code are not displayed correctly. Be sure to pay attention when copying and pasting.\n2. Compiling dynamic libraries under Windows Compile under Windwos using clang64 toolchain of msys2, and the compiler is LLVM flang. If you do not find this option on VSCode, you can use the CMake extended scanning toolkit function to scan the clang64 directory of msys2 to find this option. After compiling and outputting the dynamic library, use the ldd command under the clang64 toolchain to scan the dependencies of the dynamic library. Except for the dynamic library that comes with the system, no other third-party dynamic libraries are linked. If you use mingw64 or ucrt64 toolchain to compile, the compiler is usually gfortran. It is best to scan the compiled dynamic library files with ldd to ensure that there are not a lot of third-party dynamic library dependencies.\nWhen loaded using STAR-CCM+, it can be recognized normally and the compiled language is displayed as Fortran. 3. Compiling dynamic libraries under Linux Compile the fortran dynamic link library under Linux using the following command:\n# Enter the project directory cd starccm_fortran_sample # Create a working directory mkdir -p build \u0026amp;\u0026amp; cd build # Configure the compilation file and link it to the STAR-CCM+ library cmake .. -G \u0026#34;Ninja\u0026#34; -DUSE_DOUBLE_PRECISION=ON \\ -DSTARCCM_USER_LIB_DIR=${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/gnu11.4-r8/lib \\ -DSTARCCM_STD_LIB_DIR=${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64 # Compile dynamic library cmake --build . --config Release # Install cmake --install . --prefix $PWD/../../UserLib Use ldd to scan the generated dynamic library file, and there is no other third-party dynamic library dependency. Use STAR-CCM+ to load and it can be recognized normally. 4. Calculation case The following is a STAR-CCM+ case demonstration3 to see whether a user library written in Fortran can be loaded into STAR-CCM+ for normal calculation.\nThe fully developed laminar flow in a circular tube is simulated. The circular tube is 3 meters long and 0.2 meters in diameter, with an air inlet at one end and an outlet at the other end. The physical parameters and inlet conditions are as follows:\nDensity: 1.0 kg/m^3 Viscosity: 2×10^-3 N-s/m^2 The laminar flow inlet of the duct is fully developed to satisfy the following relationship:uum=2[1−(rr0)2] Where: um represents the average velocity at the inlet, r0 represents the pipe diameter Create a 2D axisymmetric model. Create a new CMake project. The CMakeLists.txt file is basically the same as the previous one, with only the following parts changed:\n# Specify the source files set(SOURCES ${CMAKE_BINARY_DIR}/StarReal.f src/parabolicVelocity.f src/uflib.f ) There are only 3 source files, among which the content of StarReal.f.in is the same as the previous one. The implementation of the inlet velocity is in the parabolicVelocity.f file, and the content is as follows:\nC Initial velocity subroutine parabolicVelocity(result,size,centroid) use StarRealMod implicit none integer, intent(in) :: size real(StarReal), intent(out) :: result(size) real(CoordReal), intent(in) :: centroid(3,*) integer i real(CoordReal) radius real(CoordReal), parameter :: R = 0.1 real(CoordReal), parameter :: origin(3) = (/0.0,0.0,0.0/) C Loop through all entities applying uniform velocity do i = 1,size radius = sqrt((centroid(1,i)- origin(1))**2 + \u0026amp; (centroid(2,i) - origin(2))**2 + \u0026amp; (centroid(3,i) - origin(3))**2) result(i) = 2* (1-(radius/R)*(radius/R)) end do return end The contents of the uflib.f file where the registration function is located are as follows:\nsubroutine uflib() use StarRealMod implicit none external parabolicVelocity C Register user functions here call uffunc(parabolicVelocity, \u0026#34;BoundaryProfile\u0026#34;, \u0026amp; \u0026#34;Parabolic Velocity\u0026#34;) call ufarg(parabolicVelocity, \u0026#34;Face\u0026#34;, \u0026amp; \u0026#34;Centroid\u0026#34;, 3*CoordRealSize) return end Compile using the clang64 toolchain of msys2 under Windows and output dynamic link library files. Load the user library: Select the user library as the entry condition and the function parabolicVelocity defined above as the function. The calculation is completed and the scene is output. Velocity plot at the inlet. 使用用户程序 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFortran 用户接口参考 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSTAR-CCM+二次开发——User Code \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-04-30-use-vscode-develop-starccm-user-library-by-fortran/","tags":[{"LinkTitle":"Fortran","RelPermalink":"/tags/fortran/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/tags/star-ccm+/"}],"title":"Use VSCode to develop STAR-CCM+ user library: build dynamic link library through Fortran"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/categories/cfd/"}],"content":"STAR-CCM+ user program (user library)1 is a dynamic link library compiled and constructed according to certain rules by an external compiler (usually C/C++, but also supports Fortran in Linux). By registering the constructed dynamic link library in the sim file, certain custom functions can be implemented. A user library usually contains one or more user-defined functions, which are generally used to implement special configuration or field functions.\nUer library need to register dynamic link libraries before they can be called, and the format of dynamic link libraries is often closely related to the operating system, hardware platform, etc. Therefore, user library are usually compiled for specific platforms and cannot run across platforms.\nThis article attempts to use VSCode to write the STAR-CCM+ dynamic link library, and use the open source thermodynamic library CoolProp to expand the physical properties calculation function for STAR-CCM+; build the user library through CMake to ensure that the dynamic link library file can be correctly generated on different platforms.\n1. Introduction to CoolProp CoolProp is an open source cross-platform thermodynamic library, similar to NIST\u0026rsquo;s REFPROP. It contains various fluid physical propertiess and supports multiple programming languages. It also supports physical properties calculations through MATLAB, Excel and even javascript calls2.\nOn Windows, use the following commands to download, compile, and install CoolProp3:\n# It is recommended to use PowerShell. You need to install cmake, git and clang-cl compiler first. # Download CoolProp source code and its dependencies git clone https://github.com/CoolProp/CoolProp --recursive cd CoolProp # Create a working directory mkdir build \u0026amp;\u0026amp; cd build #Configure static library compilation file cmake .. -DCOOLPROP_STATIC_LIBRARY=ON -G \u0026#34;Visual Studio 17 2022\u0026#34; -T ClangCL,host=x64 -A x64 # Compile static library cmake --build . --config Release # Install static library cmake --install . --prefix $PWD/../../CoolPropLib Compile and install CoolProp on Linux platform using the following commands:\n# You need to install cmake, git and gcc compiler first # Download CoolProp source code and its dependencies git clone https://github.com/CoolProp/CoolProp --recursive cd CoolProp # Create a working directory mkdir -p build \u0026amp;\u0026amp; cd build # Configure static library compilation file, the default architecture is 64-bit cmake .. -DCOOLPROP_STATIC_LIBRARY=ON -DCOOLPROP_FPIC=ON -G \u0026#34;Ninja\u0026#34; # Compile static libraries and use 32 cores to speed up compilation cmake --build . --config Release -- -j32 # Install static library cmake --install . --prefix $PWD/../../CoolPropLib Since the code is well encapsulated, calling CoolProp related functions is very simple. The following is the official sample code 4:\n#include \u0026#34;CoolProp.h\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;stdlib.h\u0026gt; using namespace CoolProp; int main() { // First type (slowest, due to most string processing, exposed in DLL) std::cout \u0026lt;\u0026lt; PropsSI(\u0026#34;Dmolar\u0026#34;, \u0026#34;T\u0026#34;, 298, \u0026#34;P\u0026#34;, 1e5, \u0026#34;Propane[0.5]\u0026amp;Ethane[0.5]\u0026#34;) \u0026lt;\u0026lt; std::endl; // Default backend is HEOS std::cout \u0026lt;\u0026lt; PropsSI(\u0026#34;Dmolar\u0026#34;, \u0026#34;T\u0026#34;, 298, \u0026#34;P\u0026#34;, 1e5, \u0026#34;HEOS::Propane[0.5]\u0026amp;Ethane[0.5]\u0026#34;) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; PropsSI(\u0026#34;Dmolar\u0026#34;, \u0026#34;T\u0026#34;, 298, \u0026#34;P\u0026#34;, 1e5, \u0026#34;REFPROP::Propane[0.5]\u0026amp;Ethane[0.5]\u0026#34;) \u0026lt;\u0026lt; std::endl; // Vector example std::vector\u0026lt;double\u0026gt; z(2, 0.5); // Second type (C++ only, a bit faster, allows for vector inputs and outputs) std::vector\u0026lt;std::string\u0026gt; fluids; fluids.push_back(\u0026#34;Propane\u0026#34;); fluids.push_back(\u0026#34;Ethane\u0026#34;); std::vector\u0026lt;std::string\u0026gt; outputs; outputs.push_back(\u0026#34;Dmolar\u0026#34;); std::vector\u0026lt;double\u0026gt; T(1, 298), p(1, 1e5); std::cout \u0026lt;\u0026lt; PropsSImulti(outputs, \u0026#34;T\u0026#34;, T, \u0026#34;P\u0026#34;, p, \u0026#34;\u0026#34;, fluids, z)[0][0] \u0026lt;\u0026lt; std::endl; // Default backend is HEOS std::cout \u0026lt;\u0026lt; PropsSImulti(outputs, \u0026#34;T\u0026#34;, T, \u0026#34;P\u0026#34;, p, \u0026#34;HEOS\u0026#34;, fluids, z)[0][0] \u0026lt;\u0026lt; std::endl; // Comment me out if REFPROP is not installed std::cout \u0026lt;\u0026lt; PropsSImulti(outputs, \u0026#34;T\u0026#34;, T, \u0026#34;P\u0026#34;, p, \u0026#34;REFPROP\u0026#34;, fluids, z)[0][0] \u0026lt;\u0026lt; std::endl; // All done return return EXIT_SUCCESS; } In the correct case, the output of the above sample code is:\n40.8269 40.8269 40.8269 40.8269 40.8269 40.8269 CoolProp also supports Python calls and can be used as a calculator to conveniently calculate various working fluid physical parameters:\nIn [1]: from CoolProp.CoolProp import PropsSI In [2]: PropsSI(\u0026#39;T\u0026#39;,\u0026#39;P\u0026#39;,101325,\u0026#39;Q\u0026#39;,0,\u0026#39;Water\u0026#39;) Out[2]: 373.1242958476844 In [3]: H_V = PropsSI(\u0026#39;H\u0026#39;,\u0026#39;P\u0026#39;,101325,\u0026#39;Q\u0026#39;,1,\u0026#39;Water\u0026#39;) In [4]: H_L = PropsSI(\u0026#39;H\u0026#39;,\u0026#39;P\u0026#39;,101325,\u0026#39;Q\u0026#39;,0,\u0026#39;Water\u0026#39;) In [5]: H_V - H_L Out[5]: 2256471.5924066794 2. Develop STAR-CCM+ user library First, create a new VSCode working directory with the following file structure:\nstarccm_coolprop\t# Working directory name │ CMakeLists.txt\t# CMake configuration file │ README.md\t# Description file, to help other users better understand the project, not required ├───.vscode\t#VSCode configuration file directory │ launch.json\t# File automatically generated when starting debug mode, not required │ settings.json\t# Define CMake related variables ├───include\t# Include directory │ heos.h\t# CoolProp implementation related header files │ uclib.h\t# STAR-CCM+ user library definition header file └───src\t# Source directory heos.cpp\t# CoolProp implementation related source code uclib.cpp\t# STAR-CCM+ user library link source code In order to facilitate cross-platform, the build tool uses CMake. Edit the CMake configuration file CMakeLists.txt as follows:\n# Define the minimum CMake version required to build the project cmake_minimum_required(VERSION 3.10) # Project name and development language project(upcp LANGUAGES CXX) # The compiler must meet the C++17 specification set(CMAKE_CXX_STANDARD 17) # Check COOLPROP_SRC_DIR definition to search for CoolProp source files if(NOT DEFINED COOLPROP_SRC_DIR) message(FATAL_ERROR \u0026#34;COOLPROP_SRC_DIR is not defined. Please specify the path to the CoolProp source directory.\u0026#34;) else() message(STATUS \u0026#34;COOLPROP_SRC_DIR location : \u0026#34; ${COOLPROP_SRC_DIR}) endif() # Check COOLPROP_LIB_DIR definition to search for precompiled CoolProp library files if(NOT DEFINED COOLPROP_LIB_DIR) message(FATAL_ERROR \u0026#34;COOLPROP_LIB_DIR is not defined. Please specify the path to the CoolProp library directory.\u0026#34;) # For example: [CoolPropLib_Dir]/static_library/[platform]/[architecture]_[compiler]_[version] else() message(STATUS \u0026#34;COOLPROP_LIB_DIR location : \u0026#34; ${COOLPROP_LIB_DIR}) endif() # Check STARCCM_LIB_DIR for linking to STAR-CCM+ UserFunctions library files if(NOT DEFINED STARCCM_LIB_DIR) message(FATAL_ERROR \u0026#34;STARCCM_LIB_DIR is not defined. Please specify the path to the STAR-CCM+ UserFunctions library directory.\u0026#34;) # For example, in Windows : C:/Program Files/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/win64/clang17.0vc14.2-r8/lib # In Linux : /opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/gnu11.4-r8/lib\u0026#34; else() message(STATUS \u0026#34;STARCCM_LIB_DIR location : \u0026#34; ${STARCCM_LIB_DIR}) endif() #Define STAR-CCM+ software precision if(USE_DOUBLE_PRECISION) add_definitions(-DDOUBLE_PRECISION) message(STATUS \u0026#34;Using double precision for STAR-CCM+\u0026#34;) else() message(STATUS \u0026#34;Using float precision for STAR-CCM+\u0026#34;) endif() # Use CoolProp static library or dynamic library if(USE_SHARED_COOLPROP) add_definitions(-DCOOLPROP_LIB) message(STATUS \u0026#34;Using shared CoolProp library\u0026#34;) else() message(STATUS \u0026#34;Using static CoolProp library\u0026#34;) endif() # Header file include directory include_directories(${PROJECT_SOURCE_DIR}/include ${COOLPROP_SRC_DIR}/include ${COOLPROP_SRC_DIR}/externals/fmtlib/include ) # Add the library file directory of STAR-CCM+ and CoolProp link_directories(${STARCCM_LIB_DIR} ${COOLPROP_LIB_DIR} ) # Generate dynamic link library add_library(${CMAKE_PROJECT_NAME} SHARED src/uclib.cpp src/heos.cpp ) # Link to the STAR-CCM+ and CoolProp library files target_link_libraries(${CMAKE_PROJECT_NAME} UserFunctions CoolProp) # Installation Target install(TARGETS ${CMAKE_PROJECT_NAME} RUNTIME DESTINATION bin LIBRARY DESTINATION lib ARCHIVE DESTINATION lib/static ) heos.h defines 3 user functions, which are the density, viscosity and thermal conductivity functions of air. The code is as follows:\n#ifndef __HEOS_H__ #define __HEOS_H__ void USERFUNCTION_EXPORT airDensity(CoordReal *, int, CoordReal *, CoordReal *); void USERFUNCTION_EXPORT airViscosity(CoordReal *, int, CoordReal *, CoordReal *); void USERFUNCTION_EXPORT airConductivity(CoordReal *, int, CoordReal *, CoordReal *); #endif /* __HEOS_H__ */ heos.cpp is the implementation of these three user functions. By inputting pressure and temperature, it calls the CoolProp library to calculate the density, viscosity and thermal conductivity of the output air.\n#include \u0026#34;CoolProp.h\u0026#34; #include \u0026#34;AbstractState.h\u0026#34; #include \u0026#34;crossplatform_shared_ptr.h\u0026#34; // The above three header files are included in the include directory of CoolProp source code #include \u0026#34;uclib.h\u0026#34; #include \u0026#34;heos.h\u0026#34; /* Call CoolProp to calculate the output air density by inputting pressure and temperature */ void USERFUNCTION_EXPORT airDensity(CoordReal *result, int size, CoordReal *P, CoordReal *T) { /* In order to speed up the iteration, the low-level interface of CoolProp is used here to avoid repeatedly calling string processing functions */ /* Initialize fluid parameters using smart pointers */ shared_ptr\u0026lt;CoolProp::AbstractState\u0026gt; heos(CoolProp::AbstractState::factory(\u0026#34;HEOS\u0026#34;, \u0026#34;Air\u0026#34;)); /* Iterate through all cells */ for (int i = 0; i != size; ++i) { /* The pressure obtained from STAR-CCM+ is gauge pressure and needs to be converted to absolute pressure manually */ heos-\u0026gt;update(CoolProp::PT_INPUTS, P[i] + 101325.0, T[i]); // The default is SI units. result[i] = heos-\u0026gt;rhomass(); // Unit : kg/m^3 } } /* Call CoolProp to calculate the output air viscosity by inputting pressure and temperature */ void USERFUNCTION_EXPORT airViscosity(CoordReal *result, int size, CoordReal *P, CoordReal *T) { shared_ptr\u0026lt;CoolProp::AbstractState\u0026gt; heos(CoolProp::AbstractState::factory(\u0026#34;HEOS\u0026#34;, \u0026#34;Air\u0026#34;)); for (int i = 0; i != size; ++i) { heos-\u0026gt;update(CoolProp::PT_INPUTS, P[i] + 101325.0, T[i]); result[i] = heos-\u0026gt;viscosity(); // Unit : Pa-s } } /* Call CoolProp to calculate the output air thermal conductivity by inputting pressure and temperature */ void USERFUNCTION_EXPORT airConductivity(CoordReal *result, int size, CoordReal *P, CoordReal *T) { shared_ptr\u0026lt;CoolProp::AbstractState\u0026gt; heos(CoolProp::AbstractState::factory(\u0026#34;HEOS\u0026#34;, \u0026#34;Air\u0026#34;)); for (int i = 0; i != size; ++i) { heos-\u0026gt;update(CoolProp::PT_INPUTS, P[i] + 101325.0, T[i]); result[i] = heos-\u0026gt;conductivity(); // Unit : W/m-K } } The uclib.h header file is the header file of the STAR-CCM+ link library5, which defines the variables and function types used by the UserFunctions library. This file is a general header file.\n#ifndef UCLIB_H #define UCLIB_H #ifdef DOUBLE_PRECISION typedef double Real; #else typedef float Real; #endif typedef double CoordReal; #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif #if defined(WIN32) || defined(_WINDOWS) || defined(_WINNT) #define USERFUNCTION_EXPORT __declspec(dllexport) #define USERFUNCTION_IMPORT __declspec(dllimport) #else #define USERFUNCTION_EXPORT #define USERFUNCTION_IMPORT #endif extern void USERFUNCTION_IMPORT ucarg(void *, char *, char *, int); extern void USERFUNCTION_IMPORT ucfunc(void *, char *, char *); extern void USERFUNCTION_IMPORT ucfunction(void *, char *, char *, int, ...); void USERFUNCTION_EXPORT uclib(); #ifdef __cplusplus } #endif #endif uclib.cpp is used to register user functions so that STAR-CCM+ can correctly identify the dynamic link library and load it as a user library.\n#include \u0026#34;uclib.h\u0026#34; #include \u0026#34;heos.h\u0026#34; void USERFUNCTION_EXPORT uclib() { /* Register airDensity as a field function */ /* The reinterpret_cast\u0026lt;\u0026gt; keyword is used here to prompt C++ mandatory type conversion to avoid compiler errors */ ucfunc(reinterpret_cast\u0026lt;void*\u0026gt;(airDensity), \u0026#34;ScalarFieldFunction\u0026#34;, \u0026#34;HEOS::Air Density (kg/m^3)\u0026#34;); // Custom field functions are dimensionless by default, so it is best to add units after the name. ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airDensity), \u0026#34;Cell\u0026#34;, \u0026#34;Pressure\u0026#34;, sizeof(CoordReal)); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airDensity), \u0026#34;Cell\u0026#34;, \u0026#34;Temperature\u0026#34;, sizeof(CoordReal)); /* Register airViscosity as a field function */ ucfunc(reinterpret_cast\u0026lt;void*\u0026gt;(airViscosity), \u0026#34;ScalarFieldFunction\u0026#34;, \u0026#34;HEOS::Air Viscosity (Pa-s)\u0026#34;); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airViscosity), \u0026#34;Cell\u0026#34;, \u0026#34;Pressure\u0026#34;, sizeof(CoordReal)); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airViscosity), \u0026#34;Cell\u0026#34;, \u0026#34;Temperature\u0026#34;, sizeof(CoordReal)); /* Register airConductivity as a field function */ ucfunc(reinterpret_cast\u0026lt;void*\u0026gt;(airConductivity), \u0026#34;ScalarFieldFunction\u0026#34;, \u0026#34;HEOS::Air Thermal conductivity (W/m-K)\u0026#34;); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airConductivity), \u0026#34;Cell\u0026#34;, \u0026#34;Pressure\u0026#34;, sizeof(CoordReal)); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airConductivity), \u0026#34;Cell\u0026#34;, \u0026#34;Temperature\u0026#34;, sizeof(CoordReal)); } This project creates three new field functions. The ucfunc function is used to register the user function as a custom field functions of STAR-CCM+. The ucarg function is used to register the field function (or variable) of STAR-CCM+ as the input parameter of the user function. In this way, when the custom field functions is called, the pressure and temperature output variables can be automatically called to update.\nIn addition to custom field functionss, the user library also supports user-defined boundary conditions and region configurations. For details, please refer to the official documentation 6.\n3. Compile dynamic link library 3.1 Compile the project through VSCode+CMake If the CMake extension is installed in VSCode, you can directly call CMake to compile and generate dynamic link library files.\nFirst, define the search path for CMake to call STAR-CCM+ and CoolProp related libraries and header files in the settings.json file.\n{ \u0026#34;cmake.configureArgs\u0026#34;: [ \u0026#34;-DSTARCCM_LIB_DIR=[starccm_install_dir]/STAR-CCM+19.06.009-R8/star/lib/win64/clang17.0vc14.2-r8/lib\u0026#34;, \u0026#34;-DCOOLPROP_SRC_DIR=E:/Development/starccm/CoolProp\u0026#34;, \u0026#34;-DCOOLPROP_LIB_DIR=E:/Development/starccm/CoolPropLib/static_library/Windows/64bit_Clang_19.1.1\u0026#34;, \u0026#34;-DUSE_DOUBLE_PRECISION=ON\u0026#34; ] } Use the CMake button on the left side of VSCode to bring up the CMake configuration panel, select the build tool chain (select clang-cl here) and the compilation mode (Debug or Release). The CMake configuration information is displayed in the output window. Use the Ctrl+Shift+P shortcut key to bring up the command panel and enter CMake: Build to generate the project. After the generation is completed, the output window displays the generated dynamic link library path. Copy it to the STAR-CCM+ project directory and load it through the graphical interface. After successful loading, you can see the dynamic link library information. You can also see the generated custom field function in the field function. Call the custom field function to update the scalar field and display it. 3.2 Compile the project through CMake+ command line Since STAR-CCM+ user library depend on the operating system platform, dynamic link libraries compiled for different operating systems cannot be used interchangeably. When a project file needs to be calculated on a supercomputer, the user library source code file needs to be uploaded to the supercomputer and recompiled. Different supercomputer platforms have different Linux distributions and system dependencies. It is recommended that you compile a dynamic link library specifically for the supercomputer platform before calculation, and then call the compiled dynamic link library on the platform to perform related calculations.\nOn Linux platform, compile the user library dynamic link library using the following command.\n# Enter the project directory cd starccm_coolprop # Create a working directory mkdir -p build \u0026amp;\u0026amp; cd build # Configure the compilation file and link to the CoolProp dynamic library cmake .. -G \u0026#34;Ninja\u0026#34; -DUSE_DOUBLE_PRECISION=ON \\ -DCOOLPROP_LIB_DIR=${HOME}/Share/code/CoolPropLib/static_library/Linux/64bit_GNU_13.3.0 \\ -DCOOLPROP_SRC_DIR=${HOME}/Share/code/CoolProp \\ -DSTARCCM_LIB_DIR=${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/gnu11.4-r8/lib # Compile dynamic library cmake --build . --config Release # Install cmake --install . --prefix $PWD/../../UserLib After compilation is complete, a dynamic link library is output and loaded through the graphical interface. Automatically generated custom field functionss. Update the cloud map using custom field functionss. 4. FAQ 4.1 Dynamic link library dependencies If the compilation configuration uses the CoolProp dynamic library, the final output dynamic link library will depend on the CoolProp dynamic library file. You need to add the folder where CoolProp.dll (libCoolProp.so.X under Linux) is located to the system environment variable (PATH or LD_LIBRARY_PATH). In order to save this trouble, the CoolProp static library is used in this article. The compiled dynamic link library does not depend on the CoolProp dynamic library file, but it does not rule out that some projects still need to link to the CoolProp dynamic library.\nAnother common dependency problem is that when compiling with the GCC compiler (MinGW) on the Windows platform, regardless of whether CoolProp links to a static library or a dynamic library, the final output dynamic link library needs to be linked to the three files libstdc++-6.dll, libgcc_s_seh-1.dll and libwinpthread-1.dll that come with MinGW (different MinGW versions may differ). Therefore, it is best to add the MinGW runtime path to the system environment variables to ensure that STAR-CCM+ can search for all dependencies and call the functions in the dynamic link library normally.\nIf running on Linux and relying on other dynamic link libraries, add the -ldlibpath option to the command line when starting STAR-CCM+, and add the path to the dependent dynamic library file after this keyword.\nstarccm+ -ldlibpath [path-to--shared-file] 4.2 GLIBCXX versions STAR-CCM+ comes with a set of C++ standard libraries. If the C++ standard library that comes with the user\u0026rsquo;s Linux system is newer than the version that comes with STAR-CCM+, the compiled dynamic link library will output the following error message when loaded into STAR-CCM+:\nLoading user library /home/xxxx/Share/code/UserLib/lib/libupcp.so Could not load user library /home/xxxx/Share/code/UserLib/lib/libupcp.so /home/xxxx/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64/libstdc++.so.6: version `GLIBCXX_3.4.32\u0026#39; not found (required by /home/xxxx/Share/code/UserLib/lib/libupcp.so) Because STAR-CCM+ comes with its own C++ standard link library, but its GLIBCXX version is lower than the version on the system. When compiling, if no special specification is made, the compiler will link the link library on the system by default. When loading STAR-CCM+, the loaded dynamic link library will be linked to the standard library that comes with STAR-CCM+, resulting in an error.\nOne solution is to explicitly specify the link library search path as the STAR-CCM+ built-in path through LD_LIBRARY_PATH during compilation.\nexport LD_LIBRARY_PATH=${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64:$LD_LIBRARY_PATH However, this approach will cause problems in the system tool chain and compilation will not be able to proceed normally. Another solution is to edit the CMakeLists.txt file, add the following content, and set the link library search path through the link_directories keyword.\n# Link with STARCCM CXX LIB link_directories( \u0026#34;${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64\u0026#34; ) Then compile normally according to the method 3.2 . Check the compiled dynamic link library dependencies and link to the library that comes with STAR-CM+. The problem is solved. 4.3 Computational efficiency We establish a basic STAR-CCM+ model to illustrate the issue of computational efficiency.\nCreate a new sim file, a two-dimensional axisymmetric problem, to simulate the heating of air in the pipe. The basic model state equation uses an ideal gas, the specific heat capacity is a constant, and the thermal conductivity and dynamic viscosity are based on Sutherland’s Law. The grid size is 3629. Four cores solve the problem in parallel for 3000 steps, which takes 85 seconds. Then we replace the ideal gas in the physical model with user-defined EOS, load the compiled dynamic link library, and define the gas density, dynamic viscosity and thermal conductivity through the custom field functions generated by the dynamic link library. The specific heat capacity remains constant. The mesh size remains unchanged. The same 4 cores solved 3000 steps in parallel, taking 235 seconds, nearly doubling the time. The problem is with the user function we registered. Because each function and each step of the calculation must call the function from CoolProp, traverse the grid cells to obtain the temperature and pressure values, and then calculate the output result through the CoolProp function. Each function executes the above operations in a loop. The loop operation is independent of the solver iterative calculation itself. The larger the grid size, the longer the loop execution time. Moreover, we have defined three functions with the same functionality, which is equivalent to executing three additional loop operations in addition to the solver iterative calculation.\nFor some larger models and projects that are sensitive to solution time, it is generally not recommended to define physical properties through user library with CoolProp. However, in terms of post-processing, it is still very useful to update the scalar scene and output the results by calling the CoolProp library.\n使用用户程序 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAvailable Wrappers \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStatic Library \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC++ Sample Code \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n类型定义(C) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n用户程序示例 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-04-29-use-vscode-develop-starccm-user-library-with-coolprop/","tags":[{"LinkTitle":"C++","RelPermalink":"/tags/c++/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/tags/star-ccm+/"}],"title":"Use VSCode to develop STAR-CCM+ user library: Calculate physical properties through CoolProp"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/categories/cfd/"}],"content":"The STAR-CCM+ Simulation Assistant is equivalent to a plug-in that encapsulates Java macro commands. By calling macro commands, some procedural operations are performed. Compared with directly executing Java macro files, it can interact with users better and is obviously more friendly to users who are not familiar with Java. Because the simulation assistant is mostly used in pre- and post-processing, many consulting companies tend to adopt the same approach when developing customized plug-ins for this purpose.\nThe official document uses NetBeans to demonstrate the process of creating a simulation assistant project, but the NetBeans version demonstrated is very old. The official document is updated slowly and the description is too general. Here, VSCode is used to re-implement and demonstrate it again. For relevant configurations, please refer to Debug STAR-CCM+ macros with VSCode . The following demonstration materials are from the official STAR-CCM+ tutorial file.\n1. Create a simulation assistant project Refer to Debug STAR-CCM+ macros with VSCode to create a Java project. Select No build tools as the project type and name it according to your needs.\nAfter the project is created, edit the settings.json file under [project folder]/.vscode and add the following directories to the project dependencies1:\n[STAR-CCM+_Installation]/star/lib/java/platform/core [STAR-CCM+_Installation]/star/lib/java/platform/core/locale [STAR-CCM+_Installation]/star/lib/java/platform/lib [STAR-CCM+_Installation]/star/lib/java/platform/modules [STAR-CCM+_Installation]/star/lib/java/platform/modules/ext [STAR-CCM+_Installation]/star/lib/java/platform/modules/locale Because the simulation assistant needs to be loaded into STAR-CCM+, you need to ensure that the jdk versions of the two are consistent. Edit the settings.json file in [project folder]/.vscode and add a line of settings to specify the jdk version that comes with STAR-CCM+.\n\u0026#34;java.jdt.ls.java.home\u0026#34;: \u0026#34;[STAR-CCM+_Installation]/jdk/[platform]/jdk[version]\u0026#34; Confirm the jdk version in JAVA PROJECTS and make sure it is consistent with the version provided by STAR-CCM+. 2. Project Develop Delete the automatically generated source files under [project folder]/src, create a folder Assistant, and copy and paste the Java source files in the official tutorial into it 2. Create a folder called XHTML under [project folder]/src and copy and paste the XHTML file from the official tutorial into it. Next, follow the guidance of the official tutorial 3 to complete the other parts.\nThe entire simulation assistant project is very simple, and the basic operation is divided into 6 steps:\nImport geometry files Create analysis domain Create analysis physics Generate volume mesh Create display scene Set the number of solution steps and run the solution. Each step of the operation corresponds to a java file, and each java file corresponds to an XHTML. The java file is similar to the macro mentioned above, and can be quickly edited by recording macros + copying and pasting. The XHTML file provides user interaction instructions and operation entrances, allowing users to call corresponding operation commands according to prompts. When writing your own project file, it is recommended to modify it based on the official tutorial file, copy and paste the macro snippet you recorded into it and edit it, trying to meet the template requirements of the original file. For specific writing rules, please refer to the official document 4.\n3. Release and test The simulation assistant can be released in the form of jar packaging, and use the VSCode JAVA PROJECTS function to package and output the jar file. Just choose the bin directory and do not repeat packaging of other dependencies. The packaged jar file is in the project folder. After creating or loading a sim file in STAR-CCM+, load the Simulation Assistant through the File menu. Run the full test. Because the tutorial cases are old, many APIs have been deprecated in the new version, and there are some errors, so you need to adjust the code yourself.\n4. Debugging the simulation assistant The debugging of the simulation assistant is exactly the same as the debugging process of the Java macro in Debug STAR-CCM+ macros with VSCode . The only difference is that the loading method has changed from \u0026ldquo;playing macros\u0026rdquo; to \u0026ldquo;loading simulation assistant\u0026rdquo;.\nThe code of the launch.json file in the .vscode directory of the project folder is as follows:\n{ \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Debug (Attach)\u0026#34;, \u0026#34;projectName\u0026#34;: \u0026#34;starccm\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;hostName\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 8765 } ] } Start the STAR-CCM+ main program through the command line:\n\u0026lt;InstallationDirectory\u0026gt;/star/bin/starccm+ -jvmargs \u0026#39;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8765\u0026#39; Before entering debugging and starting execution, add breakpoints in the source code. When the code reaches the breakpoint, VSCode will prompt a break, and you can check the variables at the breakpoint. It should be noted that if the simulation assistant is released through jar packaging, make sure that the jar package version is consistent with the current source code before entering debugging.\n创建 NetBeans 项目 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n将 Java 包、Java 类和 XHTML 文件添加到模拟助手项目 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n模拟助手：管道内流模拟辅助 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n开发模拟助手 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-04-22-use-vscode-coding-starccm-assistant/","tags":[{"LinkTitle":"Java","RelPermalink":"/tags/java/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/tags/star-ccm+/"}],"title":"Develop STAR-CCM+ simulation assistant using VSCode"},{"categories":[],"content":"Hi, I\u0026rsquo;m Andrew Moa, an engineer who has worked for top domestic refrigeration companies and leading automotive enterprises. I have served as a researcher and simulation engineer.\nContact: Andrew.Moa2005@163.com ","permalink":"https://andrewmoa.site/about/","tags":[],"title":"About"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/categories/cfd/"}],"content":"As mentioned earlier, the recording and writing of STAR-CCM+ macro files are essentially Java files, so it can be developed and debugged using Java programming methods. If complex business scenarios are involved, additional functions need to be added. The program itself is relatively complex, and it is difficult to wait until the entire program is written before testing it. It is inevitable to debug during the development process. The development tool used by the official document is the old version of NetBeans. Many functions have changed. In addition, the official document description is too simple. Most people are still confused about the debugging process of STAR-CCM+ after reading it.VSCode As a leader in emerging IDE, you can not only support Java programming through expansion, but also copilot To expand the integrated powerful AI programming capabilities, this article uses VSCode to demonstrate the debugging process of STAR-CCM+ macro files.\n1. VSCode configuration First of all, you need to install the Java extension in VSCode, and at least the following must be installed:\nLanguage Support for Java(TM) by Red Hat Debugger for Java Project Manager for Java You can also install this expansion package directly and install all the Java expansions you need at one time:Extension Pack for Java Download a JDK and install it. If you don\u0026rsquo;t want to download the JDK, you can also find the JDK that comes with the installation package in the STAR-CCM+ installation path and add it to the environment variables. 2. Create a Java project Enter in the VSCode command panel (Ctrl+Shift+P) Java: Create Java Project, create a new java project. Project type selection No build tools . In the pop-up dialog box, select the project folder location, and then enter the project name (for example: starccm), and the new project is completed, you can see the structure of the java project. Copy the [STAR-CCM+_Installation]/star/lib/java/platform/modules/ext folder to the lib folder of the new project1. Copy the previously recorded or written java macros to the java file generated by the project. You can see that the syntax check and code highlighting prompts have taken effect. At this time, modifying the code can automatically complete the supplementary code snippet, or you can call copilot to automatically fill in and correct code errors through Ctrl+I. 3. Configuration debugging In the project folder .vscode Modify in the directory launch.json File, edit code as follows.\n{ \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Debug (Attach)\u0026#34;, \u0026#34;projectName\u0026#34;: \u0026#34;starccm\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;hostName\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 8765 } ] } if .vscode No below launch.json This file can be clicked on the left side of VSCode运行与调试button, generate the file according to the prompts, and edit it as above. Start the STAR-CCM+ main program through the command line, be careful to starccm+ The startup parameters are attached later. address The port number behind should be as above launch.json The port numbers filled in the file are the same.\n\u0026lt;InstallationDirectory\u0026gt;/star/bin/starccm+ -jvmargs \u0026#39;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8765\u0026#39; After the above configuration is completed, you can open or create a new sim file in STAR-CCM+ to test the java program we wrote.\nThen click on the left side of VSCode 运行与调试 button to enter debug mode. After successfully entering debug mode, the call stack and breakpoint information will be displayed on the modulation page. If you fail to enter debug mode, a dialog box will pop up. It is necessary to confirm whether STAR-CCM+ has normal startup, whether there are additional startup parameters, and launch.json Is there any error in writing the file? 4. Debugging process Below is a simple java file to demonstrate the debugging process.\n// The following line is automatically generated during recording. It has no effect and can be commented out. // package macro; import star.common.*; public class testCCMDebug extends StarMacro { // Macro operation entry public void execute() { pringMsg(); } // Demonstrates the function of printing messages private void pringMsg() { Simulation simulation_0 = getActiveSimulation(); simulation_0.println(\u0026#34;Hello, starccm+!\u0026#34;); simulation_0.println(\u0026#34;This is a macro example.\u0026#34;); simulation_0.println(\u0026#34;This macro is used to test STAR-CCM+ debug mode.\u0026#34;); String fileName = simulation_0.getPresentationName() + \u0026#34;.sim\u0026#34;; simulation_0.println(\u0026#34;The simulation file name is: \u0026#34; + fileName); } } Then add breakpoints to it. The key point is 2, Be sure to use \u0026ldquo;play macro\u0026rdquo; to load the java file in STAR-CCM+ . When the execution reaches a breakpoint, an interrupt will be prompted in VSCode, so that the debugging execution process can be controlled through VSCode. Final execution effect: The debugging execution reaches a breakpoint, and STAR-CCM+ will enter a pause state during the process. At this time, clicking the Cancel button (the red X next to the progress bar) is useless. After the VSCode debugging is completed, if you want to enter debugging again, you need to use \u0026ldquo;play macro\u0026rdquo; in STAR-CCM+ to reload the java file.\n使用 IDE 调试宏 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n学习STAR-CCM+编程语言：在Eclipse中进行二次开发调试 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-04-18-use-vscode-debug-starccm-marco/","tags":[{"LinkTitle":"Java","RelPermalink":"/tags/java/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/tags/star-ccm+/"}],"title":"Debug STAR-CCM+ macros with VSCode"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/categories/cfd/"}],"content":"STAR-CCM+ macro is essentially a java file, and the syntax is no different from ordinary java. Using macros can help us simplify the processing process, especially some repetitive process operations. Completing some process operations by writing macro files can greatly liberate human resources, and even complete certain operations during the solution process.\n1. Record macros The essence of STAR-CCM+ macro is a collection of various operation commands in the analysis process. Instead of explaining the complex and cumbersome java syntax + API from scratch, it is better to start with engineering issues directly.\nThe entrance to the STAR-CCM+ macro operation is in the toolbar in the upper left corner. From left to right, the \u0026ldquo;Play Macro\u0026rdquo;, \u0026ldquo;Start Recording\u0026rdquo;, \u0026ldquo;Pause Recording\u0026rdquo; and \u0026ldquo;Stop Recording\u0026rdquo; buttons. The relevant operation options can also be found in the \u0026ldquo;File\u0026rdquo; menu. Clicking \u0026ldquo;Start Recording\u0026rdquo; will bring up a dialog box that prompts the location of the macro to be saved. If some scene-based graphics operations are used during the process, you can check \u0026ldquo;Including Graphic Commands\u0026rdquo;; but it is not recommended to check it in general, especially when it involves supercomputer submissions. Next, you can see the code for recording the java process in the output window. At this time, the analysis operations can be performed normally. STAR-CCM+ will convert the related operations into the corresponding java code and display them in the output window. When the analysis operation is completed, click \u0026ldquo;Stop recording\u0026rdquo; and you can see the complete java code in the output window, and the code is also saved to the java file.\nBelow is a recorded macro code example, which is used to clear the original mesh and re-divid the body mesh.\n// Simcenter STAR-CCM+ macro: reGenerateMesh.java // Written by Simcenter STAR-CCM+ 19.06.009 // The two lines of comments above are generated during recording, recording the macro file name and STAR-CCM+ version information. package macro; import java.util.*; import star.common.*; import star.meshing.*; // The class name and file name should be consistent public class reGenerateMesh extends StarMacro { // This function is the entry point of the entire macro public void execute() { execute0(); } // This function performs mesh clearing and meshing operations. private void execute0() { Simulation simulation_0 = getActiveSimulation(); MeshPipelineController meshPipelineController_0 = simulation_0.get(MeshPipelineController.class); // Clear the original grid meshPipelineController_0.clearGeneratedMeshes(); // Generate Volume Mesh meshPipelineController_0.generateVolumeMesh(); } } For the specific code, you can check the help file of STAR-CCM+, but most API naming is more intuitive and can be inferred based on their naming. If the operations to be performed are complicated, there are many processes. It is recommended to decompose the process, record different java files separately, and then combine them. If the recording operation requires running the solver, you can change to a simple model for recording, or you can lower the parameters a little to save time and resources.\n2. Write macros With the Java file recorded above, the following writing is much easier.\nNext, combine these modules and write a new Java macro file. The function is to automatically divide the grid after reading the sim file, set parameters and run the solution. After the solution is completed, the corresponding drawing and scene files are output, and the result file with the specified file name is saved.\nIt is recommended to use IDE tools here. With the help of syntax checking, code highlighting and automatic completion, it can reduce the chance of errors, which is very friendly to beginners. The official tutorial and help file of STAR-CCM+ demonstrates NetBeans. Some people also like to use Eclipse or IntelliJ IDEA. There is no big difference here, so choose according to your personal preferences.\n// Simcenter STAR-CCM+ macro: meshAndRun.java // Written by Simcenter STAR-CCM+ 19.06.009 package macro; import java.util.*; import star.common.*; import star.meshing.*; import star.base.neo.*; import star.vis.*; public class meshAndRun extends StarMacro { // Input parameters for analysis double env_temp = 25.0; // Unit: C double fan_speed = 2450.0; // Unit: rpm // Some parameters boolean autoSave = true; int maxStep = 10; int autoSaveStep = 1000; boolean saveAsResultsFile = true; // It is best to write the full path here, otherwise it will be saved to the ${HOME} directory by default under Windows String resultsFileName = \u0026#34;final_results.sim\u0026#34;; // Macro operation entry public void execute() { // Perform analysis operations if (autoSave) { enableAutoSave(); } else { disableAutoSave(); } generateMesh(); modifyParameters(); modifyMaxStep(); run(); exportPlot(); exportScene(); if (saveAsResultsFile) { saveAs(); } } // Generate Volume Mesh private void generateMesh() { Simulation simulation_0 = getActiveSimulation(); MeshPipelineController meshPipelineController_0 = simulation_0.get(MeshPipelineController.class); // meshPipelineController_0.clearGeneratedMeshes(); meshPipelineController_0.generateVolumeMesh(); } // Set up autosave private void enableAutoSave() { Simulation simulation_0 = getActiveSimulation(); AutoSave autoSave_0 = simulation_0.getSimulationIterator().getAutoSave(); autoSave_0.setAutoSaveBatch(true); autoSave_0.setAutoSaveMesh(true); AutoSaveFileSet autoSaveFileSet_0 = ((AutoSaveFileSet) autoSave_0.getAutoSaveFileSetManager() .getObject(\u0026#34;Auto Save File Set 1\u0026#34;)); StarUpdate starUpdate_0 = autoSaveFileSet_0.getStarUpdate(); IterationUpdateFrequency iterationUpdateFrequency_0 = starUpdate_0.getIterationUpdateFrequency(); IntegerValue integerValue_0 = iterationUpdateFrequency_0.getIterationFrequencyQuantity(); integerValue_0.getQuantity().setValue(autoSaveStep); starUpdate_0.setEnabled(true); } // Cancel autosave private void disableAutoSave() { Simulation simulation_0 = getActiveSimulation(); AutoSave autoSave_0 = simulation_0.getSimulationIterator().getAutoSave(); autoSave_0.setAutoSaveMesh(false); autoSave_0.setAutoSaveBatch(false); } // Setting the input parameters for the analysis private void modifyParameters() { Simulation simulation_0 = getActiveSimulation(); ScalarGlobalParameter scalarGlobalParameter_0 = ((ScalarGlobalParameter) simulation_0 .get(GlobalParameterManager.class).getObject(\u0026#34;env_temp\u0026#34;)); Units units_0 = ((Units) simulation_0.getUnitsManager().getObject(\u0026#34;C\u0026#34;)); scalarGlobalParameter_0.getQuantity().setValueAndUnits(env_temp, units_0); ScalarGlobalParameter scalarGlobalParameter_1 = ((ScalarGlobalParameter) simulation_0 .get(GlobalParameterManager.class).getObject(\u0026#34;fan_speed\u0026#34;)); Units units_1 = ((Units) simulation_0.getUnitsManager().getObject(\u0026#34;rpm\u0026#34;)); scalarGlobalParameter_1.getQuantity().setValueAndUnits(fan_speed, units_1); } // Set the maximum number of steps private void modifyMaxStep() { Simulation simulation_0 = getActiveSimulation(); StepStoppingCriterion stepStoppingCriterion_0 = ((StepStoppingCriterion) simulation_0 .getSolverStoppingCriterionManager().getSolverStoppingCriterion(\u0026#34;Maximum Steps\u0026#34;)); IntegerValue integerValue_0 = stepStoppingCriterion_0.getMaximumNumberStepsObject(); integerValue_0.getQuantity().setValue(maxStep); } // Run the solver private void run() { Simulation simulation_0 = getActiveSimulation(); // ResidualPlot residualPlot_0 = ((ResidualPlot) // simulation_0.getPlotManager().getPlot(\u0026#34;Residuals\u0026#34;)); // residualPlot_0.openInteractive(); simulation_0.getSimulationIterator().run(); } // Output plot private void exportPlot() { Simulation simulation_0 = getActiveSimulation(); MonitorPlot monitorPlot_0 = ((MonitorPlot) simulation_0.getPlotManager().getPlot(\u0026#34;mass_flow\u0026#34;)); // You can use relative paths or absolute paths here. monitorPlot_0.export(resolvePath(\u0026#34;mass_flow.csv\u0026#34;), \u0026#34;,\u0026#34;); } // Output scene private void exportScene() { Simulation simulation_0 = getActiveSimulation(); Scene scene_0 = simulation_0.getSceneManager().getScene(\u0026#34;Geometry\u0026#34;); scene_0.export3DSceneFileAndWait(resolvePath(\u0026#34;Geometry.sce\u0026#34;), \u0026#34;Geometry\u0026#34;, \u0026#34;\u0026#34;, false, SceneFileCompressionLevel.OFF); } // Save the results private void saveAs() { Simulation simulation_0 = getActiveSimulation(); simulation_0.saveState(resultsFileName); } } 3. Run the macro After writing, you can try running the following to see if there are any errors reported. If you try to run the solution, you can reduce the number of solutions to see if the output file is correct. Change the conditions and parameters, test them several times, and after complete tests, there will be no errors reported before it can be used in the production environment.\nIf you want to submit an overcomputer operation, you should-batchAppend the java macro file path 1 to the switch.\nstarccm+ [path-to-sim-file] -batch [path-to-java-file] -np [number-of-threads] ... 编制应用程序脚本 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-04-17-starccm-record-and-write-marco/","tags":[{"LinkTitle":"Java","RelPermalink":"/tags/java/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/tags/star-ccm+/"}],"title":"STAR-CCM+ macro file recording and writing"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"Recently, when submitting calculations with OpenFOAM and SU2, the error message \u0026ldquo;Authorization required, but no authorization protocol specified\u0026rdquo; repeatedly appeared. Although the calculation was eventually completed by ignoring it, the constant error message made me feel uneasy.\nI suspected that it was a problem with OpenMPI, and verified it with the following command:\nmpirun -np 2 hostname Sure enough, an error message appeared: After looking through the information online, the more reliable solution is this 1: Add the following environment variables to the Slurm script:\nexport HWLOC_COMPONENTS=-gl Test it and the error problem is solved: github issue \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-04-16-solve-openmpi-warning/","tags":[{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Openfoam","RelPermalink":"/tags/openfoam/"},{"LinkTitle":"Su2","RelPermalink":"/tags/su2/"}],"title":"Solve the \"Authorization required, but no authorization protocol specified\" error message of OpenMPI"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"Recently, a problem was discovered that Abaqus and CFX cannot perform calculations on Windows shared folders mounted on Linux. Linux is deployed on a virtual machine. Although there will be no error when starting the calculation program on the local path of the virtual machine, it will cause the virtual disk to occupy more space and have a certain impact on the read and write efficiency.\n1. SMB version problem 1.1 SMBv3 When using cifs to mount a shared folder before, I did not pay close attention to the version of the SMB protocol1. The following command line does not specify the protocol version:\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o uid=xxx,gid=xxx,username=xxx Use the mount command to check that the default connection protocol is version 3.1: When running the Abaqus program in the mount directory of the 3.1 version protocol, the calculation fails and the following error is reported:\nmpirun: Warning one or more remote shell commands exited with non-zero status, which may indicate a remote access problem. driverExceptions.AbaqusExecutionError: (\u0026#39;SIMULIA Job Layout Engine\u0026#39;, 255, \u0026#39;abaqus_test\u0026#39;) !!! !!! SIM wrap utility command error: !!! System Error open: Invalid parameters !!! driverExceptions.AbaqusExecutionError: (\u0026#39;SIM Wrap-up\u0026#39;, 1, \u0026#39;abaqus_test\u0026#39;) When running the CFX solver in the mount directory of the 3.1 version protocol, the calculation fails and the following error is reported:\nAn error has occurred in cfx5solve: Error copying MMS input file mms for reading: Input/output error 1.2 SMBv2 Add the keyword vers=2.0 to the mount command line to explicitly specify the protocol version as 2.0:\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o vers=2.0,uid=xxx,gid=xxx,username=xxx Use the mount command to check and confirm that the protocol version is 2.0: Running the Abaqus program in the mount directory of the 2.0 version protocol can complete the calculation, but there is an error:\nmpirun: Warning one or more remote shell commands exited with non-zero status, which may indicate a remote access problem. driverExceptions.AbaqusExecutionError: (\u0026#39;SIMULIA Job Layout Engine\u0026#39;, 255, \u0026#39;abaqus_test\u0026#39;) Running the CFX solver in the mount directory of the 2.0 version protocol still fails to calculate, and the error message is the same as the SMBv3 version.\n1.3 SMBv1 Specify the protocol version as 1.0:\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o vers=1.0,uid=xxx,gid=xxx,username=xxx mount confirms: Running the Abaqus program in the mount directory of the 1.0 version protocol, like the SMBv2 version, can complete the calculation but there is an error message.\nRunning the CFX solver in the mount directory of the 1.0 version protocol, the calculation cannot be completed, and the error message is as follows:\nAn error has occurred in cfx5solve: Error copying MMS input file mms for reading: Operation not supported 2. SMB symbolic link problem 2.1 CFX environment variables According to the official Ansys instructions 2, CFX errors are mostly symbolic link problems. Add the following environment variables to the slurm calculation script:\nexport CFX5_DISABLE_SYMLINKS=1 Tested in SMBv1~SMBv3 protocol versions, after adding this environment variable, CFX can complete the calculation, and the problem is solved.\n2.2 Enable symbolic link function in SMBv3 Add the symbolic link option mfsymlinks3 to the mount command:\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o mfsymlinks,uid=xxx,gid=xxx,username=xxx The measured effect is the same as adding CFX environment variables in [2.1] (#21-cfx environment variables), which can solve the problem that CFX cannot calculate. However, Abaqus still reports an error and cannot complete the calculation. It seems that the problem of Abaqus is not related to symbolic links.\n3. Summary If you need to run Abaqus and CFX solvers on a Windows shared folder mounted by Linux, it is recommended to use the following command to explicitly specify the SMBv2 version and enable the symbolic link function:\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o vers=2.0,mfsymlinks,uid=xxx,gid=xxx,username=xxx The above command line can ensure the normal calculation of CFX and Abaqus.\n在 Windows 中检测、启用和禁用 SMBv1、SMBv2 和 SMBv3 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA local Linux machine is connected to a Windows machine through Samba. \u0026hellip; \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n挂载SMB协议文件系统 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-04-15-linux-mount-windows-dir-cannot-run-abaqus-and-cfx/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/tags/abaqus/"},{"LinkTitle":"Cfx","RelPermalink":"/tags/cfx/"},{"LinkTitle":"Ubuntu","RelPermalink":"/tags/ubuntu/"}],"title":"Abaqus and CFX solvers cannot be run in Linux mounts Windows shared folders"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"When Ansa is used as Abaqus preprocessor for analysis, the problem of applying bolt pretension is often encountered. In addition to the complex settings, it is also easy to make mistakes, and various problems may occur if you are not careful. The following records the Ansa bolt pretension setting process and some easy mistakes, which will be used as a reference for related analysis later.\n1. Solid+Assistant The connection between the solid bolt and the flange surface must be established first. For details, please refer to Abaqus contact settings 。 Individual bolts can be set up via the AUXILIARIES → PRTENS → Assistant Assistant. In the pop-up wizard, select Surface - Solid Elements or Surface - Solid Property, then select the bolt entity. Check the Detect and create all possible pretensions. below to automatically search for similar entities and create the same pretension. Next, select the first reference point on the bolt body. Next, select the second reference point to define the bolt pretension direction. A reference plane will be generated at the first reference point. Note that the pretension direction must be parallel to the bolt axis. Next, enter the pretension force of 50kN and check the box labeled \u0026ldquo;Fixed\u0026rdquo;. Two \u0026ldquo;STEP\u0026quot;s will be automatically generated. The first \u0026ldquo;STEP\u0026rdquo; is used to load the pretension, and the second \u0026ldquo;STEP\u0026rdquo; is used to fix the pretension. Other loads can be applied to the second \u0026ldquo;STEP\u0026rdquo;. Finally, define the normal direction of the reference plane to confirm the application of the bolt pretension. After completing the above steps, you can submit the calculation.\nWhere things can go wrong：\nIf the non-convergence is caused by contact problems, you can add contact control Contact Control→Stabilize to improve the stability of the solution. If the system still does not converge after adding contact control, it is recommended to replace the contact between the solid bolt and the flange surface with a Coupling constraint. If the calculation results show that the bolt becomes longer, it is most likely that the normal definition of the reference plane in step 6 is reversed. Simply change the normal direction of the reference plane. After the calculation is completed, open it through Meta to view the stress distribution state inside the solid bolt. 2. Beam+Assistant When the problem to be studied does not involve the strength of the bolt body, beam elements can be used to replace the bolt entity, which can reduce the amount of calculation, improve calculation efficiency, and avoid the problem of non-convergence of calculation.\nFor beam elements, you need to create a new beam PID first, define the cross section and material of the bolts, select circular as the cross section, and define the bolt radius. It is necessary to create a new beam unit through Beam (it is recommended to first create two coupling constraints through the bolt holes, and then create a new Beam unit through the center of the coupling constraints), and then assign it to the PID above. At this time, the Beam unit has only one section. Then go to Mesh and split the Beam unit into multiple segments by inserting points through Insert. Next, start the wizard, select Beam Elements, and select the middle section of the beam unit. If the beam unit was not split in the previous step, you can select the following Split Beams, which will automatically split the beam unit into 3 sections and select the middle section. Similarly, enter the pretension force of 50kN and select the fixed pretension state. Confirm creation of pretension. After completing the above steps, you can submit the calculation.\nCommon Mistakes：\nMeta does not support displaying the stress distribution of beam elements. If you want to view the stress distribution of beam elements, you need to use Abaqus Viewer to render the result file.\nYou need to turn on Render beam profiles in View → ODB Display Options to display the status of the beam elements. To display stresses for beam elements, select BEAM_STRESS. If you want to view the stress distribution state of the beam in the results, it is recommended to select the output of all variables in the *OUTPUT keyword, so that BEAM_STRESS can be viewed when rendering with Abaqus Viewer. 3. Beam+Connection For connection settings, please refer to Ansa Quick Connection . Bolt pretension can also be set through Connection, which is especially suitable for multiple bolt connections, where models need to be replaced frequently or parameters need to be adjusted. Here, the beam element is selected for the bolt body.\nThe steps to create PID are the same as step 2 in the previous chapter. Select CBEAM for Body Type, activate Create Pretension and enter the pretension value. Multiple bolt stress display: 4. Summary Solid+Assistant : The bolt body stress result is relatively more accurate, but the calculation is large and it is easy to fail to converge. Beam+Assistant : The calculation is small and it is easier to converge, but it can only be used when the bolt body strength is not examined. Beam+Connection : The calculation is small, it is easy to converge, and the change and replacement operations are more convenient. It also cannot directly examine the bolt body strength. ","permalink":"https://andrewmoa.site/post/2025-04-03-ansa-abaqus-bolt-pretension/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/tags/abaqus/"},{"LinkTitle":"Ansa","RelPermalink":"/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Ansa as Abaqus pre-processing analysis bolt pretension setting"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/categories/code/"}],"content":"For frequent Markdown users, image storage has always been a challenge. Markdown itself does not support embedded images, traditionally requiring external links to local or online files. However, publishing articles online raises storage issues. While image hosting services exist, the process can be cumbersome.\nortunately, Markdown supports image rendering via Base64 encoding. This tool Image2Base64 simplifies conversions between image files and Base64 encoding.\n1. Features Below is the graphical interface of the software (this screenshot uses Base64 rendering): The left panel displays images. You can drag and drop images to open them, supporting formats like BMP, PNG, and JPG (GIF animations are not supported). Right-click menus provide basic operations like zooming and resetting.\nThe Paste button below the left panel reads image data from the clipboard, enabling seamless integration with screenshot tools.\nThe right panel displays the generated Base64 encoding. The \u0026ldquo;Markdown\u0026rdquo; checkbox adds Markdown image syntax tags. Clicking Copy copies the text data to the clipboard for direct pasting into MD files.\nNote: The dropdown menu next to Copy allows selecting the image format for Base64 encoding. Base64 length varies depending on the original image format, complexity, and compression.\nThe Save as button below the right panel saves images in their original format or as Base64-encoded TXT files. The bidirectional arrows in the middle convert images to Base64 or vice versa. Important: When converting Base64 to images, ensure the input contains no Markdown syntax tags to avoid errors.\nTested with PNG format, this tool successfully renders images in Marktext and Joplin . Other formats depend on the Markdown editor\u0026rsquo;s rendering engine. For screenshots, PNG is recommended.\n2. Code 2.1 C++ Implementation The software is based on Qt6. Image format encoding, rendering and base64 conversion are all implemented through Qt.\nImage display is implemented through QLabel, which overloads the QLabel class1 and makes some adjustments.\nphotolabel.h：\n#ifndef PHOTOLABEL_H #define PHOTOLABEL_H #include \u0026lt;QObject\u0026gt; #include \u0026lt;QLabel\u0026gt; #include \u0026lt;QMenu\u0026gt; #include \u0026lt;QDragEnterEvent\u0026gt; #include \u0026lt;QDropEvent\u0026gt; class PhotoLabel : public QLabel { Q_OBJECT public: explicit PhotoLabel(QWidget* parent = nullptr); void openFile(QString); //打开图片文件 void clearShow(); //清空显示 void setImage(QImage\u0026amp;); //设置图片 void openAction(); //调用打开文件对话框 void pasteAction(); //粘贴来自剪贴板的图片 const QImage\u0026amp; getImage(); //调用存储的图片数据 signals: // 图片加载成功信号 void imageLoadSuccess(); protected: void contextMenuEvent(QContextMenuEvent* event) override; //右键菜单 void paintEvent(QPaintEvent* event) override; //QPaint画图 void wheelEvent(QWheelEvent* event) override; //鼠标滚轮滚动 void mousePressEvent(QMouseEvent* event) override; //鼠标摁下 void mouseMoveEvent(QMouseEvent* event) override; //鼠标松开 void mouseReleaseEvent(QMouseEvent* event) override; //鼠标发射事件 //拖放文件 void dragEnterEvent(QDragEnterEvent* event) override; void dragMoveEvent(QDragMoveEvent* event) override; void dropEvent(QDropEvent* event) override; private slots: void initWidget(); //初始化 void onSelectImage(); //选择打开图片 void onPasteImage(); //选择粘贴图片 void onZoomInImage(); //图片放大 void onZoomOutImage(); //图片缩小 void onPresetImage(); //图片还原 private: QImage m_image; //显示的图片 qreal m_zoomValue = 1.0; //鼠标缩放值 int m_xPtInterval = 0; //平移X轴的值 int m_yPtInterval = 0; //平移Y轴的值 QPoint m_oldPos; //旧的鼠标位置 bool m_pressed = false; //鼠标是否被摁压 QString m_localFileName; //文件名称 QMenu* m_menu; //右键菜单 }; #endif // PHOTOLABEL_H photolabel.cpp：\n#include \u0026#34;photolabel.h\u0026#34; #include \u0026lt;QPainter\u0026gt; #include \u0026lt;QDebug\u0026gt; #include \u0026lt;QWheelEvent\u0026gt; #include \u0026lt;QFileDialog\u0026gt; #include \u0026lt;QClipboard\u0026gt; #include \u0026lt;QApplication\u0026gt; #include \u0026lt;QMimeData\u0026gt; #include \u0026lt;QFileInfo\u0026gt; #include \u0026lt;QMessageBox\u0026gt; PhotoLabel::PhotoLabel(QWidget* parent) :QLabel(parent) { initWidget(); } void PhotoLabel::initWidget() { //初始化右键菜单 m_menu = new QMenu(this); QAction* loadImage = new QAction; loadImage-\u0026gt;setText(tr(\u0026#34;\u0026amp;Open new...\u0026#34;)); connect(loadImage, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onSelectImage); m_menu-\u0026gt;addAction(loadImage); QAction* pasteImage = new QAction; pasteImage-\u0026gt;setText(tr(\u0026#34;\u0026amp;Paste image\u0026#34;)); connect(pasteImage, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onPasteImage); m_menu-\u0026gt;addAction(pasteImage); m_menu-\u0026gt;addSeparator(); QAction* zoomInAction = new QAction; zoomInAction-\u0026gt;setText(tr(\u0026#34;Zoom in \u0026amp;+\u0026#34;)); connect(zoomInAction, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onZoomInImage); m_menu-\u0026gt;addAction(zoomInAction); QAction* zoomOutAction = new QAction; zoomOutAction-\u0026gt;setText(tr(\u0026#34;Zoom out \u0026amp;-\u0026#34;)); connect(zoomOutAction, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onZoomOutImage); m_menu-\u0026gt;addAction(zoomOutAction); QAction* presetAction = new QAction; presetAction-\u0026gt;setText(tr(\u0026#34;\u0026amp;Reset\u0026#34;)); connect(presetAction, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onPresetImage); m_menu-\u0026gt;addAction(presetAction); m_menu-\u0026gt;addSeparator(); /* QAction *clearAction = new QAction; clearAction-\u0026gt;setText(\u0026#34;Clear\u0026#34;); connect(clearAction, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::clearShow); m_menu-\u0026gt;addAction(clearAction); */ } void PhotoLabel::openFile(QString path) { if (path.isEmpty()) { return; } if (!m_image.load(path)) { QMessageBox::warning(this, tr(\u0026#34;Error\u0026#34;), tr(\u0026#34;Cannot load file!\u0026#34;)); return; } m_zoomValue = 1.0; m_xPtInterval = 0; m_yPtInterval = 0; m_localFileName = path; emit this-\u0026gt;imageLoadSuccess(); update(); } void PhotoLabel::clearShow() { m_localFileName = \u0026#34;\u0026#34;; m_image = QImage(); this-\u0026gt;clear(); } void PhotoLabel::setImage(QImage\u0026amp; img) { if (img.isNull()) { return; } m_zoomValue = 1.0; m_xPtInterval = 0; m_yPtInterval = 0; m_localFileName = \u0026#34;\u0026#34;; m_image = img.copy(0, 0, img.width(), img.height()); emit imageLoadSuccess(); update(); } void PhotoLabel::openAction() { this-\u0026gt;onSelectImage(); } void PhotoLabel::pasteAction() { this-\u0026gt;onPasteImage(); } const QImage\u0026amp; PhotoLabel::getImage() { return m_image; } void PhotoLabel::paintEvent(QPaintEvent* event) { if (m_image.isNull()) return QWidget::paintEvent(event); QPainter painter(this); // 根据窗口计算应该显示的图片的大小 int width = qMin(m_image.width(), this-\u0026gt;width()); int height = int(width * 1.0 / (m_image.width() * 1.0 / m_image.height())); height = qMin(height, this-\u0026gt;height()); width = int(height * 1.0 * (m_image.width() * 1.0 / m_image.height())); // 平移 painter.translate(this-\u0026gt;width() / 2 + m_xPtInterval, this-\u0026gt;height() / 2 + m_yPtInterval); // 缩放 painter.scale(m_zoomValue, m_zoomValue); // 绘制图像 QRect picRect(-width / 2, -height / 2, width, height); painter.drawImage(picRect, m_image); QWidget::paintEvent(event); } void PhotoLabel::wheelEvent(QWheelEvent* event) { int value = event-\u0026gt;angleDelta().y() / 15; if (value \u0026lt; 0) //放大 onZoomInImage(); else //缩小 onZoomOutImage(); update(); } void PhotoLabel::mousePressEvent(QMouseEvent* event) { m_oldPos = event-\u0026gt;pos(); m_pressed = true; this-\u0026gt;setCursor(Qt::ClosedHandCursor); //设置鼠标样式 } void PhotoLabel::mouseMoveEvent(QMouseEvent* event) { if (!m_pressed) return QWidget::mouseMoveEvent(event); QPoint pos = event-\u0026gt;pos(); int xPtInterval = pos.x() - m_oldPos.x(); int yPtInterval = pos.y() - m_oldPos.y(); m_xPtInterval += xPtInterval; m_yPtInterval += yPtInterval; m_oldPos = pos; update(); } void PhotoLabel::mouseReleaseEvent(QMouseEvent*/*event*/) { m_pressed = false; this-\u0026gt;setCursor(Qt::ArrowCursor); //设置鼠标样式 } void PhotoLabel::dragEnterEvent(QDragEnterEvent* event) { if (event-\u0026gt;mimeData()-\u0026gt;hasUrls()) { event-\u0026gt;acceptProposedAction(); } else { event-\u0026gt;ignore(); } } void PhotoLabel::dragMoveEvent(QDragMoveEvent* event) { } void PhotoLabel::dropEvent(QDropEvent* event) { QList\u0026lt;QUrl\u0026gt; urls = event-\u0026gt;mimeData()-\u0026gt;urls(); if (urls.empty()) return; QString fileName = urls.first().toLocalFile(); QFileInfo info(fileName); if (!info.isFile()) return; this-\u0026gt;openFile(fileName); } void PhotoLabel::contextMenuEvent(QContextMenuEvent* event) { QPoint pos = event-\u0026gt;pos(); pos = this-\u0026gt;mapToGlobal(pos); m_menu-\u0026gt;exec(pos); } void PhotoLabel::onSelectImage() { QString path = QFileDialog::getOpenFileName(this, tr(\u0026#34;Open a image file\u0026#34;), \u0026#34;./\u0026#34;, tr(\u0026#34;Images (*.bmp *.png *.jpg *.jpeg *.gif *.tiff)\\nAll files (*.*)\u0026#34;)); if (path.isEmpty()) return; openFile(path); } void PhotoLabel::onPasteImage() { QClipboard* clipboard = QApplication::clipboard(); QImage img = clipboard-\u0026gt;image(); if (img.isNull()) { return; } this-\u0026gt;setImage(img); } void PhotoLabel::onZoomInImage() { m_zoomValue += 0.05; update(); } void PhotoLabel::onZoomOutImage() { m_zoomValue -= 0.05; if (m_zoomValue \u0026lt;= 0) { m_zoomValue = 0.05; return; } update(); } void PhotoLabel::onPresetImage() { m_zoomValue = 1.0; m_xPtInterval = 0; m_yPtInterval = 0; update(); } The base64 and image conversion code is placed in the slot function of the window control.\nwidget.cpp：\nvoid Widget::updateCode() //图片数据转换成base64编码 { QImage image = ui-\u0026gt;viewer-\u0026gt;getImage(); if (image.isNull()) { QMessageBox::warning(this, tr(\u0026#34;Error\u0026#34;), tr(\u0026#34;Please load an image file!\u0026#34;)); return; } QByteArray ba; QBuffer buf(\u0026amp;ba); image.save(\u0026amp;buf, format.toStdString().c_str()); QByteArray md5 = QCryptographicHash::hash(ba, QCryptographicHash::Md5); QString strMd5 = md5.toHex(); QString head_md = QString::fromUtf8(\u0026#34;![%1](%2)\u0026#34;); QString prefix = QString::fromUtf8(\u0026#34;data:image/%1;base64,\u0026#34;).arg(format); QString code = QString::fromUtf8(ba.toBase64()); if (ui-\u0026gt;checkBox-\u0026gt;isChecked()) { ui-\u0026gt;textEdit-\u0026gt;setText(head_md.arg(strMd5).arg(prefix + code)); } else { ui-\u0026gt;textEdit-\u0026gt;setText(prefix + code); } buf.close(); int num = ui-\u0026gt;textEdit-\u0026gt;toPlainText().length(); ui-\u0026gt;label-\u0026gt;setText(tr(\u0026#34;Length : \u0026#34;) + QString::number(num)); } void Widget::updateImage() // base64编码转换成图片数据 { QString p_b = ui-\u0026gt;textEdit-\u0026gt;toPlainText(); if (p_b.isEmpty()) { return; } if (p_b.contains(QRegularExpression(\u0026#34;data:image[/a-z]*;base64,\u0026#34;))) { // 清除base64编码的html标签 p_b = p_b.remove(QRegularExpression(\u0026#34;data:image[/a-z]*;base64,\u0026#34;)); } QImage image; if (!image.loadFromData(QByteArray::fromBase64(p_b.toLocal8Bit()))) { QMessageBox::warning(this, tr(\u0026#34;Error\u0026#34;), tr(\u0026#34;Fail to decrypt codes!\u0026#34;)); return; } ui-\u0026gt;viewer-\u0026gt;setImage(image); } 2.2 Python Implementation Similarly, the software also provides a Python-based implementation, which also realizes image display by overloading QLabel.\nphotolabel.py：\n# This Python file uses the following encoding: utf-8 import sys from PySide6.QtCore import (Qt, qDebug, QFileInfo, QMimeData, QRect, QPoint) from PySide6.QtGui import (QAction, QImage, QAction, QDragEnterEvent, QDragMoveEvent, QDropEvent, QContextMenuEvent, QPaintEvent, QMouseEvent, QWheelEvent, QPainter, QClipboard, QCursor) from PySide6.QtWidgets import (QApplication, QLabel, QMenu, QMessageBox, QFileDialog) class PhotoLabel(QLabel): def __init__(self, parent): super().__init__(parent) self.m_image = QImage() # 显示的图片 self.m_zoomValue = 1.0 # 鼠标缩放值 self.m_xPtInterval = 0 # 平移X轴的值 self.m_yPtInterval = 0 # 平移Y轴的值 self.m_oldPos = QPoint() # 旧的鼠标位置 self.m_pressed = False # 鼠标是否被摁压 self.m_localFileName = None # 文件名称 self.m_menu = None self.initial_widget() def initial_widget(self): self.m_menu = QMenu(self) load_image = QAction(u\u0026#34;\u0026amp;Open new...\u0026#34;, self) load_image.triggered.connect(self.on_select_image) self.m_menu.addAction(load_image) paste_image = QAction(u\u0026#34;\u0026amp;Paste image\u0026#34;, self) paste_image.triggered.connect(self.on_paste_image) self.m_menu.addAction(paste_image) self.m_menu.addSeparator() zoom_in_action = QAction(u\u0026#34;Zoom in \u0026amp;+\u0026#34;, self) zoom_in_action.triggered.connect(self.on_zoom_in_image) self.m_menu.addAction(zoom_in_action) zoom_out_action = QAction(u\u0026#34;Zoom out \u0026amp;-\u0026#34;, self) zoom_out_action.triggered.connect(self.on_zoom_out_image) self.m_menu.addAction(zoom_out_action) self.m_menu.addSeparator() preset_action = QAction(u\u0026#34;\u0026amp;Reset\u0026#34;, self) preset_action.triggered.connect(self.on_preset_image) self.m_menu.addAction(preset_action) self.m_menu.addSeparator() # clear_action = QAction(u\u0026#34;\u0026amp;Reset\u0026#34;, self) # clear_action.triggered.connect(self.clear_show) # self.m_menu.addAction(clear_action) def open_file(self, path: str): # 打开图片文件 if path is None: return if self.m_image.load(path) is False: QMessageBox.warning(self, \u0026#34;Error\u0026#34;, \u0026#34;Cannot load file!\u0026#34;) return self.m_zoomValue = 1.0 self.m_xPtInterval = 0 self.m_yPtInterval = 0 self.m_localFileName = path self.update() def clear_show(self): # 清空显示 self.m_localFileName = None self.m_image = QImage() self.clear() def set_image(self, image: QImage): # 设置图片 if image is None: return self.m_zoomValue = 1.0 self.m_xPtInterval = 0 self.m_yPtInterval = 0 self.m_localFileName = None self.m_image = image.copy(0, 0, image.width(), image.height()) self.update() def open_action(self): # 调用打开文件对话框 self.on_select_image() def paste_action(self): # 粘贴来自剪贴板的图片 self.on_paste_image() def get_image(self) -\u0026gt; QImage: # 调用存储的图片数据 return self.m_image def contextMenuEvent(self, event: QContextMenuEvent): # 右键菜单 pos = event.pos() pos = self.mapToGlobal(pos) self.m_menu.exec(pos) def paintEvent(self, event: QPaintEvent): # QPaint画图 if self.m_image.isNull(): # super().paintEvent(event) return painter = QPainter(self) # 根据窗口计算应该显示的图片的大小 width = min(self.m_image.width(), self.width()) height = int(width * 1.0 / (self.m_image.width() * 1.0 / self.m_image.height())) height = min(height, self.height()) width = int(height * 1.0 * (self.m_image.width() * 1.0 / self.m_image.height())) # 平移 painter.translate(self.width() / 2 + self.m_xPtInterval, self.height() / 2 + self.m_yPtInterval) # 缩放 painter.scale(self.m_zoomValue, self.m_zoomValue) # 绘制图像 pic_rect = QRect(int(-width / 2), int(-height / 2), width, height) painter.drawImage(pic_rect, self.m_image) # super().paintEvent(event) def wheelEvent(self, event: QWheelEvent): # 鼠标滚轮滚动 value = int(event.angleDelta().y() / 15) if value \u0026lt; 0: # 放大 self.on_zoom_in_image() else: # 缩小 self.on_zoom_out_image() self.update() def mousePressEvent(self, event: QMouseEvent): # 鼠标摁下 self.m_oldPos = event.pos() self.m_pressed = True self.setCursor(Qt.ClosedHandCursor) # 设置鼠标样式 def mouseMoveEvent(self, event: QMouseEvent): # 鼠标松开 if self.m_pressed is False: # super().mouseMoveEvent(event) return pos = event.pos() xp = pos.x() - self.m_oldPos.x() yp = pos.y() - self.m_oldPos.y() self.m_xPtInterval += xp self.m_yPtInterval += yp self.m_oldPos = pos self.update() def mouseReleaseEvent(self, event: QMouseEvent): # 鼠标发射事件 self.m_pressed = False self.setCursor(Qt.ArrowCursor) # 设置鼠标样式 # 拖放文件 def dragEnterEvent(self, event: QDragEnterEvent): if event.mimeData().hasUrls(): event.acceptProposedAction() else: event.ignore() def dragMoveEvent(self, event: QDragMoveEvent): pass def dropEvent(self, event: QDropEvent): urls = event.mimeData().urls() if urls is None: return file_name = urls[0].toLocalFile() info = QFileInfo(file_name) if info.isFile() is False: return self.open_file(file_name) def on_select_image(self): # 选择打开图片 path, _ = QFileDialog.getOpenFileName(self, \u0026#34;Open a image file\u0026#34;, \u0026#34;./\u0026#34;, \u0026#34;Images (*.bmp *.png *.jpg *.jpeg *.gif *.tiff)\\nAll files (*.*)\u0026#34;) if path is None: return info = QFileInfo(path) if info.isFile() is False: return self.open_file(path) def on_paste_image(self): # 选择粘贴图片 clipboard = QApplication.clipboard() img = clipboard.image() if img.isNull(): return self.set_image(img) def on_zoom_in_image(self): # 图片放大 self.m_zoomValue += 0.05 self.update() def on_zoom_out_image(self): # 图片缩小 self.m_zoomValue -= 0.05 if self.m_zoomValue \u0026lt;= 0: self.m_zoomValue = 0.05 return self.update() def on_preset_image(self): # 图片还原 self.m_zoomValue = 1.0 self.m_xPtInterval = 0 self.m_yPtInterval = 0 self.update() if __name__ == \u0026#34;__main__\u0026#34;: pass It is worth noting that in the Python implementation code, the conversion of binary data to base64 is completed through Python\u0026rsquo;s str function, so the output string will contain the label b'...', which needs to be removed through string slicing.widget.py：\ndef update_code(self): #图片→base64 image = self.ui.viewLabel.get_image() if image.isNull(): QMessageBox.warning(self, \u0026#34;Error\u0026#34;, \u0026#34;Please load an image file!\u0026#34;) return ba = QByteArray() buf = QBuffer(ba) image.save(buf, self.m_format) md5 = QCryptographicHash.hash(ba, QCryptographicHash.Md5) strMd5 = str(md5.toHex())[2:-1] prefix = \u0026#34;data:image/{};base64,\u0026#34;.format(self.m_format) code = str(ba.toBase64())[2:-1] if self.ui.checkBox.isChecked(): self.ui.textEdit.setText(\u0026#34;![{0}]({1})\u0026#34;.format(strMd5, prefix + code)) else: self.ui.textEdit.setText(prefix + code) buf.close() num = len(self.ui.textEdit.toPlainText()) self.ui.lengthLabel.setText(\u0026#34;Length : \u0026#34; + str(num)) def update_image(self): #base64→图片 p_b = self.ui.textEdit.toPlainText() if len(p_b) == 0: return if re.match(\u0026#34;data:image[/a-z]*;base64,\u0026#34;, p_b): p_b = re.sub(\u0026#34;data:image[/a-z]*;base64,\u0026#34;, \u0026#34;\u0026#34;, p_b, count=1) image = QImage() if image.loadFromData(QByteArray.fromBase64(p_b.encode())) is False: QMessageBox.warning(self, \u0026#34;Error\u0026#34;, \u0026#34;Fail to decrypt codes!\u0026#34;) return self.ui.viewLabel.set_image(image) The UI is all implemented through QtDesigner. Whether it is implemented in C++ or Python, the software interface effect is consistent.\n15、Qt显示图片并支持缩放、移动等操作 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-04-02-image2base64/","tags":[{"LinkTitle":"C++","RelPermalink":"/tags/c++/"},{"LinkTitle":"Python","RelPermalink":"/tags/python/"},{"LinkTitle":"Qt","RelPermalink":"/tags/qt/"}],"title":"A Qt6-based image base64 conversion tool"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/categories/cfd/"}],"content":"1. Compile cfmesh The early compiled and installed OpenFOAM version of com, with the version number v2412, and there is no source code with cfmesh. According to the official documentation, you need to manually download the source code file of cfmesh:\ncd $WM_PROJECT_DIR git submodule update --init --recursive plugins/cfmesh The following error occurs:\nfatal: fatal: not a git repository (or any of the parent directories): .git Okay, let\u0026rsquo;s try another approach. Download the source code directly through git to the specified folder, folder and URL path to view it .gitmodules document:\ncd $WM_PROJECT_DIR git clone https://develop.openfoam.com/Community/integration-cfmesh.git plugins/cfmesh Enter the cfmesh folder and start compiling:\ncd plugins/cfmesh ./Allwmake -jN # Replace N with the number of CPU cores Run pMesh to verify that the installation is successful:\npMesh -help 2. Compile the ccm tool OpenFOAM\u0026rsquo;s ccm tool includes ccmToFoam and foamToCcm, the former is used to convert the ccm format grid output by STAR-CCM+ into the grid of OpenFOAM, and the latter is the opposite. You need to compile and install the third-party library libccmio first:\ncd $WM_THIRD_PARTY_DIR # Download libccmio wget http://portal.nersc.gov/project/visit/third_party/libccmio-2.6.1.tar.gz # Alternative links # wget https://sourceforge.net/projects/foam-extend/files/ThirdParty/libccmio-2.6.1.tar.gz tar xvzf libccmio-2.6.1.tar.gz # Decompress the downloaded compressed file ./makeCCMIO # Run the libccmio compiler Next compile ccmToFoam:\n# Compile libccm first cd $WM_PROJECT_DIR/src/conversion/ccm ./Allwmake # Then compile ccmToFoam and foamToCcm cd $WM_PROJECT_DIR/applications/utilities/mesh/conversion/ccm ./Allwmake Run ccmToFoam to verify that the installation is successful:\nccmToFoam -help ","permalink":"https://andrewmoa.site/post/2025-03-27-openfoam-compile-install-cfmesh-and-ccmtool/","tags":[{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Openfoam","RelPermalink":"/tags/openfoam/"}],"title":"OpenFOAM compiles and installs cfmesh and ccm tools"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"SU2 is an open source CFD solver developed by the School of Aeronautics and Astronautics at Stanford University. It is based on C++ and Python and is similar to OpenFOAM, but does not support polyhedral meshes. Compared with OpenFOAM, SU2 has more advantages in solving high-speed compressible flows.\nDownload SU2 source code:\nmkdir $HOME/su2code \u0026amp;\u0026amp; cd $HOME/su2code # Only clone the latest commit version to speed up downloading git clone https://github.com/su2code/SU2.git --depth=1 Define environment variables and create a new configuration file su2.env:\ntouch $HOME/su2code/su2.env chmod +x $HOME/su2code/su2.env vim $HOME/su2code/su2.env Add the following content to the su2.env file, save and exit:\n#!/bin/sh # Define SU2 environment variables export SU2_RUN=$HOME/su2code/bin # su2 installation path after compilation export SU2_HOME=$HOME/su2code/SU2 # su2 source code folder path export PATH=$PATH:$SU2_RUN export PYTHONPATH=$SU2_RUN:$PYTHONPATH The compiler configuration file meson_options.txt is located in the SU2 source code folder. Adjust the compilation options according to your needs:\nvim $HOME/su2code/SU2/meson_options.txt Here, enable mpi and blas support and modify the values ​​of the following two lines:\noption(\u0026#39;with-mpi\u0026#39;, type : \u0026#39;feature\u0026#39;, value : \u0026#39;enabled\u0026#39;, description: \u0026#39;enable MPI support\u0026#39;) option(\u0026#39;enable-openblas\u0026#39;, type : \u0026#39;boolean\u0026#39;, value : true, description: \u0026#39;enable BLAS and LAPACK support via OpenBLAS\u0026#39;) If it is an Intel machine, it is recommended to enable mkl support.\nThe default supported blas library is openblas. You need to download the openblas library first:\nsudo apt install libopenblas-dev -y Enter the downloaded source code directory and run the compiler\n# Load environment variables source $HOME/su2code/su2.env # Enter the source code folder cd $SU2_HOME # Configure the compiler and generate ninja build files # The configuration process will automatically download external dependencies from git # It takes a lot of time... ./meson.py build --prefix=$SU2_RUN/.. # Start compiling and installing ./ninja -C build install Verify that the installation was successful:\nSU2_CFD --help If the installation is successful, the software version number and help information will be output.\nWrite the slurm calculation script for SU2:\n#!/bin/bash #SBATCH --job-name=su2_test #SBATCH --partition=debug #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 1 #SBATCH --ntasks-per-node=32 cd $SLURM_SUBMIT_DIR source ${HOME}/su2code/su2.env # The absolute path should be filled in export CFG_FILE=`find . -name \u0026#34;*.cfg\u0026#34;` export MACHINEFILE=$SLURM_JOBID.nodes scontrol show hostnames $SLURM_JOB_NODELIST \u0026gt; $MACHINEFILE mpiexec -np $SLURM_NPROCS --machinefile $MACHINEFILE SU2_CFD $CFG_FILE Put the script file, SU2\u0026rsquo;s cfg file and mesh into the same folder, and submit the calculation task through the sbatch command.\n","permalink":"https://andrewmoa.site/post/2025-03-27-ubuntu-compile-su2/","tags":[{"LinkTitle":"Slurm","RelPermalink":"/tags/slurm/"},{"LinkTitle":"Su2","RelPermalink":"/tags/su2/"},{"LinkTitle":"Ubuntu","RelPermalink":"/tags/ubuntu/"}],"title":"Ubuntu compile and install SU2"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/categories/cfd/"}],"content":"1. Fluent First, write the Fluent jou script:\n/file/read-case \u0026#34;small_fan.cas.h5\u0026#34; /solve/initialize/hyb-initialization /solve/iterate 100 /file/write-case \u0026#34;small_fan_results.cas.h5\u0026#34; ok /file/write-data \u0026#34;small_fan_results.dat.h5\u0026#34; ok /exit ok This jou file is very simple. It tells Fluent which file to read, how to initialize, how many steps to iterate, and how to save until the final exit. If the calculation is more complicated, such as involving UDF loading or special condition initialization settings, you need to add corresponding command lines. If you are not familiar with how to write TUI commands, you can record scripts through the command line window under the Fluent graphical interface.\nWriting Slurm scripts:\n#!/bin/bash #SBATCH --job-name=fluent_test\t# Job Name #SBATCH --partition=debug #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 1\t# Number of computing nodes #SBATCH --ntasks-per-node=32\t# Number of computing processes per node cd $SLURM_SUBMIT_DIR source ${HOME}/opt/ansys2025r1.env\t# Load the license setting environment variable. An absolute path should be used here. export FLUENT=/ansys_inc/v251/fluent/bin/fluent\t# Fluent executable file path export MPI_TYPE=intel # intel or openmpi export JOU_FILE=`find . -name \u0026#34;*.jou\u0026#34;` export MACHINEFILE=$SLURM_JOBID.node scontrol show hostnames $SLURM_JOB_NODELIST \u0026gt; $MACHINEFILE #Note that Fluent has four calculation modes according to 2D, 3D, single and double precision: 2d, 3d, 2ddp, 3ddp. Choose the corresponding calculation mode according to your needs. $FLUENT 3ddp -g -t$SLURM_NPROCS -cnf=$MACHINEFILE -mpi=$MPI_TYPE -ssh -i $JOU_FILE Save the above script, put the cas file and jou file to be submitted into the folder where the script is located, and submit the script through the sbatch command. After the calculation is completed, download the output result file to the local machine for processing.\n2. CFX Compared with Fluent, CFX calculation scripts are much simpler:\n#!/bin/bash #SBATCH --job-name=cfx_test\t# Job Name #SBATCH --partition=debug #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 1\t# Number of computing nodes #SBATCH --ntasks-per-node=32\t# Number of computing processes per node cd $SLURM_SUBMIT_DIR source ${HOME}/opt/ansys2025r1.env\t# Load the license setting environment variable. An absolute path should be used here. export CFX=/ansys_inc/v251/CFX/bin/cfx5solve\t# CFX executable file path export DEF_FILE=`find . -name \u0026#34;*.def\u0026#34;` hostnames=`scontrol show hostnames $SLURM_JOB_NODELIST` hostnames=`echo $hostnames | sed -e \u0026#39;s/ /,/g\u0026#39;` $CFX -def $DEF_FILE -double -part $SLURM_NPROCS -par-dist $hostnames -start-method \u0026#39;Intel MPI Distributed Parallel\u0026#39; -name $SLURM_JOB_NAME Put the script file and def file into the same folder and submit it.\nReferences:\nJournal 脚本编写指南 Fluent 极客 —— 强大的自定义功能（UDF，jou，参数化，expression，ACT ） ANSYS - CFX, Fluent, Mechanical CFX本地多核批处理文件编写方法 ","permalink":"https://andrewmoa.site/post/2025-03-26-slurm-submit-fluent-and-cfx-script/","tags":[{"LinkTitle":"Cfx","RelPermalink":"/tags/cfx/"},{"LinkTitle":"Fluent","RelPermalink":"/tags/fluent/"},{"LinkTitle":"Slurm","RelPermalink":"/tags/slurm/"}],"title":"Slurm submits Fluent and CFX calculation scripts"},{"categories":[{"LinkTitle":"Tech","RelPermalink":"/categories/tech/"}],"content":"Tabby is a very good-looking terminal tool. I first used it to replace the local terminal, and then I found more and more advantages. First of all, it has built-in support for SSH connection and SFTP file transfer. The setting operation is simple, avoiding the tedious settings in the Windows terminal. Secondly, it can be used to replace the original Mintty interface of MSYS2 and Cygwin to achieve seamless switching between different terminals. Regarding how to call MSYS2 in the Tabby terminal, the following is a record of the configuration method:\nFirst clone a CMD configuration in Tabby settings: Fill in the name of the MSYS2 toolchain you want to call: Note that you cannot directly assign an ico file to the icon here, as Tabby cannot recognize it. You must convert the ico icon to svg format. There are many online ico2svg conversion resources, and you can find a lot of them by searching online:\nACCONVERT - ICO to SVG CDKM - ICO to SVG FreeConvert - ICO to SVG After the conversion is complete, download the svg file, open it with a text tool, copy and paste the svg source code into the icon bar above, and Tabby will be able to display the icon normally.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE svg PUBLIC \u0026#34;-//W3C//DTD SVG 1.1//EN\u0026#34; \u0026#34;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\u0026#34;\u0026gt; \u0026lt;svg version=\u0026#34;1.1\u0026#34; id=\u0026#34;Layer_1\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; xmlns:xlink=\u0026#34;http://www.w3.org/1999/xlink\u0026#34; x=\u0026#34;0px\u0026#34; y=\u0026#34;0px\u0026#34; width=\u0026#34;16px\u0026#34; height=\u0026#34;16px\u0026#34; viewBox=\u0026#34;0 0 16 16\u0026#34; enable-background=\u0026#34;new 0 0 16 16\u0026#34; xml:space=\u0026#34;preserve\u0026#34;\u0026gt; \u0026lt;image id=\u0026#34;image0\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; x=\u0026#34;0\u0026#34; y=\u0026#34;0\u0026#34; xlink:href=\u0026#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAAAIGNIUk0AAHomAACAhAAA+gAAAIDo AAB1MAAA6mAAADqYAAAXcJy6UTwAAADPUExURb5kPsBpRdWbgtSZgL9lQMBoQ8+McMyEZsJuS/36 +P///9uplL5lP+3UyevOws2HafDc1NCPdOjHuejIur9mQO3TyeXCs8NxTv37+vv18sh7W8h8XP/+ /uS+rt2umtOXfuXBsvfs6Pjv6+bDtMFqRvny7vHf1/36+dyrlvXn4e/a0dadhMBpRP79/f7+/fz4 9sJuSvHe1vnw7PPi2+O9rNqmkN2tmeC2pP78+8Z3Vfft6L9mQfz59+fFt8JtSdKVe79nQsuDZcV0 UtGSd8NvTFLhcR8AAAABYktHRApo0PRWAAAAB3RJTUUH6QMaAysVnHCKGgAAAJpJREFUGNONj1cS ggAUA2NBIdi7WAErYEFFBRXr/c8k7QC8n8zuRyYPSHWZbC4fhFAoJkKUKAOlMiuJqJI1oE42EtEk pRbaZCfmbq9PKoMhOYrFmDI5maoa9Yhn84VC6svVmkYkZJrWhtvd3qYkBmwdNAFH8hR2O4FweAYu vLowSc/CzbMtwL0/gqUGVR/+8xUWvT/h9u8vxaN/UKUNLao7WagAAAAldEVYdGRhdGU6Y3JlYXRl ADIwMjUtMDMtMjZUMDM6NDM6MjArMDA6MDDH702KAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDI1LTAz LTI2VDAzOjQzOjIwKzAwOjAwtrL1NgAAACh0RVh0ZGF0ZTp0aW1lc3RhbXAAMjAyNS0wMy0yNlQw Mzo0MzoyMSswMDowMEfQ310AAAAASUVORK5CYII=\u0026#34; /\u0026gt; \u0026lt;/svg\u0026gt; isable dynamic tab title: Set according to your needs, it is recommended to select it. Group: You need to create a new profile group first, and then select it here.\nThe execution commands, parameters, and environment variables are shown in the figure. Fill in the HOME directory of MSYS2 in the working directory and HOME directory, and fill in the tool chain to be called in MSYSTEM.\nAfter saving in step 3 above, you can quickly call the Clang64 terminal of MSYS2 through the icon in the Tabby title bar. Other tool chains are configured in the same way. ","permalink":"https://andrewmoa.site/post/2025-03-26-tabby-setup-msys2/","tags":[],"title":"Tabby configuration MSYS2"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"1. Preparation 161GB of disk space is required for the full installation. Please ensure that the remaining available disk space meets the requirements. You can choose the installation content according to your needs. At least 8GB of memory is required during the installation process, and 16~32GB of memory is recommended.\nDependent tools during the installation process:\nsudo apt install libnsl2 libpcre3 lsb-* ldap-utils libunistring5 xfonts-100dpi xfonts-75dpi For installation of other runtime dependent tools, please refer to the official documentation.\nThe Ansys2025R1 installation file contains 9 .iso installation files. Use the following command to mount the .iso file to the specified path:\nmkdir ${HOME}/ISO/1 sudo mount ${HOME}/Share/Ansys/ANSYS2025R1_LINX64_DISK1.iso ${HOME}/ISO/1 -o loop In the same way, mount the remaining installation packages to other folders.\n2. Install the license server 2.1 Install Ansys License Manager Since Ansys installation requires a graphical interface, you need to first access the Ubuntu desktop system through a remote connection.\nEnter the mount point of the first installation package and run the installer:\ncd ${HOME}/ISO/1 sudo ./INSTALL # Since it needs to be run as a service, it is recommended to run the installer with administrator user privileges License installation path selection: /opt/ansys_inc: After the installation is complete, exit the installer.\nVerify that the Ansys License Server daemon is running:\nps -eaf | grep ansyslmd If the ansyslmd process is running, kill it:\nkill -s 9 [PID] # [PID] Fill in the PID of the ansyslmd process 2.2 Configure the license server We need to unzip the files in the /shared_files folder in the ansys.2025r1.linx64-ssq.tar.gz compressed package to the installation directory and overwrite:\ntar xvzf ansys.2025r1.linx64-ssq.tar.gz sudo cp -r shared_files /opt/ansys_inc/ Next, configure the license service. The following command runs the license configuration program:\nsudo /opt/ansys_inc/shared_files/licensing/init_ansyslm_tomcat start Then use the browser to open the address: http://localhost:1084/ANSYSLMCenter.html, locate the Get System Hostid information column, find HOSTID, copy it and use it later: Open the license_2024.12.15.txt file, replace XXXXXXXXXXXX with the HOSTID copied above in the line SERVER localhost XXXXXXXXXXXX 1055 at the beginning, and save the file. Locate the Add a License File column and load the license_2024.12.15.txt file that you just edited. There is a problem here, prompting that lmgrd cannot be found and the server daemon cannot be started. Run the following command:\nsudo cp /opt/ansys_inc/shared_files/licensing/linx64/update/lmgrd /opt/ansys_inc/shared_files/licensing/linx64/lmgrd Just load the file again: Locate the View Status/Start/Stop License Manager column and confirm the server running status: Exit the browser and close the license configuration program, and confirm whether the daemon is running:\nsudo /opt/ansys_inc/shared_files/licensing/init_ansyslm_tomcat stop ps -eaf | grep ansyslmd If the license server and the main program are installed on different machines, you need to open port 1055 of the license server in the firewall:\nsudo ufw allow from any to any port 1055 proto tcp 2.3 Add custom service After completing the above settings, when the server is restarted, the system will not automatically start the daemon process. You need to start ansyslmd with the following command:\nsudo /opt/ansys_inc/shared_files/licensing/start_ansyslmd In order to prevent the server from being unable to calculate due to license issues after power failure and restart, the following custom service is added through systemd to realize the automatic start of the daemon process at startup. The following command creates a new ansyslmd service:\nsudo touch /usr/lib/systemd/system/ansyslmd.service sudo vim /usr/lib/systemd/system/ansyslmd.service sudo chmod 754 /usr/lib/systemd/system/ansyslmd.service Add the following content and save and exit:\n[Unit] Description=Ansys License Deamon After=ansyslmd.service [Service] Type=forking User=root Group=root ExecStart=/opt/ansys_inc/shared_files/licensing/start_ansyslmd ExecReload= ExecStop=/opt/ansys_inc/shared_files/licensing/stop_ansyslmd [Install] WantedBy=multi-user.target Start the service with the following command:\nsudo systemctl enable ansyslmd sudo systemctl start ansyslmd Check the service status:\nsudo systemctl status ansyslmd The service is started successfully. After the installation is complete, you can restart it to verify.\n3. Install the main program You also need to enter the Ubuntu desktop system to install the main program.\nEnter the mount point of the first installation package and run the installation program:\ncd ${HOME}/ISO/1 sudo ./INSTALL # Install with root privileges Solve the same choice to install in: /opt/ansys_inc Choose what to install according to your needs: Configure CAD geometry interface. CAD software is not installed on this machine. Select Skip: Depending on the installation content selected earlier, a bunch of configuration file paths will be asked later. Set them according to your needs. If there is no such a thing, just click Next. Choose whether to create a desktop shortcut. Select it. It may be useful. Confirm the installation content and click Next to start copying files: You can check the installation progress. During the installation, you will be asked to load other .iso file mount paths. The installation is complete, and there are some errors. Ignore it and exit. Next, unzip the files in the v251 folder in the ansys.2025r1.linx64-ssq.tar.gz compressed package to the installation directory and overwrite:\ntar xvzf ansys.2025r1.linx64-ssq.tar.gz sudo cp -r v251 /opt/ansys_inc/ After the main program is installed, unmount the mount point of the .iso file:\nsudo umount ${HOME}/ISO/1 Unmount the remaining mount points in the same way.\n4. Configure the main program license After installing the main program, you need to configure the license for the main program. There are two ways.\n4.1 Method 1 → Configure environment variables Create and edit the environment variable file in ${HOME}/opt/:\ntouch ${HOME}/opt/ansys2025r1.env chmod +x ${HOME}/opt/ansys2025r1.env vim ${HOME}/opt/ansys2025r1.env Add the following content, save and exit:\nexport ANSYSLMD_LICENSE_FILE=1055@localhost You can load the environment variables with the following command later.\nsource ${HOME}/opt/ansys2025r1.env You can add the above command line to the ${HOME}/.bashrc file, so that the environment variables will be automatically loaded every time you log in.\n4.2 Method 2→Ansys Licensing Settings Run the following command to start the Licensing Settings program:\n/ansys_inc/v251/licensingclient/linx64/LicensingSettings Follow the steps to set the following:\nChange the configuration file level from Installion to User Set the status to Enable Fill in 1055 for the port number Fill in localhost for the Server1 address Click the Test button to test Click Save to save the configuration Save and exit to complete the license configuration.\n5. Start the main program After configuring the license, start Workbench with the following command:\n/ansys_inc/v251/Framework/bin/Linux64/runwb2 Similarly, the entry points of some key main programs are as follows:\nApplication How to Launch ACP /ansys_inc/v251/ACP/ACP.sh CFD-Post /ansys_inc/v251/cfdpost CFX /ansys_inc/v251/CFX/bin/cfx5(Note) FLUENT /ansys_inc/v251/fluent/bin/fluent ICEM CFD /ansys_inc/v251/icemcfd/icemcfd ICEPAK /ansys_inc/v251/icepak/icepak Motion /ansys_inc/v251/Motion/solver/rundfs.sh Polyflow Classic /ansys_inc/v251/polyflow/bin/polyflow TurboGrid /ansys_inc/v251/TurboGrid/bin/cfxtg Sherlock /ansys_inc/v251/sherlock/runSherlock Workbench /ansys_inc/v251/Framework/bin/Linux64/runwb2 Electronics /ansys_inc/v251/AnsysEM/ansysedt (Note): cfx5 can be replaced by cfx5launch, cfx5pre, cfx5solve or cfx5post. They correspond to different functional modules.\nAt this point, Ansys is installed.\n","permalink":"https://andrewmoa.site/post/2025-03-25-ubuntu24.04-install-ansys2025r1/","tags":[{"LinkTitle":"Ansys","RelPermalink":"/tags/ansys/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"},{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Ubuntu","RelPermalink":"/tags/ubuntu/"}],"title":"Install Ansys2025R1 on Ubuntu24.04"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/categories/cfd/"}],"content":"When I used STAR-CCM+ to do calculations on a Windows workstation, sometimes I had to submit more than ten or twenty computing tasks in one night. Of course, it was impossible to run more than ten tasks at the same time, and it was impossible to watch it run and submit them manually one by one. A few years ago, I wrote this simple queue calculation template using PowerShell, and I share it with you here.\n$title = \u0026#34;STAR-CCM+ 19.06.009-r8\u0026#34;\t# Window title, fill it in as you like $host.ui.RawUI.WindowTitle = $title $STARCCM_PATH = \u0026#34;D:\\XXX\\Siemens\\19.06.009-R8\\STAR-CCM+19.06.009-R8\\star\\lib\\win64\\clang17.0vc14.2-r8\\lib\u0026#34;\t# Fill in the absolute installation path of STAR-CCM+ on this machine $env:path += \u0026#34;;$STARCCM_PATH\u0026#34; $run_dir = $pwd $thread_number = 32\t# Fill in the number of CPU cores of this machine $Array = Get-ChildItem -Path $run_dir -Name \u0026#34;*.sim\u0026#34; $n = 0 foreach($item in $Array) { $n += 1 $sub_dir = $n.ToString() + \u0026#34;_\u0026#34; + $item.Substring(0,$item.Length-4) mkdir $sub_dir mv $item $sub_dir cd $sub_dir $host.ui.RawUI.WindowTitle = $title + \u0026#34; - \u0026#34; + $item + \u0026#34; - \u0026#34; + $n + \u0026#34;/\u0026#34; + $Array.Count $log = $item + \u0026#34;.log\u0026#34; starccm+ $item -batch run -np $thread_number -mpi ms | tee $log cd $run_dir } Save the above command line in text format as a .ps1 script file, put it in the same folder as the .sim file to be submitted for calculation, and then run this script through the terminal. It will automatically count the number of queued tasks, transfer the calculated .sim file to a new subfolder, and generate a .log log file. You can also monitor the operation status in the output window. Close the terminal after the calculation is completed.\nThe only drawback is that it does not support macro files, nor does it support temporary addition or insertion of calculation examples.\nYou can make some targeted adjustments according to your own situation.\nIf you encounter garbled characters in the output window and log file, it is mostly caused by your PowerShell not supporting UTF-8. Refer to the following method 1 and enter in the PowerShell window:\n# Configuration files are usually located at: C:\\Users\\用户名\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 # If not, create a new one notepad $PROFILE # Editing the Configuration File Add the following content to the configuration file, save and exit:\n$OutputEncoding = [console]::InputEncoding = [console]::OutputEncoding = [Text.UTF8Encoding]::UTF8 WindowsPowerShell中文乱码问题解决 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-03-25-posershell-starccm-script/","tags":[{"LinkTitle":"Powershell","RelPermalink":"/tags/powershell/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/tags/star-ccm+/"}],"title":"PowerShell编写STAR-CCM+自动排队计算脚本"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"Since Ubuntu runs in a virtual machine, when you mount a Windows shared folder through Ubuntu, some CAE software will report a calculation error when running it in the mount point. Consider sharing the Ubuntu folder with Windows, so you need to build a Samba server on the Ubuntu system.\nFirst, install the samba package on Ubuntu:\nsudo apt install samba -y Create a shared folder:\nmkdir ${HOME}/LinuxShare Edit the Samba configuration file /etc/samba/smb.conf:\nsudo vim /etc/samba/smb.conf Add the following content to the end of the file, save and exit:\n[Ubuntu_Share] # The name of the shared folder displayed on the client comment = Samba # Comment, displayed to the user path = /home/***/LinuxShare # The local path of the shared folder, fill in the absolute path public = yes # Whether anonymous users are allowed to access writable = yes # Whether users are allowed to edit available = yes # Whether it is available browseable = yes # Whether it can be browsed on the network valid users = user # Fill in the Ubuntu login user name Set a password for the Samba user:\nsudo smbpasswd -a user Start the Samba service daemon:\nsudo systemctl enable smbd sudo systemctl start smbd Query the Samba service status:\nsudo systemctl status smbd Service status: Active: active (running), running normally.\nUpdate the Samba configuration file /etc/samba/smb.conf and refresh it with the following command:\nsudo service smbd restart Add firewall rules:\nsudo ufw allow samba After the configuration is complete, you can access the shared folder on the Windows side. For more information, please refer to other documents.\n","permalink":"https://andrewmoa.site/post/2025-03-24-ubuntu24.04-samba-serve/","tags":[{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Ubuntu","RelPermalink":"/tags/ubuntu/"}],"title":"Ubuntu24.04 builds Samba server"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"I have mentioned before that you can use NAT+port mapping to access a virtual machine, but I found a problem during use: every time the host machine is restarted, the default Default Switch IP address will change, causing the virtual machine to be unable to access the gateway and thus unable to connect to the Internet.\nIn order to ensure that the virtual machine is connected to the Internet, you must manually set the IP address of the adapter corresponding to the Default Switch every time you restart the machine, which is very inconvenient. The following is a solution given in reference to the Microsoft official document .\nThe previous steps here can be implemented through the graphical user interface. First, create a new internal virtual switch: Name the virtual switch and remember it for later use. Confirm the creation: Find the newly created virtual machine adapter in the Windows network adapter, change the IP address to the gateway address set for the virtual machine, and change the subnet mask to be consistent with the virtual machine settings: Next, open PowerShell with administrator privileges and use the following command line to check whether there is a NAT network:\nGet-NetNat Because there can only be one NAT network on a machine, remove the existing NAT network first:\nGet-NetNat | Remove-NetNat Setting up NAT network:\n# InLan is the NAT network name, 172.25.64.0 is the subnet gateway IP address, and /24 is the subnet prefix length. New-NetNat -Name InLan -InternalIPInterfaceAddressPrefix 172.25.64.0/24 Regarding the subnet gateway, it can be in the same network segment as the IP address of the virtual machine adapter set above, that is, the gateway address of the virtual machine, and the end of it is set to 0; in this example, the gateway address of the virtual machine above is 172.25.64.1, so the subnet gateway address is generally set to 172.25.64.0. The subnet prefix is the subnet mask. Regarding the length of the subnet prefix, generally speaking, 2xx.2xx.2xx.0 is 24 bits, and 2xx.2xx.2xx.2xx is 32 bits.\nAfter the settings are completed, use Get-NetNat to check whether there is NAT network information:\nName : InLan ExternalIPInterfaceAddressPrefix : InternalIPInterfaceAddressPrefix : 172.25.64.0/24 IcmpQueryTimeout : 30 TcpEstablishedConnectionTimeout : 1800 TcpTransientConnectionTimeout : 120 TcpFilteringBehavior : AddressDependentFiltering UdpFilteringBehavior : AddressDependentFiltering UdpIdleSessionTimeout : 120 UdpInboundRefresh : False Store : Local Active : True The new NAT network is now configured, and the newly set virtual switch is replaced in the virtual machine settings. Log in to the virtual machine again and manually set the DNS (I don’t understand why the DNS cannot be automatically obtained through the gateway, maybe it’s a BUG\u0026hellip;), otherwise the virtual machine will not be able to access the external network. ","permalink":"https://andrewmoa.site/post/2025-03-21-hyperv-switch-static-ip-address/","tags":[{"LinkTitle":"Virtual-Machine","RelPermalink":"/tags/virtual-machine/"}],"title":"Hyper-V Virtual Switch NAT Setting Static IP"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"Why did you choose SIMULIA? First, Abaqus is powerful and can solve most structural problems. Second, fluid-structure coupling is convenient. STAR-CCM+ comes with a case that shows you how to couple with Abaqus bidirectionally. Third, it is the inertia of past experience. After all, automobile companies use Abaqus a lot, and there are a lot of cases and resources.\n1. Preparation First install the development environment and some necessary software:\nsudo apt update # Update software sources sudo apt upgrade # Update locally installed software sudo apt install build-essential # Installing the Development Environment sudo apt install csh tcsh ksh gcc g++ gfortran libstdc++5 build-essential make libjpeg62 libmotif-dev 解压安装包：\nmkdir iso # Create a new iso folder to store the decompressed files tar xvzf DS.SIMULIA.SUITE.2024.LINX64.tar.gz -C ./iso # Unzip to the iso folder 2. Installation process 2.1 Start the installation program First, define the environment variables, otherwise the installation program cannot be started:\n# You can add the following content to ${HOME}/.bashrc and restart the terminal export DSYAuthOS_`lsb_release -si`=1 export DSY_Force_OS=linux_a64 export NOLICENSECHECK=true Enter the /iso/1 folder and run the installer. Start the graphical interface and run ./StartGUI.sh in the terminal. It is similar to the installation method under Windows. Just follow the wizard step by step.\nSelect the text interface here:\n./StartTUI.sh # Start the text interface installation wizard Enter to next step.\nRemember to select 4, which means FLEXnet License Server. Type 4 and then Enter to continue.\nConfirm the installation content and press Enter to start the relevant installation program.\n2.2 Installing the License Server Select P and enter the absolute path to the /iso/4 folder.\nSelect the installation path of the license server. Here, choose to install it in the path ${HOME}/opt/SIMULIA/License/2024. Enter the absolute path.\nSelect the default here and start the license server after installation.\nSelect License File Path and enter the absolute path.\nAfter entering the license file, a message appears stating that retrieval of the host ID failed. Ignore it and enter 2 to continue.\nContinue to select 2.\nConfirm the installation information.\nThe installation is complete, press Enter to continue the next program installation.\n2.3 Install the solver Select P and enter the absolute path to the /iso/5 folder.\nSelect the installation path. Here, choose to install it in the path ${HOME}/opt/SIMULIA/EstProducts/2024.\nSelect the content to be installed, select all and enter -1.\nSelect the license type, the default is 1.\nEnter the access address and port of the license server, enter 29100@localhost to define License Server 1, and skip the rest by pressing Enter.\nSelect the path of the command line program. Here, choose to install it to ${HOME}/opt/SIMULIA/var/DassaultSystemes/SIMULIA/Commands.\nSelect the path of the external plug-in. Here, choose to install it to ${HOME}/opt/SIMULIA/var/DassaultSystemes/SIMULIA/CAE/plugins/2024.\nSelect the Tosca interface according to actual needs.\nSelect the Tosca Fluid working directory, which is assigned to ${HOME}/temp.\nSelect the STAR-CCM+ installation path, set it according to actual needs, and leave it blank by default.\nSelect STAR-CCM+ license, set according to actual situation, leave blank by default.\nSelect Fluent installation path, set according to actual situation, leave blank by default.\nConfirm the installation information and press Enter to start copying files.\nThe verification program will be started during the installation process, and the verification results can be viewed. Enter to continue.\nThe solver installation is complete, Enter to exit and enter the next installation program.\n2.4 Install CAA API Select P and enter the absolute path of the /iso/6 folder.\nSelect the installation path. Here, choose to install to ${HOME}/opt/SIMULIA/EstProducts/2024.\nConfirm the content to be installed, and press Enter to continue.\nThe installation is complete, and press Enter to exit.\n2.5 Install Isight Select the installation path. Here, choose to install to ${HOME}/opt/SIMULIA/Isight/2024.\nSelect the content to be installed. -1 selects all.\nWhether to start the TomEE configuration tool, skip by default.\nThis step is also skipped by default.\nThere is no installation document, choose to skip it.\nConfirm the installation content and press Enter to start copying files.\nThe installation is complete. Press Enter to exit.\n2.6 Installation completed Confirm the installation result and press Enter to exit the SIMULIA installation program.\n3. Post-installation configuration 3.1 Startup configuration Before starting, modify the configuration file:\nvim ${HOME}/opt/SIMULIA/EstProducts/2024/linux_a64/SMA/site/custom_v6.env Add two lines at the end, save and exit:\nlicense_server_type=FLEXNET abaquslm_license_file=\u0026#34;29100@localhost\u0026#34; Create a new environment variable configuration file:\ntouch ${HOME}/opt/SIMULIA/simulia24.env chmod +x ${HOME}/opt/SIMULIA/simulia24.env vim ${HOME}/opt/SIMULIA/simulia24.env Edit the content as follows. It is best to use absolute paths, save and exit:\nexport LICENSE_PREFIX_DIR=${HOME}/opt/SIMULIA/License/2024/linux_a64/code/bin export SIMULIA_COMMAND_DIR=${HOME}/opt/SIMULIA/var/DassaultSystemes/SIMULIA/Commands export PATH=$SIMULIA_COMMAND_DIR:$LICENSE_PREFIX_DIR:$PATH export LM_LICENSE_FILE=29100@localhost Run the following command to load the environment variables:\nsource ${HOME}/opt/SIMULIA/simulia24.env Start the Abaqus graphical interface by using the following command1:\nabaqus cae -mesa abaqus view -mesa 3.2 License installation problem If Abaqus can be started normally in [3.1](#31-Startup configuration), then this step can be skipped.\nVerify whether the license server is running:\nps -eaf | grep ABAQUSLM If the license server is not running, start the license server with the following command:\n${HOME}/opt/SIMULIA/License/2024/linux_a64/code/bin/licenseStartup.sh The startup failed, and the error message is as follows:\n/home/***/opt/SIMULIA/License/2024/linux_a64/code/bin/licenseStartup.sh: 2: /home/***/opt/SIMULIA/License/2024/linux_a64/code/bin/lmgrd: not found There is no solution. Either there is a problem with the installation package or this distribution lacks some runtime libraries. Let\u0026rsquo;s wait for the experts to help.\nFortunately, the Windows version license can be installed normally. You only need to assign the license path to the Windows machine. First, open port 29100 in Windows Firewall and create a new firewall inbound rule: Then modify the license address of the custom_v6.env file in [3.1](#31-Startup Configuration) as follows:\nlicense_server_type=FLEXNET # abaquslm_license_file=\u0026#34;29100@localhost\u0026#34; abaquslm_license_file=\u0026#34;29100@172.25.64.1\u0026#34; # 172.25.64.1 is the IP address of the Windows host Modify the environment variable file ${HOME}/opt/SIMULIA/simulia24.env:\n# export LM_LICENSE_FILE=29100@localhost export LM_LICENSE_FILE=29100@172.25.64.1 After loading the environment variables, the abaqus cae command can be started normally, and the license issue will no longer be prompted. I have to admit that the Linux graphical interface is really not easy to use, but who cares? Anyway, I don\u0026rsquo;t need to process models or grids under Linux, and I just submit calculations.\n3.3 Submit cluster calculation The Slurm script of Abaqus2 is as follows:\n#!/bin/bash #SBATCH --job-name=abaqus_test #SBATCH --partition=debug #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 1 #SBATCH --ntasks-per-node=32 cd $SLURM_SUBMIT_DIR source /home/***/opt/SIMULIA/simulia24.env # Fill in the absolute path export INPFILE=`find . -name \u0026#34;*.inp\u0026#34;` export ENVFILE=/home/***/opt/SIMULIA/EstProducts/2024/linux_a64/SMA/site/abaqus_v6.env # Fill in the absolute path # Generate abaqus_6.env file and specify hosts rm -rf $PWD/abaqus_v6.env cp $ENVFILE $PWD/abaqus_v6.env node_list=$(scontrol show hostname ${SLURM_NODELIST} | sort -u) mp_host_list=\u0026#34;[\u0026#34; for host in ${node_list}; do mp_host_list=\u0026#34;${mp_host_list}[\u0026#39;$host\u0026#39;, ${SLURM_NTASKS_PER_NODE}],\u0026#34; done mp_host_list=$(echo ${mp_host_list} | sed -e \u0026#34;s/,$/]/\u0026#34;) echo \u0026#34;mp_host_list=${mp_host_list}\u0026#34; \u0026gt;\u0026gt; $PWD/abaqus_v6.env # Create a Scratch Directory mkdir scratch.$SLURM_JOB_ID abaqus job=$SLURM_JOB_NAME input=$INPFILE cpus=$SLURM_NPROCS scratch=scratch.$SLURM_JOB_ID mp_mode=mpi double=both output_precision=full resultsformat=odb int ask=off \u0026gt; $SLURM_JOB_NAME.log rm -rf $PWD/abaqus_v6.env scratch.$SLURM_JOB_ID Put the inp file and the script in the same folder and submit the script using the sbatch command. After the calculation is completed, download the results to your local computer and view the results using Abaqus Viewer or Meta.\nfranaudo/abaqus-ubuntu \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nabhpc/ABHPC-Guide \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-03-21-ubuntu-install-simulia2024/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/tags/abaqus/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"},{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Slurm","RelPermalink":"/tags/slurm/"},{"LinkTitle":"Ubuntu","RelPermalink":"/tags/ubuntu/"}],"title":"Ubuntu Install SIMULIA2024"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"Slurm, like PBS and LSF, is a commonly used task management system for supercomputers. The advantages of Slurm are that it is open source, free, and highly active. In recent years, almost all emerging supercomputer platforms in China have provided Slurm as the main task management system. After PBS was open sourced, its activity was pitifully low. After updating to the latest system, there were always problems with installation, and no response was received for the issues raised. LSF has copyright risks and is not widely used in China. It is a very rare type. As for commands and scripts, these three are similar. Once you learn one, you can easily learn the other.\nIt is still very simple to install Slurm on Ubuntu. The important tools are basically compiled. You can directly install it with apt, and other dependencies will be automatically installed:\nsudo apt install slurmd\t# Install the compute node daemon sudo apt install slurmctld # Install the management node daemon Slurm requires a dedicated user for communication and other operations. The default username of this user is slurm. The above command has actually automatically generated the slurm user in Ubuntu, which can be verified by the following command:\nlastlog | grep slurm If Ubuntu does not generate the slurm user, you can generate it with the following command:\nsudo useradd slurm Slurm configuration files are mainly in the /etc/slurm/ directory, the main configuration file is: /etc/slurm/slurm.conf, we need to generate the configuration file. The official tool to assist in generating the configuration file is provided: Slurm Configuration Tool .\nAccording to the web page content prompts, fill in some key parts of the configuration file:\nClusterName=Cluster # Cluster name, any combination of English and numbers SlurmctldHost=dell-vm # Management node, fill in the name of the machine here NodeName=dell-vm # Compute node, also fill in the name of the machine PartitionName=debug # The partition where the compute node is located. The default value is debug. CPUs=32 # The number of CPU cores of the computing node should be filled in according to the actual situation. Sockets=1 # Number of CPU slots, fill in according to actual situation CoresPerSocket＝32 # The number of cores per socket should be filled in according to the actual situation. ThreadsPerCore=1 # The recommended number of threads per core is 1. It is not recommended to enable hyperthreading. SlurmUser=slurm # The default user is slurm. It is not recommended to change to root user. StateSaveLocation=/var/spool/slurmctld # The storage folder of the management node daemon process, the default is OK SlurmdSpoolDir=/var/spool/slurmd # The storage folder of the computing point daemon process, the default is OK For more explanation, please refer to the information on the USTC website. (1)\nAfter completing the web page content, click Submit at the bottom, copy the displayed configuration file template, and save it to the /etc/slurm/slurm.conf file:\nsudo vim /etc/slurm/slurm.conf\t# Copy and paste into this file Create a read-write folder for the daemon:\nsudo mkdir /var/spool/slurmd # Ubuntu prompts that the folder already exists, ignore it sudo mkdir /var/spool/slurmctld Start the Slurm service:\nsudo systemctl enable slurmd sudo systemctl enable slurmctld sudo systemctl start slurmd sudo systemctl start slurmctld Check the startup status of the Slurm daemon:\nsudo systemctl status slurmd sudo systemctl status slurmctld The slurmd daemon process started successfully, but the slurmctld daemon process started with an error. The error information is as follows:\n(null): _log_init: Unable to open logfile `/var/log/slurmctld.log\u0026#39;: Permission denied slurmctld: fatal: Incorrect permissions on state save loc: /var/spool/slurmctld To solve this problem, the easiest way is to change SlurmUser to root. Here is another method:\nsudo touch /var/log/slurmctld.log # Create a log file for the slurmctld daemon sudo chown slurm /var/log/slurmctld.log # Change the log file owner to the slurm user sudo chown -R slurm /var/spool/slurmctld # Change the owner of the slurmctld daemon read-write folder to the slurm user Just restart the slurmctld service:\nsudo systemctl restart slurmctld For Slurm scripts and command lines, domestic users can refer to the user manual written by Jiaotong University, which is quite comprehensive and will not be listed here. (2)\nThe following are some commonly used Slurm commands:\nThe configuration of the current node can be obtained by the following command:\nslurmd -C View the node status of the current cluster:\nsinfo -N View the specified node information:\nscontrol show node dell-vm # dell-vm is the name of the compute node View the job information submitted by the current user, usually only the queued and running jobs are displayed:\nsqueue Submit a job:\nsbatch jobscript.slurm # jobscript.slurm is a calculation script written by the user, and the suffix can be omitted To view and modify job status:\nscontrol show job ${JOB_ID} # View the information of the specified job. ${JOB_ID} corresponds to the job number in the first column displayed by squeue. scontrol hold ${JOB_ID} # Pause ${JOB_ID} scontrol release ${JOB_ID} # Resume ${JOB_ID} Cancel a specific job:\nscancel ${JOB_ID} # 取消${JOB_ID} (1) Slurm资源管理与作业调度系统安装配置 (2) Slurm 作业调度系统 ","permalink":"https://andrewmoa.site/post/2025-03-20-ubuntu-install-slurm/","tags":[{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Slurm","RelPermalink":"/tags/slurm/"},{"LinkTitle":"Ubuntu","RelPermalink":"/tags/ubuntu/"}],"title":"Ubuntu Install Slurm"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"Running calculation files in a virtual machine will cause the virtual disk to expand and take up too much disk space. At this time, you can transfer the calculation files to the host disk by mounting the host folder, avoiding the problem of virtual disk expansion. Creating a shared folder in Windows is omitted here. You only need to ensure that the virtual machine can access the host through the IP address.\n1. Check the resource path The following command checks the resource path shared by the server and confirms the mount point:\nsmbclient -L 172.25.64.1 -U ${username} Mount point access path: //172.25.64.1/Share\n2. Mounting method To access Windows shared folders in Ubuntu, you must first install the cifs tool:\nsudo apt install cifs-utils Then mount the shared folder using the mount command:\nsudo mount -t cifs //172.25.64.1/Share /mnt -o username=${username},password=${password} Here, the IP address 172.25.64.1 is the gateway address of the host machine accessed in the virtual machine, Share is the folder shared by the host machine, and /mnt is the local access path of the virtual machine to be mounted. Replace ${username} and ${password} after the command with the access username and password. It should be noted that the username of the local Windows user needs to be written in the form of ${computer name}\\${username}, connected with a backslash, for example: xxx-desktop\\administrator. If it is an online account, you need to fill in the full email account name. If the password contains special escape characters such as commas, the command line should not contain ,password= and the following content, and then enter the password according to the prompts to log in.\nIf there is a problem with no read or write permissions, add dir_mode=0777,file_mode=0777 to the mount command:\nsudo mount -t cifs //172.25.64.1/Share /mnt -o dir_mode=0777,file_mode=0777,username=${username},password=${password} If you want to add read and write permissions only for certain users, specify the user and group by uid and gid:\nsudo mount -t cifs //172.25.64.1/Share /mnt -o uid=user,gid=group,username=${username},password=${password} You can view the mount status through the mount command:\nmount | grep cifs Unmount via the umount command:\nsudo umount /mnt If you want to mount automatically when booting, you need to edit /etc/fstab, the content is as follows:\n//172.25.64.1/Share /mnt cifs auto,dir_mode=0777,file_mode=0777,username=${username},password=${password} 0 0 3. Special character passwords If the password contains special escape characters, you can use the following methods to automatically mount Windows shared folders when Linux boots up1:\nCreate a credential file: To keep the password secure, it is best to save the Windows shared username and password in a file that only root can access, such as: /etc/cifs-credentials, and make sure its permissions are set to be readable only by root.\nsudo touch /etc/cifs-credentials sudo chmod 600 /etc/cifs-credentials Use a text editor to edit the file. If the password contains special characters, enter them directly in the file (without escaping). Write the username (xxx-desktop\\administrator) and password (123456,abcde):\nusername=xxx-desktop\\administrator password=123456,abcde Edit the /etc/fstab file: Open the /etc/fstab file and add the following content at the end of the file to include the mount information:\n//172.25.64.1/Share /mnt cifs credentials=/etc/cifs-credentials,iocharset=utf8,file_mode=0777,dir_mode=0777 0 0 Note that the encoding of the /etc/cifs-credentials file needs to be UTF-8.\nThe above completes the automatic mount setting at boot time. You can verify it through df -h after restart. 4. Permission issues When using cifs to mount Windows shared folders, commands such as chmod and chown are invalid, and the permissions of the mounted files and folders cannot be adjusted. Here, NFS (Network File System) is used to mount shared folders to solve this problem.\nBy default, NFS does not provide any authentication mechanism, so there is no need to verify the username and password, which poses a certain security risk. NFSv3 completes authentication based on the client IP address2, and security can be improved by specifying the client IP address.\nWindows 10 can create NFS shared folders through the third-party tool haneWIN3. Download and install it and configure it through the graphical interface. I will not go into details here, but you can refer to other related articles4. Note that after the configuration is completed, the relevant ports must be opened in the firewall. Install NFS related tools on Ubuntu and enable related services:\nsudo apt install nfs-common rpcbind sudo systemctl start rpcbind sudo systemctl enable rpcbind Use the mount command to mount the shared directory. Note the -t nfs at the end:\nsudo mount -t nfs 172.25.64.1:/Share /mnt The unmount command is the same as cifs. When the network status is suddenly interrupted, you can add the -lf switch:\nsudo umount -lf /mnt Edit /etc/fstab and add automatic mount at boot:\n172.25.64.1:/Share\t/mnt\tnfs\tdefaults,_netdev 0 0 Linux开机自动挂载window密码有转义字符的共享文件夹 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNFS身份验证 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhaneWIN \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWindows服务器使用haneWIN NFS Server快速搭建NFS服务并挂载到Linux服务器 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/post/2025-03-20-ubuntu-mount-windows-share-dir/","tags":[{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Ubuntu","RelPermalink":"/tags/ubuntu/"}],"title":"Ubuntu mounts Windows shared folder (cifs+nfs)"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/categories/linux/"}],"content":"1. Requirements Consider installing Linux on a new computer, not a dual system, because it also needs to meet daily office work. If you don’t like to mess around, you can use WSL, which is implemented using Hyper-V, and at the same time, you can access the virtual machine through port mapping.\n2. Preparation 2.1 Download Ubuntu I am a CFD, so I naturally cannot do without Fluent. This thing is a distribution version. Confirm which distribution is supported from the official website information, and install which one is supported, so as not to reinstall the system later.\nAnsys Computing Platform Support 2024R1 Choose Ubuntu here and go to the official website to download the latest release.\n2.2 Turn on Hyper-V support Search \u0026ldquo;Enable or Turn off Windows Features\u0026rdquo; in the Start Menu to enable virtualization support.\nJust hook Hyper-V, install and restart.\n3. Install the virtual machine system To start the Hyper-V interface, follow the wizard to create a new virtual machine step by step.\nA few points to note:\nIt is best to specify virtual machines and virtual disks on other partitions with more spare space. Frequent reading and writing are performed later, which will greatly swell the virtual disk files. The maximum memory allocation is smaller before the first startup, otherwise it will take a lot of time to start. It is recommended to adjust the memory size to the desired memory after installing and configuring the virtual machine system. Virtual machine settings can turn on TPM, but turn off Secure Boot (unless enabled Microsoft UEFI Certification Authority ), otherwise the installation disk cannot be loaded. The Ubuntu installation process will not be explained carefully, just follow the wizard interface to install it step by step.\n4. Configure the virtual machine grid 4.1 Virtual machine assigns IP address We need to access the virtual machine through port mapping, so we need to assign a fixed IP address to the virtual machine. Since the virtual machine uses the default Default Switch For bridge settings, you need to view the IP address assigned by the host to the adapter. You can view it in the network connection options: The address shown in the figure above is the IP address of the virtual machine accessing the host. Then assign a fixed address to the virtual machine, and fill in the IP address of the gateway and DNS on the host. The subnet mask remains the same. It is recommended to use the IP address assigned now. After the setup is complete, ping Google\u0026rsquo;s DNS to see if it can be connected to the Internet normally:\nping 8.8.8.8 4.2 Turn on the remote desktop on the virtual machine Ubuntu provides a graphical interface to configure the remote desktop, which will not be introduced here. The following is the content of configuring the RPD remote desktop through the command line. Install third-party software:\nsudo apt install xrdp Enable XRDP service:\nsudo systemctl enable xrdp sudo systemctl start xrdp Check service status: Enable XRDP service:\nsudo systemctl sattus xrdp 4.3 Check the virtual machine firewall status Ubuntu is usually ufw(Uncomplicated Firewall), the following command checks whether the firewall on the system is enabled and displays its current configuration.\nsudo ufw status If the firewall is not enabled, the following command enables the firewall:\nsudo ufw enable The following command opens port 3389 in the firewall:\nsudo ufw allow from any to any port 3389 proto tcp At this time, we can connect to the Ubuntu virtual machine through the remote desktop instead of the virtual machine\u0026rsquo;s small window. Tip: Ubuntu needs to log out of the user in the virtual machine window before using the remote connection, otherwise there will be problems such as black screen and flashback. I don\u0026rsquo;t know when this bug will be fixed.\n5. Turn on virtual machine-related services 5.1 Turn on SSH service on the virtual machine Install OpenSSH:\nsudo apt install openssh-server Check SSH server status:\nsudo systemctl status ssh If the output displays Active: active (running) , indicating that the SSH server is running.\nIf the ssh service shows Active: inactive (dead) , enable the ssh service through the following command:\nsudo systemctl enable ssh sudo systemctl start ssh The configuration file of the OpenSSH server is located in the default /etc/ssh/sshd_config . Users can repair this configuration file as needed to change the relevant configuration, such as listening to ports, allowing or prohibiting password login, restricting logged in users, etc.\n5.2 Add firewall rules for virtual machines The following command adds SSH rules to the firewall:\nsudo ufw allow OpenSSH The following status indicates that the firewall configuration is successful: 5.3 Testing SSH connection The IP address of the Ubuntu virtual machine can be viewed in the Hyper-V manager window, or can be obtained in the virtual machine through the following command:\nip addr show | grep inet ip a | grep inet # Same effect Using bash or powershell client, connect to the server via the following command, username and ip_address Replace the username and IP address of the virtual machine respectively, and you will be prompted to enter your password:\nssh username@ip_address 5.4 Testing SFTP connection After enabling ssh, sftp will be enabled by default. The port number is 22 like ssh. Connect sftp through the following command, enter your password according to the prompts to log in:\nsftp username@ip_address Upload file put : Put the local server D:\\temp\\test The following directorytest.txtUpload files to remote server /home/username/test In the directory.\nsftp\u0026gt; lcd D:/temp/test sftp\u0026gt; cd /home/username/test sftp\u0026gt; put test.txt Upload folder put : Put the local server D:\\temp\\test The following directory logs Upload folder to remote server /home/username/test In the directory.\nsftp\u0026gt; lcd D:/temp/test sftp\u0026gt; cd /home/username/test sftp\u0026gt; put -r logs Download command: get , usage is similar to put. Common sftp commands can be passed help Check. It is recommended to use third-party tools, such as FileZilla Go to log in.\n6. Port Mapping 6.1 Host port mapping First, check the port occupancy of the host. exist PowerShell or CMD Check it through the following command:\nnetstat -ano # View all ports netstat -ano | findstr 8022 # 8022 is the query port number tasklist | findstr 5748 # 5748 refers to the pid corresponding to port 8022, which is used to check the program occupying the port. Open it in the host using administrator privileges PowerShell or CMD Window, query, add, and delete port mappings through the following commands.\n# Query port mapping netsh interface portproxy show v4tov4 # Query the specified IP port mapping netsh interface portproxy show v4tov4|findstr \u0026#34;192.168.100.135\u0026#34; \u0026lt;# Add a port mapping netsh interface portproxy add v4tov4 listenport=[Host port] listenaddress=[Host IP] connectaddress=[Virtual machine IP] connectport=[Virtual machine port] #\u0026gt; # Access SSH by mapping the host machine\u0026#39;s port 8022 to the virtual machine\u0026#39;s port 22 netsh interface portproxy add v4tov4 listenport=8022 listenaddress=192.168.100.135 connectaddress=172.25.68.88 connectport=22 # Access the remote desktop by mapping the host machine\u0026#39;s ports 63389 and 63390 to the virtual machine\u0026#39;s ports 3389 and 3390 # Port number range: 1-65535, cannot exceed this range netsh interface portproxy add v4tov4 listenport=63389 listenaddress=192.168.100.135 connectaddress=172.25.68.88 connectport=3389 netsh interface portproxy add v4tov4 listenport=63390 listenaddress=192.168.100.135 connectaddress=172.25.68.88 connectport=3390 \u0026lt;# Delete a port mapping netsh interface portproxy delete v4tov4 listenaddress=[Host IP] listenport=[Host port] #\u0026gt; # Delete the port mapping defined above netsh interface portproxy delete v4tov4 listenaddress=192.168.100.135 listenport=8022 netsh interface portproxy delete v4tov4 listenaddress=192.168.100.135 listenport=63389 netsh interface portproxy delete v4tov4 listenaddress=192.168.100.135 listenport=63390 After configuring the port mapping, you can access it in the remote desktop IP address:Port number The form of the virtual machine desktop is connected to the form of the virtual machine desktop.\n6.2 Host firewall settings The host firewall must open the port so that it can be accessed through the external network. Open first Windows Defender Firewall ,exist Advanced Create new inbound rules in it. Select the port for the rule type, enter the port number to enter the port number (separated by English half-width commas), and then confirm all of it later.\nUntil now, it was finally realized that the external network user accessed the virtual machine through the host\u0026rsquo;s IP address + port number.\n","permalink":"https://andrewmoa.site/post/2025-03-19-hyperv-install-ubuntu24/","tags":[{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Ubuntu","RelPermalink":"/tags/ubuntu/"},{"LinkTitle":"Virtual-Machine","RelPermalink":"/tags/virtual-machine/"}],"title":"Hyper-V Installation Ubuntu24.04"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"Recently, I often do some cold plate structural analysis, involving thin plate modeling. Although I have tried to use Shell units for analysis, the stress and displacement results of Shell units are larger than those of Solid units, and it is more likely that the calculation will not converge when it comes to contact problems. Fortunately, Ansa provides a way to generate a body mesh by stretching Shell units, which can quickly generate a Solid mesh through Shell units, greatly saving modeling time.\nThe modeling steps are as follows:\nSimplify the thin plate entity into a shell and divide the mesh. You can refer to Extracting mid-face using Ansa . Volume Mesh → Extrude to bring up the Extrude window and select the Shell unit to be stretched. Select None for the target surface. Select Offset for the stretch method and Both sides (middle) for the direction. Select the number of layers in Steps, input the thickness of the thin plate in Distance, it is recommended to select Smooth in Biasing, and complete it in Finish. Check the quality of the generated mesh and repair poor mesh. Summary: This method is more suitable for generating volume meshes for thin plates with uniform thickness. However, it is not applicable when the thickness of the thin plate is uneven or there are T-type connections (such as plastic parts, ribs, etc., as shown in the figure below). ","permalink":"https://andrewmoa.site/post/2025-03-18-ansa-thin-mesh/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Ansa薄板网格建模"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"1. Related commands In Ansa, the following commands are commonly used to set contact for Abaqus: 2. Contact pair The most commonly used contact pairs are generally created by calling up the creation dialog box through AUXILIARIES → CONTACT → CONTACT . Common settings are generally as follows:\nDefine the contact name for easy retrieval Define the contact type as contact pair (CONTACT PAIR) or general contact (CONTACT INCLUSIONS or CONTACT EXCLUSIONS) For the contact slave surface, the unit set needs to be set in SET in advance For the contact master surface, the unit set needs to be set in SET in advance Contact behavior, define friction coefficient, etc. Contact surface adjustment behavior, select YES to define the tolerance value in the subsequent POS_TOLER Whether the contact surface produces small slip, the default is NO Contact type, the default is surface-to-surface, or node-to-surface Contact gap, define the interference or gap value of the contact 3. General Contact It shares the same command and dialog box as creating contact pairs. You only need to change the contact type to CONTACT INCLUSIONS or CONTACT EXCLUSIONS. Define contact type Contact slave surface, select the unit set set in SET Contact master surface, select the unit set set in SET (usually not required, can be left blank) Contact behavior 4. Tie Contact Call up the dialog via AUXILIARIES → CONTACT → TIE. Contact the slave surface, select the unit set set in SET Contact the master surface, select the unit set set in SET When the two contact surfaces do not overlap, you can set the contact tolerance here 5.Contact Assistant Ansa built-in function that can guide the establishment of contact. Called through the command AUXILIARIES → CONTACT → Assistant. 6. Automatically create contact pairs Called up via AUXILIARIES → CONTACT → Flanges. Select the object to search Define the search distance tolerance Check and define contact parameters in the pop-up dialog box. 7. Contact-related output settings It is usually defined in the OUTPUT tab of STEP. If there are no special requirements, just select PRESELECT as the output parameter. ","permalink":"https://andrewmoa.site/post/2025-03-10-abaqus-contact/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/tags/abaqus/"},{"LinkTitle":"Ansa","RelPermalink":"/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Abaqus Contact Settings"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"After using Ansa to output the inp file, you can submit the inp file for calculation through the Abaqus CAE window interface, or you can submit the calculation through the command line. You need to add the Path when submitting using the command line. In Windows, it is usually the path [Abaqus installation directory]\\Commands.\nIn fact, after installing Abaqus in Windows, the Abaqus Command shortcut will appear in the start menu. Clicking this shortcut can submit the abaqus calculation file through the command line without adding the Path. Use the following command to submit the inp calculation file.\nabaqus job=[file-name] cpus=[number-of-cores] double=both output_precision=full int ask=off Generally speaking, you can start the calculation by specifying the file name through job. Some other command switches are not necessary options. Their meanings are as follows:\nhelp → View help double={ explicit | both | off | constraint } → Calculation accuracy output_precision={ single | full } → Output accuracy int / interactive → Print the solution process ask=off → Direct coverage without asking memory=5gb → Specify a maximum running memory of 5GB scratch=D:\\Temp → Specify where to store temporary files References: https://abaqus-docs.mit.edu/2017/English/SIMACAEEXCRefMap/simaexc-c-analysisproc.htm ","permalink":"https://andrewmoa.site/post/2025-03-06-abaqus-command/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/tags/abaqus/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Abaqus Submit Command"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"Ansa can quickly set up connections through the Connection function.\nTake screw connection as an example here, first display all bolts separately. use TOPO → Curves → Tubes2Curve Convert bolt geometry to curves. Keep the curve and delete the bolt geometric surface. Then use Topo → Points → Edges to convert the curve into 3D points and delete the curve. Generate bolt connection points based on 3D points, and select the 3D points generated in the previous step. In addition to defining connection points accidents based on curves, connection points can also be generated based on points and faces. However, different geometric forms correspond to different connection types, and attention should be paid to selecting the corresponding geometry according to the required connection type. Here are some commonly used connection types and geometric correspondences:\n3D points: screw connection, riveting, welding points Line segments: adhesive, weld, edge wrapping, screw connection (linear segments) Surface: Adhesive Bolt connection points can be generated based on 3D points or straight line segments, here you choose to generate them with 3D points. You can see in the model tree that the generated connection type is bolt. Throw all bolt connection points into a Part and the Part is displayed as the connection type. Select Connection Manager Connection Manager, select the connection point to be defined in the box and then use the middle mouse button to enter the connection definition dialog box. Select the connection method as BOLT, and fill in the search distance according to the geometric dimensions. Select the PID to be connected by selecting the part, double-click to enter the edit, and then enter the English half-width symbol ? to open the selection dialog box, which supports up to 4 PID connections. Here, the default connection method of ANSA is to search by Part, which needs to be manually changed to PID. Supplement the bolt radius and other connection information, and press Realize to generate the connection. BEAM needs to set the material and cross-sectional dimensions, and set them in the pop-up PID dialog box. The connection effect is as follows. PS: If it is a shell mesh, you can generate a bolted mesh by using the bolt connection points generated by the 3D points above (as shown above). But if it is a solid mesh, you must connect the bolt holes with bolt connection points generated by straight line segments to generate connection points with directions, and the connection type must also select BOLT ON SOLID. ","permalink":"https://andrewmoa.site/post/2025-03-05-ansa-connector/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Ansa Quick Connection"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"The default length unit for Ansa imported step model is mm, and the weight is ton. However, the SI unit we commonly use is m for length and kg for weight. The conversion between the two units is shown in the following table.\nLENGTH MASS TIME FORCE STRESS ENERGY DENSITY YOUNG’s MUDULUS Velocity (56.3KPH) GRAVITY m kg s N Pa Joule 7.83E+03 2.07E+11 15.64 9.81 mm ton s N MPa N-mm 7.83E-09 2.07E+05 1.564E+04 9.81E+03 When defining model parameters, care should be taken to select the corresponding unit system based on the length.\n","permalink":"https://andrewmoa.site/post/2025-03-05-ansa-unit/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Ansa system of units"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"When using Ansa to build surface meshes, we all hope that the round holes can be processed into the form of washer meshes to facilitate adding constraints later. But the mesh screw holes generated by the default settings don\u0026rsquo;t seem to be processed by washer. There are several ways to do this. One is to manually add Zone Cut by geometry processing. This method is too labor intensive and is not recommended.\nAnother method requires manually Reconstruct the mesh, first set the characteristic parameters of the hole in the mesh parameter settings. Return to Mesh and Reconstrust will appear. ","permalink":"https://andrewmoa.site/post/2025-02-28-ansa-washer-mesh/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Ansa round hole washer grid"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"Use the shortcut key F11 or the button below to define the Ansa grid quality. Surface mesh quality（Abaqus） Volume Mesh Quality（Abaqus） It is best to save the configuration file locally and reopen it next time to overwrite it.\n","permalink":"https://andrewmoa.site/post/2025-02-27-ansa-mesh-quelity/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Ansa Mesh Quality Settings"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/categories/ansa/"}],"content":"ANSA has several different treatment methods for the extraction of sheet metal parts.\n1. Skin method TOPO → Mid.Surfa. → Skin , select the entire geometry and then press the middle mouse button without brain to automatically draw the middle face. It is suitable for some sheet metal structural parts with relatively uniform thickness, without characteristics such as ribs, bosses, and pits. The extracted middle surface is a geometry without a grid. It requires manual animation of the grid, which will automatically delete the original geometry and add thickness. Some extruded parts such as extruded aluminum profiles are also supported, but if there are rounded corners, you must first deal with them, otherwise the mid-side rounded corners will be retained and enlarged. 2. Casting/Extrusion method There is also a method for pulling the surface for casting and extrusions, TOPO → Mid.Surfa. → Casting/Extrusion First, make sure that the geometry is closed and there are no wrong faces and edges. On the pop-up tab, select whether the parts to be drawn on the center surface are cast or extruded, and define the minimum thickness and surface grid size. It is best to define feature hole parameters in the grid parameters before drawing the surface, which is used to generate a washer grid, because after drawing the surface, it is directly a surface grid. The thickness of the final generated mesh is automatically calculated based on the geometry, and there is no need to set the thickness in the PID. 3. Middle Multi. Hand-processed Very troublesome, not recommended. However, this method can only be used for some complex structural parts, such as injection molded parts. First, some surfaces coplanar with features such as ribs should be divided according to topology to facilitate subsequent processing. Mark the front and back of the middle face with red and green, and the blue face is the side. The mid-side drawn will be automatically hidden to facilitate processing of the remaining models. 4. Summary Skin Method: Simple, suitable for sheet metal parts, generate geometric data, good accuracy Casting/Extrusion Method: Simple, suitable for casting and extrusion parts, generate grid data, slightly poor accuracy Middle Multi. Manual processing: cumbersome, but suitable for complex structural parts such as injection molded parts, etc., to generate geometric data, the accuracy varies from person to person ","permalink":"https://andrewmoa.site/post/2025-02-27-ansa-midsurf/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/tags/cae/"}],"title":"Extracting mid-face using Ansa"},{"categories":[{"LinkTitle":"Zhihu","RelPermalink":"/categories/zhihu/"}],"content":"Recently, there are many platforms that provide supercomputing trials, and many platforms have free trial applications. Due to work needs, I applied for an account on a supercomputing platform and conducted related trials. I made a simple record of the process of deploying STAR-CCM+ software and applications on the supercomputing platform, which also provides a reference for subsequent related applications.\n1. Supercomputer platform information You can log in to the supercomputer remotely through SSH connection. Some platforms also provide webSSH and webVNC connections, which support connecting to the command line or graphical interface through a browser. For specific login methods, please refer to the relevant documents provided by the platform.\nBefore logging in for the first time to install and deploy software, you should first understand the configuration of the supercomputer platform and determine whether the platform supports the software to be installed. Use the following command to understand the distribution information of the supercomputer platform.\nlsb_release -a It can be see that the platform distribution version is CentOS, version 7.9.2009.\nThe parallel job scheduling system used by this supercomputing platform is open source Slurm, and you can view the computing resources available for call through the following commands.\nsinfo -a The output is relatively long, only a part of it is cut off here. In the picture below amd_256 It indicates the partition where the computing node is located. Remember it, and you will use it after writing scripts.\n2. Software installation Please refer to the relevant documents provided by the platform for software upload and storage.\nThis article is installed with the 16.06.010 double precision Linux version. Decompression tar.gz package by following the following command.\ntar xvzf [file-name].tar.gz The installation file is unzipped starccm+_16.06.010 In the directory, enter the directory to run .sh The file starts to be installed. Note that root user permission is not required here (in most cases, the platform will not provide a root account, but it does not affect the software installation).\n./STAR-CCM+16.06.010_01_linux-x86_64-2.17_gnu9.2-r8.sh If you connect with VNC, you can install it through the graphical interface. If you do not want to install it through the graphical interface, you can force it through the console with the following command.\n./STAR-CCM+16.06.010_01_linux-x86_64-2.17_gnu9.2-r8.sh -i console This article is installed using the console method. First, prompt LICENSE, as shown in the figure below, press ENTER continue.\nWhether to accept the user agreement, enter Y ，ENTER Confirm to continue.\nUser experience plan, choose whether to accept (Y/N) according to your needs, and will not affect subsequent use.\nInstallation location, this article selects installation in ${HOME}/opt/Siemens In the directory, enter the absolute path as prompted. Y confirm.\nInstallation information,ENTERConfirm and start copying the file.\nInstallation is complete, ENTER Confirm to exit. Remember the installation path:\n${HOME}/opt/Siemens/16.06.010-R8/STAR-CCM+16.06.010-R8\nThe harmonious process is gone, please refer to the document yourself. Support genuine versions and combat piracy.\n3. Prepare SLURM scripts Let’s write a SLURM script below and submit a study to test it briefly.\n#!/bin/bash #SBATCH --job-name=carbin_tcm #SBATCH --partition=amd_256 #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 2 #SBATCH --ntasks-per-node=64 export MPI_TYPE=openmpi # intel platform openmpi export DIR=/***/home/***/opt/Siemens/16.06.010-R8/STAR-CCM+16.06.010-R8/star/bin export CDLMD_LICENSE_FILE=/***/home/***/opt/Siemens/license.dat export SIM_FILE=carbin_tcm.sim #export JAVA_FILE=carbin_tcm.java export MACHINEFILE=$SLURM_JOBID.node scontrol show hostnames $SLURM_JOB_NODELIST \u0026gt; $MACHINEFILE $DIR/starccm+ $SIM_FILE -batch $JAVA_FILE -np $SLURM_NPROCS -machinefile $MACHINEFILE -mpi $MPI_TYPE -rsh ssh -power --job-name The specified name is the case, which can be squeue The name displayed in the command.\n--partition The specified is the partition where the compute node is located, and the call here is amd_256 Compute nodes in partition.\n--output Specify the output file name, %j or $SLURM_JOBID Indicates the current job ID, specified by the platform itself.\n--error Specifies the output error file name.\n-N Specify the number of nodes called in this case, and 2 compute nodes are called here.\n--ntasks-per-node Specify the number of processes per node, where 64 threads are called per node are specified.\nvariable $SLURM_NPROCS It represents the total number of calculation processes, which can be automatically calculated based on the above two parameters, and the total calculation process is 128.\nparameter MPI_TYPE Specify the call type of mpi, recommended intel or openmpi ，platform Higher versions are no longer supported.\nparameter DIR Specify the STAR-CCM+ installation path, that is starccm + The path to which the file is located.\nparameter CDLMD_LICENSE_FILE Specify the access path of LICENSE, which can be the file path ** or the port number@host address** .\nparameter SIM_FILE Specify the test case file name.\nparameter JAVA_FILE Specify the macro file name. If a macro is used, you can remove the comments on this line and change the file name to the called macro file name.\nparameter MACHINEFILE Specify the node file.\nOrder scontrol show hostnames SLURM_JOB_NODELIST \u0026gt; MACHINEFILE Used to output hostname to node file.\nSave the study script file as carbin_tcm.slurm , the test case file is carbin_tcm.sim , a total of 2 files, uploaded. Calculate the time step, the number of iterations, etc. .sim First define it in the file, generate a grid, configure boundary conditions and then upload and calculate it.\nIf the study file is relatively large, you can compress and upload it and then decompress it. You can also clear the grid and upload it, and regenerate the grid, define boundary conditions, and calculate by playing the macro.\n4. Submit calculation tasks pass sbatch Command submits calculation tasks.\nsbatch carbin_tcm.slurm After submission, the ID is automatically generated and queued. The ID number of this study is 899634.\npass squeue Command to view the compute task queue.\nAfter the calculation is completed, just package and download the output file.\n","permalink":"https://andrewmoa.site/post/2022-04-08-deploy-starccm-on-scp/","tags":[{"LinkTitle":"Cfd","RelPermalink":"/tags/cfd/"},{"LinkTitle":"Slurm","RelPermalink":"/tags/slurm/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/tags/star-ccm+/"}],"title":"Deploy STAR-CCM+ on supercomputing platform"},{"categories":[{"LinkTitle":"Zhihu","RelPermalink":"/categories/zhihu/"}],"content":"Conclusion first: TenFong Tech’s products QFlux and QMesh do not align with their marketing claims. QMesh is not an \u0026ldquo;independently developed advanced computational fluid dynamics (CFD) core software,\u0026rdquo; and QFlux is far from feature-complete. There is strong evidence to suggest that QFlux’s solver may have been \u0026ldquo;borrowed\u0026rdquo; from OpenFOAM.\n1. Software Overview and Download Let’s skip the preamble. TenFong Tech’s marketing claims can be seen on their official websites（http://www.tenfong.cn/、http://www.qfx-tech.com/）.\nThe company lists two products: QFlux (a CFD solver) and QMesh (a pre-processing mesh tool). Interested users can register and download them from the website.\nDue to browser settings (or perhaps intentional design), clicking the download button does not prompt a confirmation dialog. Instead, the installer is silently downloaded to the system’s temporary folder (Windows: C:\\Users[Username]\\AppData\\Local\\Temp). Finding the downloaded files requires some effort.\nAfter extraction, you’ll get two installers: QFlux_x64.exe and QMesh_x64.exe. The installation process is straightforward and won’t be detailed here.\n2. Pre-processing QMesh Let’s start with QMesh.\nQMesh uses the VTK library for mesh visualization and Qt for its GUI. However, there is no mention of Qt’s licensing (GPL, LGPL, or commercial). Did TenFong Tech properly license Qt?\nQMesh’s core mesh generator is snappyHexMesh, a tool bundled with OpenFOAM.\nThe help file openly acknowledges this. But TenFong’s website claims QMesh is \u0026ldquo;independently developed.\u0026rdquo; If the core tool is borrowed, how is this \u0026ldquo;independent\u0026rdquo;? Did they only develop a GUI wrapper?\nUpon launching, QMesh displays a 180-day trial period. The version is 2.1.3, copyrighted by \u0026ldquo;Shenzhen Qingfengxi Co., Ltd.\u0026rdquo;\nUsing Fluent’s classic elbow case, we exported an STL file from ANSA.\nQMesh imports STL files but fails to recognize pre-partitioned surfaces or allow re-partitioning.\nBackground mesh settings resemble OpenFOAM’s blockMesh, but users cannot adjust grid sizes. The \u0026ldquo;locationInMesh\u0026rdquo; parameter (familiar to OpenFOAM users) is set via the \u0026ldquo;Region\u0026rdquo; tab.\nMesh refinement and boundary layer controls mirror OpenFOAM’s snappyHexMeshDict parameters.\nClick \u0026ldquo;Grid\u0026rdquo;. In the pop-up dialog box, you can choose to set related parameters in parallel and select the grid and boundary layer.\nThrough the task manager, you can see that QMesh actually performs mesh generation operations by calling snappyHexMesh.\nSince snappyHexMesh is used for mesh generation, some inherent defects of snappyHexMesh are inevitably introduced. It can be seen that the local mesh is deformed, which is actually caused by the defects of snappyHexMesh. There is still no good solution. Since the background mesh size cannot be adjusted, when the local mesh and the background mesh overlap, a new boundary area may be generated, just like the y-minus area in the figure above.\nExport the mesh generated by QMesh to elbow.msh (Fluent Mesh format) and elbow.cgns (CGNS general format). The QMesh project file is saved in .qmh format. You can see that the size of elbow.cgns is only 4kB, and it is obvious that the CGNS format is not saved successfully.\n3. CFD Solver QFlux QFlux also relies on Qt and VTK, again without proper licensing notices.\nThe core solver is qcore. Let’s talk about later.\nQFlux’s interface (version 2.7.152.0) is copyrighted by \u0026ldquo;Shenzhen TenFong Technology Co., Ltd.\u0026rdquo;\nAttempting to import the QMesh-generated .msh file fails—ironic, given both tools are from TenFong.\nImporting an ANSA-generated CGNS file also fails to recognize boundaries.\nUsing OFoamToQFlux (a bundled tool), we converted an OpenFOAM mesh to QFlux format.\ncd /d D:\\opt\\tenfong\\QFlux #[QFlux Installion] set path=%cd%;%path% cd /d F:\\tenfong\\solve #[OpenFOAM Case Dir] OFoamToQFlux %cd%\\[OpenFOAM Case Dir]\\constant\\polyMesh out.qfx This will generate a QFlux grid file and save it in QFlux format (.qfx). You can directly open the out.qfx file.\nThis successfully displays the boundary information, and the boundary conditions can be modified later.\nThe calculation case is taken from the classic case elbow of Ansys Fluent, and the fluid and boundary parameter settings are shown in the figure above.\nFor the basic equations of QFlux, it seems that you can only choose the incompressible flow solver Incompressible (hey, where is the compressible flow solver in the propaganda?) and activate the energy equation.\nTurbulence model, nothing special to say. The turbulence parameters on the right can also be modified in the turbulenceProperties file of OpenFOAM. The only bright spot may be the near-wall processing of the turbulence model, which is better than debugging the wall functions in the k-file and nut-file one by one in OpenFOAM. (PS: It\u0026rsquo;s just a GUI. I can also achieve the same function by wrapping it with Qt.)\nThe numerical method is the choice of solver, discretization format and relaxation factor, which is basically equivalent to the controlDict, fvSchemes and fvSolution files of OpenFOAM, except that you don\u0026rsquo;t have to debug them one by one. It seems that the only solver you can choose is the SIMPLE algorithm.\nThere are only three materials in the material library: air, water and structural steel.\nFor the inlet and outlet boundary conditions, the flow direction cannot be specified as the surface normal, and the flow direction coordinates can only be entered manually. However, its turbulence parameter definition method is similar to other commercial CFD software, which is much more convenient than calculating one by one in OpenFOAM and then manually defining the k file, epsilon file, and nut file. It seems that the units of velocity and temperature cannot be changed, so if the user uses the ℃ unit, then the temperature scale must be converted by himself.\nSet the convergence conditions, and set the convergence judgment tolerances of the momentum equation, energy equation, continuity equation, and turbulent viscosity equation to 1.0e-6.\nSet the calculation conditions, the maximum number of calculation steps is 1000, and the number of parallel cores can be set. It can be seen that the MPI library used in the Windows version of QFlux is the Microsoft MPI library, which is surprisingly consistent with the Windows version of OpenFOAM. The calculation calls the qcore.exe program.\nIt can be seen that the calculation converges at about 385 steps, but when the mouse stays on the convergence curve, the label number can only be displayed to four digits after the decimal point, and the label number cannot be displayed in scientific notation.\nThe above figure is the temperature cloud map output by QFlux calculation results.\nThe above figure is the temperature cloud map of the same case calculated by OpenFOAM. The same case can be achieved by simpleFoam+scalarTransportFoam or buoyantSimpleFoam. It can be seen that the calculation results of the two are similar.\n4. QFlux vs. OpenFOAM Comparing QFlux and OpenFOAM (v2012) DLLs reveals striking similarities in structure and function names.\nUsing \u0026ldquo;Dependency Walker\u0026rdquo; to view the function names of the dynamic link library files referenced by the two, it can be found that in some key link libraries, the internal function naming methods cannot be said to be exactly the same, but can only be said to be highly similar.\nSince OpenFOAM is open source software and its source code can be downloaded online, it is hard to say that QFlux did not \u0026ldquo;borrow\u0026rdquo; from OpenFOAM source code. As for how much content was \u0026ldquo;borrowed\u0026rdquo;, it is hard to say.\nIn view of the extremely weak status of domestic CFD and other industrial software, it seems that compared with independent development, it is easier to be welcomed by software developers and investors to quickly package easy-to-use software platforms on open source platforms. Comparing the differences in function implementation, development level and code volume between OpenFOAM and QFlux, the size, code volume and maturity of OpenFOAM programs are far higher than those of QFlux. In fact, developing relevant interfaces and introducing process control on the OpenFOAM platform have always been the focus of industry research. Many companies at home and abroad are conducting related applications and secondary development around OpenFOAM. However, it is possible that only domestic Internet companies can \u0026ldquo;borrow\u0026rdquo; from open source programs while claiming to be completely \u0026ldquo;independently developed\u0026rdquo;. Of course, \u0026ldquo;Tenfong Technology\u0026rdquo; is one of them.\nIn fact, for open source platforms such as OpenFOAM, communication and innovation are the most important. The realization of new methods is far more important than industrial packaging and ease of use. This is one of the reasons why OpenFOAM has insisted on text and command line parameter control for so many years. What is more terrible than \u0026ldquo;borrowing\u0026rdquo; itself is numbness and inertia. When we are used to open source platforms, we only know how to take without giving back, and we only know how to reconstruct other people\u0026rsquo;s codes without building and improving them from the bottom up. We know what it is but not why it is\u0026hellip; When we lose the open source platform one day, will we still be able to insist on the so-called \u0026ldquo;independent development\u0026rdquo;?\n","permalink":"https://andrewmoa.site/post/2021-09-08-tenfong/","tags":[{"LinkTitle":"Cfd","RelPermalink":"/tags/cfd/"}],"title":"Unveiling TenFong Tech's Domestic CFD Software—QFlux and QMesh"},{"categories":[{"LinkTitle":"Zhihu","RelPermalink":"/categories/zhihu/"}],"content":"This paper uses different CFD solvers to verify the laminar flow problem. Verify the accuracy of each solver by listing the operating procedures and differences in the output results of different solvers.\n1. Problem description As shown in the figure below, a fixed-constant laminar flow model between two concentric cylinders is established. The flow is caused by the rotation of the inner cylinder at a constant angular velocity, while the outer cylinder remains stationary. Using periodic boundaries, only a portion of the basin needs to be modeled. The physical model and input data are shown in the following table.\nSource Ansys Verification Study References F. M. White. Viscous Fluid Flow. Section 3-2.3. McGraw-Hill Book Co., Inc.. New York, NY. 1991. Physical Model Laminar Flow, Rotating Wall The fluid physical parameters, geometric dimensions and boundary conditions are shown in the following table.\nFeatures Units Parameters Fluid Density kg/m^3 1.0 Fluid Viscosity kg/m-s 0.0002 Inner Cylindrical Radius mm 17.8 External Cylindrical Radius mm 46.28 Inner Cylindrical Speed ​​ rad/s 1.0 The solver and input model used in this article are as follows.\nSolver Input Model Ansys Fluent 2020R2 VMFL001_rot_conc_cyl_2D.msh Ansys CFX 2020R2 VMFL001_rot_conc_cyl_3D.msh Siemens STAR-CCM+ 2020.2.1(15.04.010-R8) VMFL001_rot_conc_cyl_2D.msh OpenFOAM v2006 VMFL001_rot_conc_cyl_2D.msh SU2 v7.1.1 VMFL001_rot_conc_cyl_2D.cgns 2. Fluent verification 2.1 Solver Settings Open Fluent and select the 2D solver. The number of grids is small and only one processor core is running.\nImport the mesh model VMFL001_rot_conc_cyl_2D.msh as shown below.\nChange the physical model and select Laminar flow.\nChange fluid parameters. Rename \u0026ldquo;air\u0026rdquo; to \u0026ldquo;vmfl001\u0026rdquo;, with a density of 1kg/m3, and a viscosity of 0.0002kg/m-s.\nYou can see that the imported periodic (id=10) and shadow_periodic (id=13) boundaries are not periodic boundaries, and you need to enter them in the command line window./mesh/modify-zones/make-periodicCommand, establish periodic boundaries.\n\u0026gt; /mesh/modify-zones/make-periodic Periodic zone [()] 10 Shadow zone [()] 13 Rotationally periodic? (if no, translationally) [yes] yes Create periodic zones? [yes] yes Change the innerwall (id=11) boundary to the rotating wall, and set it as follows.\nThe solver selects SIMPLE as shown below.\nThe convergence residual is changed to 1e-6 and initialized.\nThe number of iteration steps is 800, solve and save.\n2.2 Post-processing Establish 4 points at a radius of 20~35mm, and the coordinates are shown in the table below.\nName x (m) y (m) point-1 0.02 0 point-2 0.025 0 point-3 0.03 0 point-4 0.035 0 Outputs the tangential velocity of four points.\nCreate a new horizontal straight line and output the tangential velocity distribution curve under different radii.\nOutput curve point data to the file for later use.\nComparison of the laminar flow equation and the error of Fluent calculation results.\nRadius [mm] Tangential velocity (theoretical value) [m/s] Tangential velocity (Fluent simulation result) [m/s] Error [%] 20 0.0151 0.015027723 0.48 25 0.0105 0.010464473 0.34 30 0.0072 0.0071237963 1.06 35 0.0046 0.0044973576 2.23 3. CFX verification 3.1 Solver Settings Start CFX-Pre, create a new study, and import the fluent grid file VMFL001_rot_conc_cyl_3D.msh.\nNew material, named \u0026ldquo;vmfl001\u0026rdquo;. Density 1 [kg m^-3], dynamic viscosity 0.0002 [Pa s].\nCreate a new fluid domain (Domain), name it FLUID, and select FLUID in the mesh area (Location). Material Select the newly created vmfl001. Turn off the heat transfer equation, and select None(Laminar) for the turbulence equation.\nA new boundary is created in this fluid domain, and the corresponding grid boundary is shown in the following table.\nFluid Boundaries Grid Boundaries Boundary Types INNERWALL INNERWALL Wall OUTERWALL OUTERWALL Wall SYMM 1 SYMM 1 Symmetry SYMM 2 SYMM 2 Symmetry Where INNERWALL is specified as the rotating wall, as shown in the figure below, the angular velocity is 1 [radian s^-1].\nNew interfaces (Interfaces) are named PERIODIC, as shown in the figure below.\nCreate a new global initialization (Global Initialization), confirm directly, and use the default value.\nChange Solver Control, change the maximum iteration steps to 1000, change the residual type to MAX, and change the convergence residual target to 1e-6.\nSave the study and output the .def file. Open CFS-Solver Manager to load the .def file, select double precision for solver type, and start solving.\nIt was about 23 steps to converge.\n3.2 Post-processing Open CFD-Post and load the result file that was output after solving the solution. Create 4 points at a radius of 20~35mm, as shown in the following table.\nName x (m) y (m) z (m) Point 1 0.02 0 0 Point 2 0.025 0 0 Point 3 0.03 0 0 Point 4 0.035 0 0 Create the following 4 expressions (Expressions) and output the tangential velocity of 4 points.\nExpression name Expression definition VelYOnPoint1 maxVal(Velocity v)@Point 1 VelYOnPoint2 maxVal(Velocity v)@Point 2 VelYOnPoint3 maxVal(Velocity v)@Point 3 VelYOnPoint4 maxVal(Velocity v)@Point 4 Create a new Table and list the radius and speed of the 4 points.\nCreate a straight line \u0026ldquo;Line 1\u0026rdquo;, as shown in the figure below.\nCreate a chart (Chart), select the \u0026ldquo;Location\u0026rdquo; in \u0026ldquo;Data Series\u0026rdquo; and select the \u0026ldquo;Line 1\u0026rdquo; you just created, select \u0026ldquo;X\u0026rdquo; in \u0026ldquo;X Axis\u0026rdquo; and select \u0026ldquo;Variable\u0026rdquo; in \u0026ldquo;Y Axis\u0026rdquo; and select \u0026ldquo;Velocity v\u0026rdquo;. The output curve is shown in the figure below. Export curve data to a .csv file for later use.\nComparison of laminar flow equation with the error of CFX calculation results.\nRadius [mm] Tangential velocity (theoretical value) [m/s] Tangential velocity (CFX simulation result) [m/s] Error [%] 20 0.0151 0.0150925 0.05 25 0.0105 0.0105287 0.27 30 0.0072 0.00718282 0.24 35 0.0046 0.00454726 1.15 4. STAR-CCM+ Verification 4.1 Solver Settings Turn on STAR-CCM+. Since the number of meshes is small, the process option can be selected for serialization, and only one processor core is used for solving. Import the fluent grid file VMFL001_rot_conc_cyl_2D.msh.\nSelect the solver physical model, as shown in the figure below.\nThe gas material \u0026ldquo;Air\u0026rdquo; was renamed to \u0026ldquo;vmfl001\u0026rdquo;, the density was modified to 1.0 kg/m^3, and the dynamic viscosity was modified to 2.0E-4 Pa-s.\nExpand \u0026ldquo;FLUID-7\u0026rdquo; in the fluid area, select the boundaries of \u0026ldquo;PERIODIC\u0026rdquo; and \u0026ldquo;SHADOW_PERIODIC\u0026rdquo; and right-click to create the interface. The newly created interface topology is changed to \u0026ldquo;period\u0026rdquo;, the period conversion is selected as \u0026ldquo;rotate\u0026rdquo; and the rotation axis is specified as \u0026ldquo;specified axis\u0026rdquo;, and the axis direction is \u0026ldquo;[0.0, 0.0, 1.0]\u0026rdquo;.\nCreate a new rotational motion in \u0026ldquo;Tools\u0026rdquo; and \u0026ldquo;Movement\u0026rdquo;, with rotation axis direction [0.0, 0.0, 1.0], and rotation rate of 1.0 radian/s.\nChange the reference coordinate system specification option for INNERWALL boundary to \u0026ldquo;Local reference coordinate system\u0026rdquo;, and the boundary reference coordinate system specifies the coordinate system to select the newly created rotational motion.\nThe maximum number of iteration steps in the stop standard is 1000 by default, run the solution and save the study.\n4.2 Post-processing Create 4 new probe points in the derivative components, as shown in the table below.\nName x (m) y (m) z (m) p1 0.02 0 0 p2 0.025 0 0 p3 0.03 0 0 p4 0.035 0 0 Similarly, a new isosurface \u0026ldquo;y=0\u0026rdquo; is created in the derivative components, as shown in the figure below.\nCreate a new \u0026ldquo;XYZ internal table\u0026rdquo; in \u0026ldquo;Tools\u0026rdquo; and \u0026ldquo;Table\u0026rdquo;, select \u0026ldquo;[Velocity[j]]\u0026rdquo; in the scalar, select the newly created four points of p1~p4 in the component, extract the email and then tabulate it, displaying the tangential velocity of the 4 points.\nCreate a new \u0026ldquo;XY drawing\u0026rdquo; in \u0026ldquo;Drawing\u0026rdquo;, select the newly created isosurface \u0026ldquo;y=0\u0026rdquo; for the component, and select the field function in \u0026ldquo;Y type\u0026rdquo; and \u0026ldquo;Y Type 1\u0026rdquo;. The curve diagram is as follows. Export curve data to a .csv file for later use.\nComparison of the laminar flow equation with the error of STAR-CCM+ calculation results.\nRadius [mm] Tangential velocity (theoretical value) [m/s] Tangential velocity (STAR-CCM+simulation result) [m/s] Error [%] 20 0.0151 0.0153266630659337 1.50 25 0.0105 0.0106689558311781 1.61 30 0.0072 0.00671828465572254 6.69 35 0.0046 0.004432094526565 3.65 5. OpenFOAM Verification 5.1 Solver Settings First, copy the simpleCar from the official case to the current directory and rename it to vmfl001. Here, MSYS2 is used as the command line interactive interface and execute the following commands.\ncp -r $FOAM_TUTORIALS/incompressible/simpleFoam/simpleCar . mv simpleCar vmfl001 cd vmfl001 Copy the VMFL001_rot_conc_cyl_2D.msh file into the vmfl001 folder and execute the following command to convert the fluent grid to an OpenFOAM grid.\nfluentMeshToFoam VMFL001_rot_conc_cyl_2D.msh Open the constant/polyMesh/boundary file, modify it as follows, change the boundaries of \u0026ldquo;PERIODIC\u0026rdquo; and \u0026ldquo;SHADOW_PERIODIC\u0026rdquo; from the wall to the periodic boundary.\nFoamFile { version 2.0; format ascii; class polyBoundaryMesh; location \u0026#34;constant/polyMesh\u0026#34;; object boundary; } 5 ( PERIODIC { type cyclicAMI; inGroups 1(wall); nFaces 20; startFace 2320; matchTolerance 0.0001; transform rotational; rotationAxis (0 0 1); rotationCentre (0 0 0); neighbourPatch SHADOW_PERIODIC; } INNERWALL { type wall; inGroups 1(wall); nFaces 60; startFace 2340; } OUTERWALL { type wall; inGroups 1(wall); nFaces 60; startFace 2400; } SHADOW_PERIODIC { type cyclicAMI; inGroups 1(wall); nFaces 20; startFace 2460; matchTolerance 0.0001; transform rotational; rotationAxis (0 0 1); rotationCentre (0 0 0); neighbourPatch PERIODIC; } frontAndBackPlanes { type empty; inGroups 1(empty); nFaces 2400; startFace 2480; } ) Delete the \u0026ldquo;Allrun\u0026rdquo; file in the vmfl001 directory. Delete \u0026ldquo;epsilon\u0026rdquo;, \u0026ldquo;k\u0026rdquo; and \u0026ldquo;nut\u0026rdquo; in the vmfl001/0 directory, and only \u0026ldquo;p\u0026rdquo; and \u0026ldquo;U\u0026rdquo; are retained.\nModify the \u0026ldquo;p\u0026rdquo; file as follows.\nFoamFile { version 2.0; format ascii; class volScalarField; object p; } dimensions [0 2 -2 0 0 0 0]; internalField uniform 0; boundaryField { \u0026#34;(INNERWALL|OUTERWALL)\u0026#34; { type zeroGradient; } frontAndBackPlanes { type empty; } \u0026#34;(PERIODIC|SHADOW_PERIODIC)\u0026#34; { type cyclicAMI; value $internalField; } } Modify the \u0026ldquo;U\u0026rdquo; file as follows.\nFoamFile { version 2.0; format ascii; class volVectorField; object U; } dimensions [0 1 -1 0 0 0 0]; internalField uniform (0 0 0); boundaryField { INNERWALL { type rotatingWallVelocity; axis (0 0 1); origin (0 0 0); omega constant 1.0; value uniform (0 0 0); } OUTERWALL { type noSlip; } frontAndBackPlanes { type empty; } \u0026#34;(PERIODIC|SHADOW_PERIODIC)\u0026#34; { type cyclicAMI; value $internalField; } } Execute the following command to check the grid boundary condition settings.\ncheckMesh Modify the \u0026ldquo;transportProperties\u0026rdquo; file in the constant directory and change the nu value to 0.0002.\nFoamFile { version 2.0; format ascii; class dictionary; location \u0026#34;constant\u0026#34;; object transportProperties; } transportModel Newtonian; nu 0.0002; Modify the \u0026ldquo;turbulenceProperties\u0026rdquo; file in the constant directory and adopt the laminar flow model.\nFoamFile { version 2.0; format ascii; class dictionary; object turbulenceProperties; } simulationType laminar; In the system directory, only the three files \u0026ldquo;controlDict\u0026rdquo;, \u0026ldquo;fvSchemes\u0026rdquo; and \u0026ldquo;fvSolution\u0026rdquo; are retained, and all the others are deleted.\nModify the \u0026ldquo;controlDict\u0026rdquo; file as follows, solve the simpleFoam program, start the solution from the last step of saving, the iteration steps are 1,000, and the solution result is written every 100 steps.\nFoamFile { version 2.0; format ascii; class dictionary; location \u0026#34;system\u0026#34;; object controlDict; } application simpleFoam; startFrom latestTime; startTime 0; stopAt endTime; endTime 1000; deltaT 1; writeControl timeStep; writeInterval 100; purgeWrite 1; writeFormat ascii; writePrecision 8; writeCompression off; timeFormat general; timePrecision 8; runTimeModifiable true; Modify the \u0026ldquo;fvSolution\u0026rdquo; file as follows.\nFoamFile { version 2.0; format ascii; class dictionary; location \u0026#34;system\u0026#34;; object fvSolution; } solvers { p { solver GAMG; smoother GaussSeidel; tolerance 1e-08; relTol 0.01; } U { solver smoothSolver; smoother symGaussSeidel; tolerance 1e-08; relTol 0.01; } } SIMPLE { nNonOrthogonalCorrectors 0; residualControl { p 1e-8; U 1e-8; } pRefCell 0; pRefValue 0; } relaxationFactors { fields { p 0.3; } equations { U 0.7; } } Return to the vmfl001 directory, execute the following command (if the tee command is used in Windows, it can be executed in Powershell), run the solution and record the output of the solution process.\nsimpleFoam | tee simpleFoam.log 5.2 Post-processing It can be used directly under LinuxparaFoamStart ParaView for post-processing. Run the following command in Windows to generate a .foam file in the vmfl001 directory, and use ParaView to open the .foam file to load the solution result of OpenFOAM.\necho . \u0026gt; vmfl001.foam Based on vmfl001.foam, 4 new \u0026ldquo;Probe Locations\u0026rdquo; are created to extract the tangential speed at 20~35mm.\nName Center Radius ProbeLocation1 (0.020,0,0) 0 ProbeLocation2 (0.025,0,0) 0 ProbeLocation3 (0.030,0,0) 0 ProbeLocation4 (0.035,0,0) 0 Create a new GroupDatasets, including the above 4 ProbeLocations.\nSelect the GroupDatasets to see the tangential velocity values ​​of the 4 point positions in the \u0026ldquo;SpreadSheetView\u0026rdquo; window on the right.\nBased on vmfl001.foam, create a \u0026ldquo;PlotOnIntersectionCurves\u0026rdquo; drawing.\nSelect the Y-axis forward direction in the plane direction, as shown in the figure.\nSelect \u0026ldquo;Points_X\u0026rdquo; for the X-axis display variable, and select \u0026ldquo;U_Y\u0026rdquo; for the Y-axis display variable, as shown in the figure.\nThe resulting curve can be viewed in the \u0026ldquo;LineChartView\u0026rdquo; window on the right.\nSelect \u0026ldquo;File\u0026rdquo; and \u0026ldquo;Export Scene\u0026rdquo; to export curve data to the csv file.\nComparison of the laminar flow equation and the error of OpenFOAM calculation results.\nRadius [mm] Tangential velocity (theoretical value) [m/s] Tangential velocity (OpenFOAM simulation result) [m/s] Error [%] 20 0.0151 0.0150 0.66 25 0.0105 0.0104366 0.60 30 0.0072 0.00712633 1.02 35 0.0046 0.00452779 1.57 6. SU2 Verification 6.1 Solver Settings The content of the SU2 solver configuration file vmfl001.cfg is as follows:\n% ------------- DIRECT, ADJOINT, AND LINEARIZED PROBLEM DEFINITION ------------% % % Physical governing equations (EULER, NAVIER_STOKES, % WAVE_EQUATION, HEAT_EQUATION, FEM_ELASTICITY, % POISSON_EQUATION) SOLVER= INC_NAVIER_STOKES % % Specify turbulence model (NONE, SA) KIND_TURB_MODEL= NONE % % Mathematical problem (DIRECT, CONTINUOUS_ADJOINT) MATH_PROBLEM= DIRECT % % Restart solution (NO, YES) RESTART_SOL= NO % ---------------- INCOMPRESSIBLE FLOW CONDITION DEFINITION -------------------% % % Density model within the incompressible flow solver. % Options are CONSTANT (default), BOUSSINESQ, or VARIABLE. If VARIABLE, % an appropriate fluid model must be selected. INC_DENSITY_MODEL= CONSTANT % % Solve the energy equation in the incompressible flow solver INC_ENERGY_EQUATION = YES % % Initial density for incompressible flows (1.2886 kg/m^3 by default) INC_DENSITY_INIT= 1.0 % % Initial temperature for incompressible flows that include the % energy equation (288.15 K by default). Value is ignored if % INC_ENERGY_EQUATION is false. INC_TEMPERATURE_INIT= 293.15 % % Initial velocity for incompressible flows (1.0,0,0 m/s by default) INC_VELOCITY_INIT= ( 1.0, 0.0, 0.0 ) % % ---- IDEAL GAS, POLYTROPIC, VAN DER WAALS AND PENG ROBINSON CONSTANTS -------% % % Fluid model (STANDARD_AIR, IDEAL_GAS, VW_GAS, PR_GAS, % CONSTANT_DENSITY, INC_IDEAL_GAS) FLUID_MODEL= CONSTANT_DENSITY % % Specific heat at constant pressure, Cp (1004.703 J/kg*K (air)). % Incompressible fluids with energy eqn. only (CONSTANT_DENSITY, INC_IDEAL_GAS). SPECIFIC_HEAT_CP= 1004.703 % --------------------------- VISCOSITY MODEL ---------------------------------% % % Viscosity model (SUTHERLAND, CONSTANT_VISCOSITY). VISCOSITY_MODEL= CONSTANT_VISCOSITY % % Molecular Viscosity that would be constant (1.716E-5 by default) MU_CONSTANT= 0.0002 % --------------------------- THERMAL CONDUCTIVITY MODEL ----------------------% % % Conductivity model (CONSTANT_CONDUCTIVITY, CONSTANT_PRANDTL). CONDUCTIVITY_MODEL= CONSTANT_PRANDTL % % Laminar Prandtl number (0.72 (air), only for CONSTANT_PRANDTL) PRANDTL_LAM= 0.72 % % Turbulent Prandtl number (0.9 (air), only for CONSTANT_PRANDTL) PRANDTL_TURB= 0.90 % ---------------------- REFERENCE VALUE DEFINITION ---------------------------% % % Reference origin for moment computation REF_ORIGIN_MOMENT_X = 0.25 REF_ORIGIN_MOMENT_Y = 0.00 REF_ORIGIN_MOMENT_Z = 0.00 % % Reference length for pitching, rolling, and yawing non-dimensional moment REF_LENGTH= 1.0 % % Reference area for force coefficients (0 implies automatic calculation) REF_AREA= 1.0 % ----------------------- DYNAMIC MESH DEFINITION -----------------------------% % % Type of dynamic surface movement (NONE, DEFORMING, % MOVING_WALL, FLUID_STRUCTURE, FLUID_STRUCTURE_STATIC, % AEROELASTIC, EXTERNAL, EXTERNAL_ROTATION, % AEROELASTIC_RIGID_MOTION) SURFACE_MOVEMENT= MOVING_WALL % % Moving wall boundary marker(s) (NONE = no marker, ignored for RIGID_MOTION) MARKER_MOVING= ( INNERWALL ) % % Coordinates of the motion origin SURFACE_MOTION_ORIGIN= 0.0 0.0 0.0 % % Angular velocity vector (rad/s) about the motion origin SURFACE_ROTATION_RATE = 0.0 0.0 1.0 % % -------------------- BOUNDARY CONDITION DEFINITION --------------------------% % % Navier-Stokes (no-slip), constant heat flux wall marker(s) (NONE = no marker) % Format: ( marker name, constant heat flux (J/m^2), ... ) MARKER_HEATFLUX= ( OUTERWALL, 0.0 ) % % Navier-Stokes (no-slip), isothermal wall marker(s) (NONE = no marker) % Format: ( marker name, constant wall temperature (K), ... ) MARKER_ISOTHERMAL= ( INNERWALL, 313.15 ) % % Periodic boundary marker(s) (NONE = no marker) % Format: ( periodic marker, donor marker, rotation_center_x, rotation_center_y, % rotation_center_z, rotation_angle_x-axis, rotation_angle_y-axis, % rotation_angle_z-axis, translation_x, translation_y, translation_z, ... ) % MARKER_PERIODIC= ( PERIODIC, SHADOW_PERIODIC, 0, 0, 0, 0, 0, 180, 0, 0, 0 ) % % Marker(s) of the surface to be plotted or designed MARKER_PLOTTING= ( INNERWALL, OUTERWALL ) % % Marker(s) of the surface where the functional (Cd, Cl, etc.) will be evaluated MARKER_MONITORING= ( NONE ) % ------------- COMMON PARAMETERS DEFINING THE NUMERICAL METHOD ---------------% % % Numerical method for spatial gradients (GREEN_GAUSS, WEIGHTED_LEAST_SQUARES) NUM_METHOD_GRAD= WEIGHTED_LEAST_SQUARES % % Courant-Friedrichs-Lewy condition of the finest grid CFL_NUMBER= 15.0 % % Adaptive CFL number (NO, YES) CFL_ADAPT= YES % % Parameters of the adaptive CFL number (factor down, factor up, CFL min value, % CFL max value ) CFL_ADAPT_PARAM= ( 0.1, 2.0, 1.0, 1e10 ) % % Number of total iterations ITER= 10000 % ------------------------ LINEAR SOLVER DEFINITION ---------------------------% % % Linear solver for the implicit (or discrete adjoint) formulation (BCGSTAB, FGMRES) LINEAR_SOLVER= FGMRES % % Preconditioner of the Krylov linear solver (JACOBI, LINELET, LU_SGS) LINEAR_SOLVER_PREC= ILU % % Linael solver ILU preconditioner fill-in level (0 by default) LINEAR_SOLVER_ILU_FILL_IN= 0 % % Min error of the linear solver for the implicit formulation LINEAR_SOLVER_ERROR= 1E-14 % % Max number of iterations of the linear solver for the implicit formulation LINEAR_SOLVER_ITER= 25 % -------------------------- MULTIGRID PARAMETERS -----------------------------% % % Multi-Grid Levels (0 = no multi-grid) MGLEVEL= 0 % % Multi-grid cycle (V_CYCLE, W_CYCLE, FULLMG_CYCLE) MGCYCLE= W_CYCLE % % Multi-grid pre-smoothing level MG_PRE_SMOOTH= ( 1, 2, 3, 3 ) % % Multi-grid post-smoothing level MG_POST_SMOOTH= ( 0, 0, 0, 0 ) % % Jacobi implicit smoothing of the correction MG_CORRECTION_SMOOTH= ( 0, 0, 0, 0 ) % % Damping factor for the residual restriction MG_DAMP_RESTRICTION= 0.8 % % Damping factor for the correction prolongation MG_DAMP_PROLONGATION= 0.8 % -------------------- FLOW NUMERICAL METHOD DEFINITION -----------------------% % % Convective numerical method (JST, LAX-FRIEDRICH, CUSP, ROE, AUSM, HLLC, % TURKEL_PREC, MSW) CONV_NUM_METHOD_FLOW= FDS % % Monotonic Upwind Scheme for Conservation Laws (TVD) in the flow equations. % Required for 2nd order upwind schemes (NO, YES) MUSCL_FLOW= YES % % Slope limiter (NONE, VENKATAKRISHNAN, VENKATAKRISHNAN_WANG, % BARTH_JESPERSEN, VAN_ALBADA_EDGE) SLOPE_LIMITER_FLOW= NONE % % Coefficient for the Venkat\u0026#39;s limiter (upwind scheme). A larger values decrease % the extent of limiting, values approaching zero cause % lower-order approximation to the solution (0.05 by default) VENKAT_LIMITER_COEFF= 0.05 % % Time discretization (RUNGE-KUTTA_EXPLICIT, EULER_IMPLICIT, EULER_EXPLICIT) TIME_DISCRE_FLOW= EULER_IMPLICIT % --------------------------- CONVERGENCE PARAMETERS --------------------------% % % Convergence criteria (CAUCHY, RESIDUAL) CONV_CRITERIA= RESIDUAL % % Min value of the residual (log10 of the residual) CONV_RESIDUAL_MINVAL= -14 % % Start convergence criteria at iteration number CONV_STARTITER= 10 % % Number of elements to apply the criteria CONV_CAUCHY_ELEMS= 100 % % Epsilon to control the series convergence CONV_CAUCHY_EPS= 1E-14 % ------------------------- INPUT/OUTPUT INFORMATION --------------------------% % % Mesh input file MESH_FILENAME= VMFL001_rot_conc_cyl_2D.cgns % % Mesh input file format (SU2, CGNS, NETCDF_ASCII) MESH_FORMAT= CGNS % % Mesh output file MESH_OUT_FILENAME= mesh_out.cgns % % Restart flow input file SOLUTION_FILENAME= solution_flow.dat % % Restart adjoint input file SOLUTION_ADJ_FILENAME= solution_adj.dat % % Output file format (PARAVIEW, TECPLOT, STL) TABULAR_FORMAT= CSV % % Output file convergence history (w/o extension) CONV_FILENAME= history % % Output file restart flow RESTART_FILENAME= restart_flow.dat % % Output file restart adjoint RESTART_ADJ_FILENAME= restart_adj.dat % % Output file flow (w/o extension) variables VOLUME_FILENAME= flow % % Output file adjoint (w/o extension) variables VOLUME_ADJ_FILENAME= adjoint % % Output objective function gradient (using continuous adjoint) GRAD_OBJFUNC_FILENAME= of_grad.dat % % Output file surface flow coefficient (w/o extension) SURFACE_FILENAME= surface_flow % % Output file surface adjoint coefficient (w/o extension) SURFACE_ADJ_FILENAME= surface_adjoint % % Writing solution file frequency OUTPUT_WRT_FREQ= 250 % % Writing convergence history frequency SCREEN_WRT_FREQ_INNER= 1 % % Screen output SCREEN_OUTPUT= (INNER_ITER, WALL_TIME, RMS_PRESSURE, RMS_VELOCITY-X, RMS_VELOCITY-Y) Make sure vmfl001.cfg and grid file VMFL001_rot_conc_cyl_2D.cgns are in the same directory, execute the following command (if the tee command is used in Windows, it can be executed in Powershell), run the solution and record the output of the solution process.\nSU2_CFD vmfl001.cfg | tee vmfl001.log 6.2 Post-processing Use ParaView to open the flow.vtu file that solves the output, and the processing process is the same as 5.2.\nComparison of the laminar flow equation with the error of SU2 calculation results.\nRadius [mm] Tangential velocity (theoretical value) [m/s] Tangential velocity (SU2 simulation result) [m/s] Error [%] 20 0.0151 0.0150846 0.10 25 0.0105 0.0105093 0.09 30 0.0072 0.00716493 0.49 35 0.0046 0.00453409 1.43 7. Summary In terms of horizontal comparison, CFX has the highest resolution accuracy; followed by SU2; then Fluent; OpenFOAM accuracy is almost the same as Fluent, but the solution speed is faster; STAR-CCM+ accuracy is the worst.\nRadius [mm] Fluent solution error [%] CFX solution error [%] STAR-CCM+ solution error [%] OpenFOAM solution error [%] SU2 solution error [%] 20 0.48 0.05 1.50 0.66 0.10 25 0.34 0.27 1.61 0.60 0.09 30 1.06 0.24 6.69 1.02 0.49 35 2.23 1.15 3.65 1.57 1.43 ","permalink":"https://andrewmoa.site/post/2021-08-14-solver-verification-comparison/","tags":[{"LinkTitle":"Cfd","RelPermalink":"/tags/cfd/"},{"LinkTitle":"Cfx","RelPermalink":"/tags/cfx/"},{"LinkTitle":"Fluent","RelPermalink":"/tags/fluent/"},{"LinkTitle":"Openfoam","RelPermalink":"/tags/openfoam/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/tags/star-ccm+/"},{"LinkTitle":"Su2","RelPermalink":"/tags/su2/"}],"title":"Solver Verification Comparison - Laminar Flow Between Rotating and Static Concentric Cylinders"},{"categories":[{"LinkTitle":"Zhihu","RelPermalink":"/categories/zhihu/"}],"content":"1. Download source 1.1 Download OpenFOAM source Create a new OpenFOAM directory under the ${HOME}:\ncd ${HOME} mkdir OpenFOAM \u0026amp;\u0026amp; cd OpenFOAM Download the source of OpenFOAM and ThirdParty from GitHub and put it in the ${HOME}/OpenFOAM:\ngit clone https://github.com/OpenFOAM/OpenFOAM-dev --depth=1 git clone https://github.com/OpenFOAM/ThirdParty-dev --depth=1 1.2 Download Torque (PBS) source Here we use Torque from AUR. CentOS, Debian and SUSE operating systems can download ready-made binary packages from OpenPBS on Github. If you use OpenPBS or PBS Pro, please skip Section 3 of this article and refer to other documents to configure PBS.\nDownload Torque(PBS) source from AUR:\ngit clone https://aur.archlinux.org/torque.git cd torque wget http://wpfilebase.s3.amazonaws.com/torque/torque-6.1.1.1.tar.gz 2. Compile and install OpenFOAM 2.1 Setting Environment Variables Edit the ${HOME}/.bashrc file and add the following two lines:\n#OpenFOAM source ${HOME}/OpenFOAM/OpenFOAM-dev/etc/bashrc WM_MPLIB=OPENMPI The WM_MPLIB=OPENMPI at the end means to use the recompiled OpenMPI library.\nUpdate the environment variables:\nsource ${HOME}/.bashrc Verify that the environment variables are correct:\necho $WM_PROJECT_DIR echo $WM_THIRD_PARTY_DIR If the OpenFOAM compilation directory can be output correctly, it means that the environment variables are set correctly.\n2.2 Compile third-party libraries Enter the ThirdParty-dev directory and compile the third-party library:\ncd $WM_THIRD_PARTY_DIR wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.1.tar.gz tar -xvzf openmpi-2.1.1.tar.gz ./Allwmake Since WM_MPLIB=OPENMPI is specified above, you need to manually download the OpenMPI source code file here. Use wget to download the OpenMPI source code package and decompress it. You can add the -jN option after Allwmake to enable multi-core parallel compilation. Here, N should be replaced with the number of cores. Allwmake will automatically compile OpenMPI.\nwhich mpirun \u0026gt;${WM_THIRD_PARTY_DIR}/platforms/linux64Gcc/openmpi-2.1.1/bin/mpirun which mpicc \u0026gt;${WM_THIRD_PARTY_DIR}/platforms/linux64Gcc/openmpi-2.1.1/bin/mpicc 2.3 Compiling ParaView Continue compiling ParaView in the ThirdParty-dev directory:\ncd $WM_THIRD_PARTY_DIR ./makeParaView -mpi wmRefresh After compiling, run wmRefresh to refresh the environment variables.\n2.4 Compiling OpenFOAM Change to the OpenFOAM-dev directory and compile OpenFOAM:\ncd $WM_PROJECT_DIR ./Allwmake -jN OpenFOAM takes a long time to compile, so it is recommended to enable multi-core parallel compilation. Here N should be replaced with the number of cores.\n2.5 Test Execute the following command to copy the cavity folder from the instance file that comes with OpenFOAM to the current path:\nmkdir $FOAM_RUN \u0026amp;\u0026amp; cd $FOAM_RUN cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily . cd pitzDaily Generate mesh:\nblockMesh Calculation:\nsimpleFoam | tee simpleFoam.log Launch ParaView for post-processing:\nparaFoam 2.6 Update Use the git command to update the source code files in the OpenFOAM-dev and ThirdParty-dev directories, and then recompile and install:\ngit pull wcleanPlatform ./Allwmake -jN 3. Compile and install Torque (PBS) 3.1 Install Torque Enter the torque directory:\ncd ${HOME}/OpenFOAM/torque Compile Torque:\nmakepkg Install Torque：\nsudo pacman -U torque-6.1.1.1-2-x86_64.pkg.tar.zst Start the service (pbs_mom.service and pbs_sched.service are in the src/torque-6.1.1.1/contrib/systemd directory of the source code and need to be copied manually):\nsudo systemctl enable pbs_server sudo systemctl enable trqauthd sudo systemctl start pbs_server sudo systemctl start trqauthd sudo cp pbs_mom.service /usr/lib/systemd/system sudo systemctl enable pbs_mom sudo systemctl start pbs_mom sudo cp pbs_sched.service /usr/lib/systemd/system sudo systemctl enable pbs_sched sudo systemctl start pbs_sched 3.2 Server Configuration Refer to archlinux wiki to complete Torque configuration.\nEdit the /etc/hosts file (example) and add the server node and compute node IP addresses:\n192.168.100.101 master #192.168.100.102 cluster01 #192.168.100.103 cluster02 Change the hostname in the /var/spool/torque/server_name file to the server node hostname:\nmaster Execute the following command and select Y to create a new server-side configuration file (only run once, the prompt will overwrite the existing configuration file):\nsudo pbs_server -t create Execute the following command to run the server daemon process:\nsudo trqauthd Initialize default settings:\nsudo qmgr -c \u0026#34;set server acl_hosts = master\u0026#34; sudo qmgr -c \u0026#34;set server scheduling=true\u0026#34; sudo qmgr -c \u0026#34;create queue batch queue_type=execution\u0026#34; sudo qmgr -c \u0026#34;set queue batch started=true\u0026#34; sudo qmgr -c \u0026#34;set queue batch enabled=true\u0026#34; sudo qmgr -c \u0026#34;set queue batch resources_default.nodes=1\u0026#34; sudo qmgr -c \u0026#34;set queue batch resources_default.walltime=3600\u0026#34; sudo qmgr -c \u0026#34;set server default_queue=batch\u0026#34; sudo qmgr -c \u0026#34;set server keep_completed = 60\u0026#34; Verify setting:\nqmgr -c \u0026#39;p s\u0026#39; Edit the file /var/spool/torque/server_priv/nodes and add a compute node (a server can also be a compute node). The format is HOSTNAME np=x gpus=y:\nmaster np=8 gpus=1 3.3 Computional node settings Edit the /var/spool/torque/mom_priv/config file and add the following information:\npbsserver master # Server host name, consistent with nodes logevent 255 # Number of logging events Generate and register the key file to ensure smooth SSH access between hosts:\ncd $HOME/.ssh ssh-keygen -t rsa cp id_rsa.pub authorized_keys 3.4 Restart the server process Run the following command on the server to restart the server process.\nsudo killall -s 9 pbs_server sudo pbs_server 3.5 Start the computional node process Execute the following command to start the compute node process:\nsudo pbs_mom Execute the following command to display the status of the compute node:\npbsnodes -a The following information (example) is output, indicating that the configuration is successful:\nmaster state = free power_state = Running np = 4 ntype = cluster mom_service_port = 15002 mom_manager_port = 15003 gpus = 1 If the node state is displayed as down, it means that the node is unavailable. You can use the following command to force the node to be released:\nsudo qmgr -a -c \u0026#39;set node master state=free\u0026#39; We can write a systemd service script to automatically release the node. The script content is as follows, save it as /usr/lib/systemd/system/pbs_autofree.service file:\n[Unit] Description=Auto free pbs_server Requires=network.target After=network.target trqauthd.service pbs_server.service pbs_mom.service StartLimitIntervalSec=0 [Service] Type=simple Restart=always RestartSec=30 User=root ExecStart=qmgr -a -c \u0026#39;set node master state=free\u0026#39; [Install] WantedBy=multi-user.target Start the service:\nsudo systemctl enable pbs_autofree sudo systemctl start pbs_autofree Turn off services to save resources:\nsudo systemctl disable pbs_autofree sudo systemctl stop pbs_autofree 4. Parallelization of OpenFOAM job submission 4.1 Preparing the case Execute the following command to copy the cavity folder from the instance file that comes with OpenFOAM to the current path:\ncd $FOAM_RUN mkdir cluster \u0026amp;\u0026amp; cd cluster cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily . cd pitzDaily Create a new file called decomposeParDict in the system directory to partition the grid and prepare for parallelization. The file content is as follows, and the number of grid partitions is 2:\nFoamFile { version 2.0; format ascii; class dictionary; note \u0026#34;mesh decomposition control dictionary\u0026#34;; object decomposeParDict; } numberOfSubdomains 2; method scotch; 4.2 Submitting jobs Create a new script file named pitzDaily.pbs to submit parallel jobs. The file content is as follows, using 1 node, 2 CPU cores per node for calculation (nodes=1:ppn=2), 2 threads (mpirun -np 2), which should be consistent with the number of grid partitions in the decomposeParDict file:\n#!/bin/bash #PBS -l nodes=1:ppn=2 # #PBS -N pitzDaily #PBS -A OpenFOAM #PBS -o pitzDaily.out #PBS -e pitzDaily.err # source ${HOME}/OpenFOAM/OpenFOAM-dev/etc/bashrc WM_MPLIB=OPENMPI export RUN_DIR=${FOAM_RUN}/cluster/pitzDaily cd ${RUN_DIR} blockMesh decomposePar mpirun -np 2 simpleFoam -parallel | tee simpleFoam.log reconstructPar tar -Jcvf pitzDaily_results.tar.xz * Submitting a job using qsub:\nchmod 755 pitzDaily.pbs qsub pitzDaily.pbs Check the job status using qstat:\nqstat -a After the calculation is completed, download the pitzDaily_results.tar.xz file and unzip it, and start ParaView for post-processing:\nparaFoam ","permalink":"https://andrewmoa.site/post/2021-08-12-archlinux-openfoam-pbs/","tags":[{"LinkTitle":"Linux","RelPermalink":"/tags/linux/"},{"LinkTitle":"Pbs","RelPermalink":"/tags/pbs/"}],"title":"Compile and install OpenFOAM and parallelize PBS under ArchLinux"},{"categories":[],"content":"","permalink":"https://andrewmoa.site/manifest.json","tags":[],"title":""},{"categories":[],"content":"","permalink":"https://andrewmoa.site/search/_index.de/","tags":[],"title":""},{"categories":[],"content":"","permalink":"https://andrewmoa.site/search/_index.es/","tags":[],"title":""},{"categories":[],"content":"","permalink":"https://andrewmoa.site/search/_index.fr/","tags":[],"title":""},{"categories":[],"content":"","permalink":"https://andrewmoa.site/search/_index.hi/","tags":[],"title":""},{"categories":[],"content":"","permalink":"https://andrewmoa.site/search/_index.jp/","tags":[],"title":""},{"categories":[],"content":"","permalink":"https://andrewmoa.site/search/_index.nl/","tags":[],"title":""},{"categories":[],"content":"","permalink":"https://andrewmoa.site/search/_index.pl/","tags":[],"title":""},{"categories":[],"content":"","permalink":"https://andrewmoa.site/search/_index.ru/","tags":[],"title":""},{"categories":[{"LinkTitle":"Code","RelPermalink":"/zh-cn/categories/code/"}],"content":"之前演示矩阵运算加速的时候本来想尝试一下AMD自家的ROCm，程序编译完运行结果碰到报错：\nrocBLAS error: Cannot read D:\\example\\efficiency_v3\\rocm\\build\\Release\\/rocblas/library/TensileLibrary.dat: No such file or directory for GPU arch : gfx1150 List of available TensileLibrary Files : 查询官网文档，ROCm不支持Radeon 880M核显(AI H 365w处理器)1。除非自己编译rocblas，否则没法运行。\n查了一下网上的教程文章234，没有适配这款核显的，自己开搞吧。\n1. 下载工具和源码 需要用到的工具见下表，没有就去官网下载安装。\n序号 软件/工具包 版本 1 Visual Studio Community 2022 2 AMD HIP SDK for Windows5 6.2.4 3 Python 3.13 4 Git for Windows 2.7.4 5 Strawberry Perl 5.40.0.1 6 CMake 3.30.1 我的机器上装了vcpkg，除了前两个从官网下载安装之外，后面的都是从vcpkg的downloads\\tools目录提取的，将路径添加到环境变量即可。\n# 配置工具包路径 $Env:HIP_PATH=\u0026#34;C:\\Program Files\\AMD\\ROCm\\6.2\u0026#34; $Env:GIT_BIN_PATH = \u0026#34;D:\\vcpkg\\downloads\\tools\\git-2.7.4-windows\\bin\u0026#34; $Env:PERL_PATH = \u0026#34;D:\\vcpkg\\downloads\\tools\\perl\\5.40.0.1\u0026#34; $Env:CMAKE_BIN_PATH = \u0026#34;D:\\vcpkg\\downloads\\tools\\cmake-3.30.1-windows\\cmake-3.30.1-windows-i386\\bin\u0026#34; # 配置系统环境变量 $Env:PATH += \u0026#34;;$Env:HIP_PATH\\bin;$Env:GIT_BIN_PATH;$Env:PERL_PATH\\perl\\site\\bin;$Env:PERL_PATH\\perl\\bin;$Env:PERL_PATH\\c\\bin;$Env:CMAKE_BIN_PATH\u0026#34; 从Github上下载rocBLAS和Tensile的源码，注意版本和本机安装的AMD HIP SDK版本要一致。这里还要下载补丁文件Tensile-fix-fallback-arch-build.patch，确保可以为不支持的显卡编译内容。\n# 创建工作目录 New-Item -Path D:\\rocblas_build -ItemType Directory # 下载rocBLAS git clone -b rocm-6.2.4 https://github.com/ROCm/rocBLAS # 下载Tensile git clone -b rocm-6.2.4 https://github.com/ROCm/Tensile # 下载Tensile补丁 Invoke-WebRequest -Uri https://raw.githubusercontent.com/ulyssesrr/docker-rocm-xtra/f25f12835c1d0a5efa80763b5381accf175b200e/rocm-xtra-rocblas-builder/patches/Tensile-fix-fallback-arch-build.patch -OutFile Tensile-fix-fallback-arch-build.patch 鉴于国内Github访问现状，下载下来的源码最好先打包备份一下，确保出错后可以回滚更改。\n2. 修改源码 进入Tensile文件夹打补丁：\n# 复制patch文件到git文件夹 Copy-Item -Path Tensile-fix-fallback-arch-build.patch -Destination $PWD/Tensile # 进入Tensile源码目录 cd Tensile # 应用patch文件 git apply Tensile-fix-fallback-arch-build.patch 这里应用补丁失败，看来补丁版本太旧了，需要手动修改一下。下面是修改后的Tensile-fix-fallback-arch-build.patch文件，将它另存为并参考上面的命令重新打补丁即可。\ndiff --git a/Tensile/TensileCreateLibrary.py b/Tensile/TensileCreateLibrary.py index ac0486d8..d069949c 100644 --- a/Tensile/TensileCreateLibrary.py +++ b/Tensile/TensileCreateLibrary.py @@ -949,12 +949,11 @@ def generateLogicDataAndSolutions(logicFiles, args): for key, value in masterLibraries.items(): if key != \u0026#34;fallback\u0026#34;: value.insert(deepcopy(masterLibraries[\u0026#34;fallback\u0026#34;])) - for archName in archs: - archName = archName.split(\u0026#39;-\u0026#39;, 1)[0] - if archName not in masterLibraries: - print1(\u0026#34;Using fallback for arch: \u0026#34; + archName) - masterLibraries[archName] = deepcopy(masterLibraries[\u0026#34;fallback\u0026#34;]) - masterLibraries[archName].version = args.version + for architectureName in parseArchitecturesFromArgs(args.Architecture, True): + if architectureName not in masterLibraries: + print(\u0026#34;Using fallback for arch: \u0026#34;+architectureName) + masterLibraries[architectureName] = deepcopy(masterLibraries[\u0026#34;fallback\u0026#34;]) + masterLibraries[architectureName].version = args.version masterLibraries.pop(\u0026#34;fallback\u0026#34;) @@ -1027,6 +1027,17 @@ def WriteClientLibraryFromSolutions(solutionList, libraryWorkingPath, tensileSou return (codeObjectFiles, newLibrary) +def parseArchitecturesFromArgs(architectureArgValue, handleLiteralAllAsList): + if architectureArgValue == \u0026#39;all\u0026#39; and handleLiteralAllAsList: + archs = [gfxName(arch) for arch in globalParameters[\u0026#39;SupportedISA\u0026#39;]] + else: + if \u0026#34;;\u0026#34; in architectureArgValue: + archs = architectureArgValue.split(\u0026#34;;\u0026#34;) # user arg list format + else: + archs = architectureArgValue.split(\u0026#34;_\u0026#34;) # workaround for cmake list in list issue + + return archs + ################################################################################ # Write Master Solution Index CSV ################################################################################ @@ -1167,10 +1178,7 @@ def TensileCreateLibrary(): if not os.path.exists(logicPath): printExit(\u0026#34;LogicPath %s doesn\u0026#39;t exist\u0026#34; % logicPath) - if \u0026#34;;\u0026#34; in arguments[\u0026#34;Architecture\u0026#34;]: - archs = arguments[\u0026#34;Architecture\u0026#34;].split(\u0026#34;;\u0026#34;) # user arg list format - else: - archs = arguments[\u0026#34;Architecture\u0026#34;].split(\u0026#34;_\u0026#34;) # workaround for cmake list in list issue + archs = parseArchitecturesFromArgs(arguments[\u0026#34;Architecture\u0026#34;], False) logicArchs = set() for arch in archs: if arch in architectureMap: 接下来要给Tensile的源码增加AMD Radeon 880M的架构，用hipinfo命令查看本机显卡架构，下面的gcnArchName:所对应的gfx1150才是显卡的架构名称。\n-------------------------------------------------------------------------------- device# 0 Name: AMD Radeon(TM) 880M Graphics ... gcnArchName: gfx1150 ... 然后在Tensile\\Tensile\\Source\\CMakeLists.txt、Tensile\\AsmCaps.py和Tensile\\Common.py3个文件当中增加显卡架构名称和参数，也可以打下面的补丁：\ndiff --git a/Tensile/AsmCaps.py b/Tensile/AsmCaps.py index 783f9af8..27f29f5a 100644 --- a/Tensile/AsmCaps.py +++ b/Tensile/AsmCaps.py @@ -771,6 +771,50 @@ CACHED_ASM_CAPS = \\ \u0026#39;v_mov_b64\u0026#39;: False, \u0026#39;v_pk_fma_f16\u0026#39;: True, \u0026#39;v_pk_fmac_f16\u0026#39;: False}, + (11, 5, 0): {\u0026#39;HasAddLshl\u0026#39;: True, + \u0026#39;HasAtomicAdd\u0026#39;: True, + \u0026#39;HasDirectToLdsDest\u0026#39;: False, + \u0026#39;HasDirectToLdsNoDest\u0026#39;: False, + \u0026#39;HasExplicitCO\u0026#39;: True, + \u0026#39;HasExplicitNC\u0026#39;: True, + \u0026#39;HasGLCModifier\u0026#39;: True, + \u0026#39;HasNTModifier\u0026#39;: False, + \u0026#39;HasLshlOr\u0026#39;: True, + \u0026#39;HasMFMA\u0026#39;: False, + \u0026#39;HasMFMA_b8\u0026#39;: False, + \u0026#39;HasMFMA_bf16_1k\u0026#39;: False, + \u0026#39;HasMFMA_bf16_original\u0026#39;: False, + \u0026#39;HasMFMA_constSrc\u0026#39;: False, + \u0026#39;HasMFMA_f64\u0026#39;: False, + \u0026#39;HasMFMA_f8\u0026#39;: False, + \u0026#39;HasMFMA_i8_908\u0026#39;: False, + \u0026#39;HasMFMA_i8_940\u0026#39;: False, + \u0026#39;HasMFMA_vgpr\u0026#39;: False, + \u0026#39;HasMFMA_xf32\u0026#39;: False, + \u0026#39;HasSMulHi\u0026#39;: True, + \u0026#39;HasWMMA\u0026#39;: True, + \u0026#39;KernargPreloading\u0026#39;: False, + \u0026#39;MaxLgkmcnt\u0026#39;: 15, + \u0026#39;MaxVmcnt\u0026#39;: 63, + \u0026#39;SupportedISA\u0026#39;: True, + \u0026#39;SupportedSource\u0026#39;: True, + \u0026#39;VOP3v_dot4_i32_i8\u0026#39;: False, + \u0026#39;v_dot2_f32_f16\u0026#39;: True, + \u0026#39;v_dot2c_f32_f16\u0026#39;: True, + \u0026#39;v_dot4_i32_i8\u0026#39;: False, + \u0026#39;v_dot4c_i32_i8\u0026#39;: False, + \u0026#39;v_fma_f16\u0026#39;: True, + \u0026#39;v_fma_f32\u0026#39;: True, + \u0026#39;v_fma_f64\u0026#39;: True, + \u0026#39;v_fma_mix_f32\u0026#39;: True, + \u0026#39;v_fmac_f16\u0026#39;: False, + \u0026#39;v_fmac_f32\u0026#39;: True, + \u0026#39;v_mac_f16\u0026#39;: False, + \u0026#39;v_mac_f32\u0026#39;: False, + \u0026#39;v_mad_mix_f32\u0026#39;: False, + \u0026#39;v_mov_b64\u0026#39;: False, + \u0026#39;v_pk_fma_f16\u0026#39;: True, + \u0026#39;v_pk_fmac_f16\u0026#39;: False}, (11, 5, 1): {\u0026#39;HasAddLshl\u0026#39;: True, \u0026#39;HasAtomicAdd\u0026#39;: True, \u0026#39;HasDirectToLdsDest\u0026#39;: False, diff --git a/Tensile/Common.py b/Tensile/Common.py index e440e942..57813169 100644 --- a/Tensile/Common.py +++ b/Tensile/Common.py @@ -229,7 +229,7 @@ globalParameters[\u0026#34;SupportedISA\u0026#34;] = [(8,0,3), (9,4,0), (9,4,1), (9,4,2), (10,1,0), (10,1,1), (10,1,2), (10,3,0), (10,3,1), (11,0,0), (11,0,1), (11,0,2), - (11,5,1)] # assembly kernels writer supports these architectures + (11,5,0), (11,5,1)] # assembly kernels writer supports these architectures globalParameters[\u0026#34;CleanupBuildFiles\u0026#34;] = False # cleanup build files (e.g. kernel assembly) once no longer needed globalParameters[\u0026#34;GenerateManifestAndExit\u0026#34;] = False # Output manifest file with list of expected library objects and exit @@ -308,7 +308,7 @@ architectureMap = { \u0026#39;gfx1010\u0026#39;:\u0026#39;navi10\u0026#39;, \u0026#39;gfx1011\u0026#39;:\u0026#39;navi12\u0026#39;, \u0026#39;gfx1012\u0026#39;:\u0026#39;navi14\u0026#39;, \u0026#39;gfx1030\u0026#39;:\u0026#39;navi21\u0026#39;, \u0026#39;gfx1031\u0026#39;:\u0026#39;navi22\u0026#39;, \u0026#39;gfx1032\u0026#39;:\u0026#39;navi23\u0026#39;, \u0026#39;gfx1034\u0026#39;:\u0026#39;navi24\u0026#39;, \u0026#39;gfx1035\u0026#39;:\u0026#39;rembrandt\u0026#39;, \u0026#39;gfx1100\u0026#39;:\u0026#39;navi31\u0026#39;, \u0026#39;gfx1101\u0026#39;:\u0026#39;navi32\u0026#39;, \u0026#39;gfx1102\u0026#39;:\u0026#39;navi33\u0026#39;, - \u0026#39;gfx1151\u0026#39;:\u0026#39;gfx1151\u0026#39; + \u0026#39;gfx1150\u0026#39;:\u0026#39;gfx1150\u0026#39;, \u0026#39;gfx1151\u0026#39;:\u0026#39;gfx1151\u0026#39; } def getArchitectureName(gfxName): diff --git a/Tensile/Source/CMakeLists.txt b/Tensile/Source/CMakeLists.txt index e973a9ed..8904f4a7 100644 --- a/Tensile/Source/CMakeLists.txt +++ b/Tensile/Source/CMakeLists.txt @@ -51,9 +51,9 @@ if(NOT DEFINED CXX_VERSION_STRING) endif() if(CMAKE_CXX_COMPILER STREQUAL \u0026#34;hipcc\u0026#34;) - set(TENSILE_GPU_ARCHS gfx803 gfx900 gfx906:xnack- gfx908:xnack- gfx90a:xnack- gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1100 gfx1101 gfx1102 CACHE STRING \u0026#34;GPU architectures\u0026#34;) + set(TENSILE_GPU_ARCHS gfx803 gfx900 gfx906:xnack- gfx908:xnack- gfx90a:xnack- gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1100 gfx1101 gfx1102 gfx1150 CACHE STRING \u0026#34;GPU architectures\u0026#34;) else() - set(TENSILE_GPU_ARCHS gfx803 gfx900 gfx906 gfx908 gfx90a gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1100 gfx1101 gfx1102 CACHE STRING \u0026#34;GPU architectures\u0026#34;) + set(TENSILE_GPU_ARCHS gfx803 gfx900 gfx906 gfx908 gfx90a gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1100 gfx1101 gfx1102 gfx1150 CACHE STRING \u0026#34;GPU architectures\u0026#34;) endif() include(CMakeDependentOption) 然后在rocBLAS的源码rocBLAS\\CMakeLists.txt文件中也要增加显卡架构名称，可以打下面的补丁：\ndiff --git a/CMakeLists.txt b/CMakeLists.txt index dd521358..9c074139 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -111,7 +111,7 @@ list( APPEND CMAKE_PREFIX_PATH ${ROCM_PATH}/llvm ${ROCM_PATH} ${ROCM_PATH}/hip / set( TARGET_LIST_ROCM_5.6 \u0026#34;gfx803;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102\u0026#34;) set( TARGET_LIST_ROCM_5.7 \u0026#34;gfx803;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102\u0026#34;) set( TARGET_LIST_ROCM_6.0 \u0026#34;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102\u0026#34;) -set( TARGET_LIST_ROCM_6.2.4 \u0026#34;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102;gfx1151\u0026#34;) +set( TARGET_LIST_ROCM_6.2.4 \u0026#34;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102;gfx1150;gfx1151\u0026#34;) if(ROCM_PLATFORM_VERSION) if(${ROCM_PLATFORM_VERSION} VERSION_LESS 5.7.0) 3. 编译安装 下面的命令初始化x64 Native Tools Command Prompt for VS命令行工具并进入rocBLAS源码目录，执行python命令下载vcpkg安装依赖。边下载边编译，笔记本的话最好插上电并打开性能模式。\n# 载入vc环境 cmd /c \u0026#34;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\u0026#34; # 进入rocBLAS源码目录 cd rocBLAS # 执行以下命令，下载一堆依赖 python rdeps.py 依赖文件有一部分会存在临时文件夹C:\\github里面，编译成功后再删除它。\n接下来执行命令生成库文件，时间有点长：\npython rmake.py -a gfx1150 --no-lazy-library-loading --no-merge-architectures -t $PWD\\..\\Tensile --cmake-arg=\u0026#34;-DROCM_PLATFORM_VERSION=6.2.4\u0026#34; 执行完后会在rocBLAS源码目录的build\\release\\staging路径下面生成rocblas.dll文件。\n最后执行以下命令，将生成的库文件和Tensile的架构文件复制到新的文件夹中：\n# 创建文件夹 New-Item -Path $PWD\\..\\rocblas_dll -ItemType Directory # 复制rocblas.dll文件到新文件夹中 Copy-Item -Path $PWD\\build\\release\\staging\\rocblas.dll -Destination $PWD\\..\\rocblas_dll # 复制Tensile架构文件 Copy-Item -Path $PWD\\build\\release\\Tensile\\library -Destination $PWD\\..\\rocblas_dll\\rocblas\\library -Recurse 生成的dll文件比HIP SDK自带的dll小了很多，如何使用新生成的rocblas有两种办法：\n将rocblas_dll文件夹中打包的dll文件及架构文件和编译出来的程序一起发布，比较简单，不用改动HIP SDK的安装文件；\n将HIP SDK安装路径的bin添加到PATH中，备份原来的rocblas.dll然后复制新生成的rocblas.dll到该路径，复制新生成的gfx1150的TensileLibrary_gfx1150.dat文件和Kernels.so-000-gfx1150.hsaco到HIP SDK安装路径的bin\\rocblas\\library路径下面，将生成的TensileManifest.txt的内容拷贝添加到原来的TensileManifest.txt中。\n最后别忘了删除临时文件夹：C:\\github\n4. 编译测试 接下来编一个矩阵乘法计算程序，测试一下ROCm加速的性能怎么样。主程序main.c和矩阵乘法运算(一)-使用OpenMP加速循环计算 中的2.1一样，blas.c文件如下所示：\n// References: https://github.com/ROCm/rocBLAS-Examples/blob/develop/Languages/C/main.c #include \u0026lt;assert.h\u0026gt; #include \u0026lt;hip/hip_runtime_api.h\u0026gt; #include \u0026lt;math.h\u0026gt; #include \u0026lt;rocblas/rocblas.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #ifndef CHECK_HIP_ERROR #define CHECK_HIP_ERROR(error) \\ if (error != hipSuccess) \\ { \\ fprintf(stderr, \\ \u0026#34;hip error: \u0026#39;%s\u0026#39;(%d) at %s:%d\\n\u0026#34;, \\ hipGetErrorString(error), \\ error, \\ __FILE__, \\ __LINE__); \\ } #endif #ifndef CHECK_ROCBLAS_STATUS #define CHECK_ROCBLAS_STATUS(status) \\ if (status != rocblas_status_success) \\ { \\ fprintf(stderr, \u0026#34;rocBLAS error: \u0026#34;); \\ fprintf(stderr, \\ \u0026#34;rocBLAS error: \u0026#39;%s\u0026#39;(%d) at %s:%d\\n\u0026#34;, \\ rocblas_status_to_string(status), \\ status, \\ __FILE__, \\ __LINE__); \\ } #endif void matrix_multiply_float(int n, float A[], float B[], float C[]) { size_t rows, cols; rows = cols = n; typedef float data_type; rocblas_handle handle; rocblas_status rstatus = rocblas_create_handle(\u0026amp;handle); CHECK_ROCBLAS_STATUS(rstatus); hipStream_t test_stream; rstatus = rocblas_get_stream(handle, \u0026amp;test_stream); CHECK_ROCBLAS_STATUS(rstatus); data_type *da = 0; data_type *db = 0; data_type *dc = 0; CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;da, n * cols * sizeof(data_type))); CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;db, n * cols * sizeof(data_type))); CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;dc, n * cols * sizeof(data_type))); // upload asynchronously from pinned memory rstatus = rocblas_set_matrix_async(rows, cols, sizeof(data_type), A, n, da, n, test_stream); rstatus = rocblas_set_matrix_async(rows, cols, sizeof(data_type), B, n, db, n, test_stream); // scalar arguments will be from host memory rstatus = rocblas_set_pointer_mode(handle, rocblas_pointer_mode_host); CHECK_ROCBLAS_STATUS(rstatus); data_type alpha = 1.0; data_type beta = 2.0; // invoke asynchronous computation rstatus = rocblas_sgemm(handle, rocblas_operation_none, rocblas_operation_none, rows, cols, n, \u0026amp;alpha, da, n, db, n, \u0026amp;beta, dc, n); CHECK_ROCBLAS_STATUS(rstatus); // fetch results asynchronously to pinned memory rstatus = rocblas_get_matrix_async(rows, cols, sizeof(data_type), dc, n, C, n, test_stream); CHECK_ROCBLAS_STATUS(rstatus); // wait on transfer to be finished CHECK_HIP_ERROR(hipStreamSynchronize(test_stream)); CHECK_HIP_ERROR(hipFree(da)); CHECK_HIP_ERROR(hipFree(db)); CHECK_HIP_ERROR(hipFree(dc)); rstatus = rocblas_destroy_handle(handle); CHECK_ROCBLAS_STATUS(rstatus); } void matrix_multiply_double(int n, double A[], double B[], double C[]) { size_t rows, cols; rows = cols = n; typedef double data_type; rocblas_handle handle; rocblas_status rstatus = rocblas_create_handle(\u0026amp;handle); CHECK_ROCBLAS_STATUS(rstatus); hipStream_t test_stream; rstatus = rocblas_get_stream(handle, \u0026amp;test_stream); CHECK_ROCBLAS_STATUS(rstatus); data_type *da = 0; data_type *db = 0; data_type *dc = 0; CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;da, n * cols * sizeof(data_type))); CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;db, n * cols * sizeof(data_type))); CHECK_HIP_ERROR(hipMalloc((void **)\u0026amp;dc, n * cols * sizeof(data_type))); // upload asynchronously from pinned memory rstatus = rocblas_set_matrix_async(rows, cols, sizeof(data_type), A, n, da, n, test_stream); rstatus = rocblas_set_matrix_async(rows, cols, sizeof(data_type), B, n, db, n, test_stream); // scalar arguments will be from host memory rstatus = rocblas_set_pointer_mode(handle, rocblas_pointer_mode_host); CHECK_ROCBLAS_STATUS(rstatus); data_type alpha = 1.0; data_type beta = 2.0; // invoke asynchronous computation rstatus = rocblas_dgemm(handle, rocblas_operation_none, rocblas_operation_none, rows, cols, n, \u0026amp;alpha, da, n, db, n, \u0026amp;beta, dc, n); CHECK_ROCBLAS_STATUS(rstatus); // fetch results asynchronously to pinned memory rstatus = rocblas_get_matrix_async(rows, cols, sizeof(data_type), dc, n, C, n, test_stream); CHECK_ROCBLAS_STATUS(rstatus); // wait on transfer to be finished CHECK_HIP_ERROR(hipStreamSynchronize(test_stream)); CHECK_HIP_ERROR(hipFree(da)); CHECK_HIP_ERROR(hipFree(db)); CHECK_HIP_ERROR(hipFree(dc)); rstatus = rocblas_destroy_handle(handle); CHECK_ROCBLAS_STATUS(rstatus); } cmake配置文件CMakeLists.txt如下，通过变量ROCM_DIR传递HIP安装路径，这里编译用的是-DROCM_DIR=C:/Program Files/AMD/ROCm/6.2。\ncmake_minimum_required(VERSION 3.13) string(REGEX MATCHALL \u0026#34;[a-zA-Z]+\\ |[a-zA-Z]+$\u0026#34; DIRNAME \u0026#34;${CMAKE_CURRENT_SOURCE_DIR}\u0026#34;) project(${DIRNAME} LANGUAGES C) message(STATUS \u0026#34;PROJECT_NAME: ${PROJECT_NAME}\u0026#34;) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) message(STATUS \u0026#34;EXECUTE_FILE_NAME: ${EXECUTE_FILE_NAME}\u0026#34;) # setting aocl directory if(DEFINED ROCM_DIR) message(STATUS \u0026#34;ROCM_DIR is set to: ${ROCM_DIR}\u0026#34;) else() message(FATAL_ERROR \u0026#34;ROCM_DIR is not defined. Please set it to the AOCL installation directory.\u0026#34;) endif() set(CMAKE_C_FLAGS \u0026#34;${CMAKE_C_FLAGS} -D__HIP_PLATFORM_AMD__\u0026#34;) # Enable AOCL BLAS set(ROCM_INCLUDE_DIRS ${ROCM_DIR}/include) set(ROCM_LINK_DIR ${ROCM_DIR}/lib) set(ROCM_LIBS \u0026#34;rocblas;amdhip64\u0026#34;) set(SRC_LIST src/main.c src/blas.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${ROCM_INCLUDE_DIRS} ) target_link_directories(${EXECUTE_FILE_NAME} PRIVATE ${ROCM_LINK_DIR} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ${ROCM_LIBS} ) 在Windows平台下使用vs2022的clang-cl编译，运行效果如下：\nPS D:\\example\\efficiency_v3\\rocm\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/rocm/build/Release/rocm_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.595000s(230.990Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.146214s(939.986Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.147548s(931.486Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.146811s(936.165Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.152827s(899.311Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.143523s(957.610Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.147841s(929.642Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.152921s(898.757Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.173812s(790.734Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.172323s(797.565Gflops) Average Gflops: 831.225, Max Gflops: 957.610 Average Time: 0.197882s, Min Time: 0.143523s PS D:\\example\\efficiency_v3\\rocm\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/rocm/build/Release/rocm_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 1.694739s(81.097Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 1.185994s(115.885Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 1.262083s(108.898Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 1.232974s(111.469Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 1.156926s(118.797Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 1.220483s(112.610Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 1.270335s(108.191Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 1.209922s(113.593Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 1.226809s(112.030Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 1.335011s(102.950Gflops) Average Gflops: 108.552, Max Gflops: 118.797 Average Time: 1.279528s, Min Time: 1.156926s 性能表现和opencl差不多，期待开发更多玩法。\nSystem requirements (Windows) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAMD 680M显卡编译rocBLAS使用SD \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWindows编译AMD ROCm rocblas教程 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWindows下编译rocBLAS \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAMD HIP SDK for Windows \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-06-27-windows-compile-rocblas-rocm-6_2_4/","tags":[{"LinkTitle":"C++","RelPermalink":"/zh-cn/tags/c++/"}],"title":"Windows下编译rocblas-rocm-6.2.4"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/zh-cn/categories/code/"}],"content":"MPI是一种并行计算协议，也是目前高性能计算集群最为常用的接口程序。MPI通过进程间消息进行通讯，可以跨节点调用多核心执行并行计算，这是OpenMP所不具备的。不同平台均有mpi实现，比如Windows下的MS-MPI和Intel MPI，Linux下的OpenMPI和MPICH等。\n1. MPI并行加速循环计算 1.1 C实现 mpi需要对主程序接口进行初始化，建立消息广播机制；同时将需要计算的数组进行分割，广播到不同的进程中。以往这些操作都是openmp或其他并行库内部实现的，程序员不需要关心底层如何实现。但使用mpi需要程序员对每个进程的全局和局部空间进行手动分配，需要控制每个消息的广播，无疑增加了额外的学习成本。\n这里将main.c文件修改如下。\n#include \u0026#34;mpi.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;time.h\u0026gt; #include \u0026lt;math.h\u0026gt; #define MAX(a, b) ((a) \u0026gt; (b) ? (a) : (b)) #define MIN(a, b) ((a) \u0026lt; (b) ? (a) : (b)) extern void matrix_multiply_float(int n, int rank, int size, float local_A[], float B[], float local_C[]); extern void matrix_multiply_double(int n, int rank, int size, double local_A[], double B[], double local_C[]); // Initialize matrix void initialize_matrix_float(int n, float matrix[]) { srand((unsigned)time(NULL)); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { matrix[i * n + j] = rand() / (float)(RAND_MAX); } } } void initialize_matrix_double(int n, double matrix[]) { srand((unsigned)time(NULL)); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { matrix[i * n + j] = rand() / (double)(RAND_MAX); } } } // Execute matrix multiply and print results int mpi_float(int dim, int loop_num, double *ave_gflops, double *max_gflops, double *ave_time, double *min_time) { int rank, size; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;size); // Use volatile to prevent compiler optimizations volatile float *a, *b, *local_c; struct timespec start_ns, end_ns; double cpu_time, total_cpu_time; // 计算每个进程处理的行数 int rows_per_process = dim / size; int remainder = dim % size; if (rank \u0026lt; remainder) { rows_per_process++; } for (int i = 0; i \u0026lt; loop_num; i++) { int check_indices[2]; float check_value; // 主进程分配完整矩阵内存 if (rank == 0) { a = (float *)malloc(dim * dim * sizeof(float)); b = (float *)malloc(dim * dim * sizeof(float)); if (a == NULL || b == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } initialize_matrix_float(dim, a); initialize_matrix_float(dim, b); // 生成校验值 int check_row = rand() % dim; int check_col = rand() % dim; check_value = 0.0; for (int k = 0; k \u0026lt; dim; k++) { check_value += a[check_row * dim + k] * b[k * dim + check_col]; } check_indices[0] = check_row; check_indices[1] = check_col; // 广播校验行列索引和校验值 MPI_Bcast(check_indices, 2, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026amp;check_value, 1, MPI_FLOAT, 0, MPI_COMM_WORLD); } else { a = NULL; b = NULL; MPI_Bcast(check_indices, 2, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026amp;check_value, 1, MPI_FLOAT, 0, MPI_COMM_WORLD); } // 为每个进程分配局部矩阵空间 float *local_A = (float *)malloc(rows_per_process * dim * sizeof(float)); local_c = (float *)calloc(rows_per_process * dim, sizeof(float)); if (local_A == NULL || local_c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } // 分发矩阵 A 的行到各个进程 int *sendcounts = (int *)malloc(size * sizeof(int)); int *displs = (int *)malloc(size * sizeof(int)); int offset = 0; for (int p = 0; p \u0026lt; size; p++) { sendcounts[p] = (p \u0026lt; remainder) ? (rows_per_process * dim) : ((rows_per_process - 1) * dim); displs[p] = offset; offset += sendcounts[p]; } MPI_Scatterv(a, sendcounts, displs, MPI_FLOAT, local_A, rows_per_process * dim, MPI_FLOAT, 0, MPI_COMM_WORLD); // 所有进程都需要完整的矩阵 B float *full_B = (float *)malloc(dim * dim * sizeof(float)); if (full_B == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed for full_B\\n\u0026#34;); return 0; } if (rank == 0) { memcpy(full_B, b, dim * dim * sizeof(float)); } MPI_Bcast(full_B, dim * dim, MPI_FLOAT, 0, MPI_COMM_WORLD); timespec_get(\u0026amp;start_ns, TIME_UTC); matrix_multiply_float(dim, rank, size, local_A, full_B, local_c); timespec_get(\u0026amp;end_ns, TIME_UTC); cpu_time = (end_ns.tv_sec - start_ns.tv_sec) + (end_ns.tv_nsec - start_ns.tv_nsec) / 1e9; // 使用 MPI_Reduce 对所有进程的 cpu_time 求和 MPI_Reduce(\u0026amp;cpu_time, \u0026amp;total_cpu_time, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); if (rank == 0) { // 计算平均值 cpu_time = total_cpu_time / size; } // 将平均值广播给所有进程 MPI_Bcast(\u0026amp;cpu_time, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD); double gflops = 1e-9 * dim * dim * dim * 2 / cpu_time; if (rank == 0) { printf(\u0026#34;%d\\t: %d x %d Matrix multiply wall time : %.6fs(%.3fGflops)\\n\u0026#34;, i + 1, dim, dim, cpu_time, gflops); fflush(stdout); // 强制刷新标准输出缓冲区 } // 收集所有进程的局部结果到主进程 float *c = NULL; if (rank == 0) { c = (float *)malloc(dim * dim * sizeof(float)); if (c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed for c matrix\\n\u0026#34;); return 0; } } MPI_Gatherv(local_c, rows_per_process * dim, MPI_FLOAT, c, sendcounts, displs, MPI_FLOAT, 0, MPI_COMM_WORLD); // 主进程进行校验 if (rank == 0) { int check_row = check_indices[0]; int check_col = check_indices[1]; float result_value = c[check_row * dim + check_col]; if (fabs(result_value - check_value) \u0026gt; 0.001) { fprintf(stderr, \u0026#34;Verification failed at iteration %d: expected %.6f, got %.6f\\n\u0026#34;, i + 1, check_value, result_value); } free(c); } // Free memory if (rank == 0) { free(a); free(b); } free(local_A); free(local_c); free(sendcounts); free(displs); free(full_B); *ave_gflops += gflops; *max_gflops = MAX(*max_gflops, gflops); *ave_time += cpu_time; *min_time = MIN(*min_time, cpu_time); } *ave_gflops /= loop_num; *ave_time /= loop_num; return 1; } int mpi_double(int dim, int loop_num, double *ave_gflops, double *max_gflops, double *ave_time, double *min_time) { int rank, size; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;size); // Use volatile to prevent compiler optimizations volatile double *a, *b, *local_c; struct timespec start_ns, end_ns; double cpu_time, total_cpu_time; // 计算每个进程处理的行数 int rows_per_process = dim / size; int remainder = dim % size; if (rank \u0026lt; remainder) { rows_per_process++; } for (int i = 0; i \u0026lt; loop_num; i++) { int check_indices[2]; double check_value; // 主进程分配完整矩阵内存 if (rank == 0) { a = (double *)malloc(dim * dim * sizeof(double)); b = (double *)malloc(dim * dim * sizeof(double)); if (a == NULL || b == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } initialize_matrix_double(dim, a); initialize_matrix_double(dim, b); // 生成校验值 int check_row = rand() % dim; int check_col = rand() % dim; check_value = 0.0; for (int k = 0; k \u0026lt; dim; k++) { check_value += a[check_row * dim + k] * b[k * dim + check_col]; } check_indices[0] = check_row; check_indices[1] = check_col; // 广播校验行列索引和校验值 MPI_Bcast(check_indices, 2, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026amp;check_value, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD); } else { a = NULL; b = NULL; MPI_Bcast(check_indices, 2, MPI_INT, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026amp;check_value, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD); } // 为每个进程分配局部矩阵空间 double *local_A = (double *)malloc(rows_per_process * dim * sizeof(double)); local_c = (double *)calloc(rows_per_process * dim, sizeof(double)); if (local_A == NULL || local_c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } // 分发矩阵 A 的行到各个进程 int *sendcounts = (int *)malloc(size * sizeof(int)); int *displs = (int *)malloc(size * sizeof(int)); int offset = 0; for (int p = 0; p \u0026lt; size; p++) { sendcounts[p] = (p \u0026lt; remainder) ? (rows_per_process * dim) : ((rows_per_process - 1) * dim); displs[p] = offset; offset += sendcounts[p]; } MPI_Scatterv(a, sendcounts, displs, MPI_DOUBLE, local_A, rows_per_process * dim, MPI_DOUBLE, 0, MPI_COMM_WORLD); // 所有进程都需要完整的矩阵 B double *full_B = (double *)malloc(dim * dim * sizeof(double)); if (full_B == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed for full_B\\n\u0026#34;); return 0; } if (rank == 0) { memcpy(full_B, b, dim * dim * sizeof(double)); } MPI_Bcast(full_B, dim * dim, MPI_DOUBLE, 0, MPI_COMM_WORLD); timespec_get(\u0026amp;start_ns, TIME_UTC); matrix_multiply_double(dim, rank, size, local_A, full_B, local_c); timespec_get(\u0026amp;end_ns, TIME_UTC); cpu_time = (end_ns.tv_sec - start_ns.tv_sec) + (end_ns.tv_nsec - start_ns.tv_nsec) / 1e9; // 使用 MPI_Reduce 对所有进程的 cpu_time 求和 MPI_Reduce(\u0026amp;cpu_time, \u0026amp;total_cpu_time, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); if (rank == 0) { // 计算平均值 cpu_time = total_cpu_time / size; } // 将平均值广播给所有进程 MPI_Bcast(\u0026amp;cpu_time, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD); double gflops = 1e-9 * dim * dim * dim * 2 / cpu_time; if (rank == 0) { printf(\u0026#34;%d\\t: %d x %d Matrix multiply wall time : %.6fs(%.3fGflops)\\n\u0026#34;, i + 1, dim, dim, cpu_time, gflops); fflush(stdout); // 强制刷新标准输出缓冲区 } // 收集所有进程的局部结果到主进程 double *c = NULL; if (rank == 0) { c = (double *)malloc(dim * dim * sizeof(double)); if (c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed for c matrix\\n\u0026#34;); return 0; } } MPI_Gatherv(local_c, rows_per_process * dim, MPI_DOUBLE, c, sendcounts, displs, MPI_DOUBLE, 0, MPI_COMM_WORLD); // 主进程进行校验 if (rank == 0) { int check_row = check_indices[0]; int check_col = check_indices[1]; double result_value = c[check_row * dim + check_col]; if (fabs(result_value - check_value) \u0026gt; 0.000001) { fprintf(stderr, \u0026#34;Verification failed at iteration %d: expected %.6f, got %.6f\\n\u0026#34;, i + 1, check_value, result_value); } free(c); } // Free memory if (rank == 0) { free(a); free(b); } free(local_A); free(local_c); free(sendcounts); free(displs); free(full_B); *ave_gflops += gflops; *max_gflops = MAX(*max_gflops, gflops); *ave_time += cpu_time; *min_time = MIN(*min_time, cpu_time); } *ave_gflops /= loop_num; *ave_time /= loop_num; return 1; } int main(int argc, char *argv[]) { int rank; MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); int n = 10; // Default matrix size exponent int loop_num = 5; // Number of iterations for averaging double ave_gflops = 0.0, max_gflops = 0.0; // Average and maximum Gflops double ave_time = 0.0, min_time = 1e9; // Average and minimum time int use_double = 0; // Default to float precision // Help message if (argc == 1 || (argc == 2 \u0026amp;\u0026amp; (strcmp(argv[1], \u0026#34;-h\u0026#34;) == 0 || strcmp(argv[1], \u0026#34;--help\u0026#34;) == 0))) { if (rank == 0) { printf(\u0026#34;Usage: mpiexec/mpirun [-n/-np $NUM_PROCS] %s [-n SIZE] [-l LOOP_NUM] [-float|-double]\\n\u0026#34;, argv[0]); printf(\u0026#34; -n SIZE Specify matrix size, like 2^SIZE (default: 10)\\n\u0026#34;); printf(\u0026#34; -l LOOP_NUM Specify number of iterations (default: 5)\\n\u0026#34;); printf(\u0026#34; -float Use float precision (default)\\n\u0026#34;); printf(\u0026#34; -double Use double precision\\n\u0026#34;); printf(\u0026#34; -h, --help Show this help message\\n\u0026#34;); } MPI_Finalize(); return 0; } // Parse -n, -l, -float, -double options int double_flag = 0, float_flag = 0; for (int argi = 1; argi \u0026lt; argc; ++argi) { if (strcmp(argv[argi], \u0026#34;-n\u0026#34;) == 0 \u0026amp;\u0026amp; argi + 1 \u0026lt; argc) { n = atoi(argv[argi + 1]); argi++; } else if (strcmp(argv[argi], \u0026#34;-l\u0026#34;) == 0 \u0026amp;\u0026amp; argi + 1 \u0026lt; argc) { loop_num = atoi(argv[argi + 1]); argi++; } else if (strcmp(argv[argi], \u0026#34;-double\u0026#34;) == 0) { double_flag = 1; } else if (strcmp(argv[argi], \u0026#34;-float\u0026#34;) == 0) { float_flag = 1; } } if (double_flag \u0026amp;\u0026amp; float_flag) { if (rank == 0) { fprintf(stderr, \u0026#34;Error: Cannot specify both -double and -float options.\\n\u0026#34;); } MPI_Finalize(); return 1; } use_double = double_flag ? 1 : 0; int dim = (int)pow(2, n); if (use_double) { if (rank == 0) { printf(\u0026#34;Using double precision for matrix multiplication.\\n\u0026#34;); } if (!mpi_double(dim, loop_num, \u0026amp;ave_gflops, \u0026amp;max_gflops, \u0026amp;ave_time, \u0026amp;min_time)) { MPI_Finalize(); return 1; } } else { if (rank == 0) { printf(\u0026#34;Using float precision for matrix multiplication.\\n\u0026#34;); } if (!mpi_float(dim, loop_num, \u0026amp;ave_gflops, \u0026amp;max_gflops, \u0026amp;ave_time, \u0026amp;min_time)) { MPI_Finalize(); return 1; } } if (rank == 0) { printf(\u0026#34;Average Gflops: %.3f, Max Gflops: %.3f\\n\u0026#34;, ave_gflops, max_gflops); printf(\u0026#34;Average Time: %.6fs, Min Time: %.6fs\\n\u0026#34;, ave_time, min_time); } MPI_Finalize(); return 0; } 为了确保合并之后的矩阵没有错误，主程序中增加了随机坐标校验机制。\nmpi.c是循环嵌套矩阵运算的具体实现。\nvoid matrix_multiply_float(int n, int rank, int size, float local_A[], float B[], float local_C[]) { // 计算每个进程处理的行数 int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // 矩阵乘法运算 for (int i = 0; i \u0026lt; rows_per_process; i++) { for (int j = 0; j \u0026lt; n; j++) { for (int k = 0; k \u0026lt; n; k++) { local_C[i * n + j] += local_A[i * n + k] * B[k * n + j]; } } } } // Parallel matrix multiply void matrix_multiply_double(int n, int rank, int size, double local_A[], double B[], double local_C[]) { // 计算每个进程处理的行数 int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // 矩阵乘法运算 for (int i = 0; i \u0026lt; rows_per_process; i++) { for (int j = 0; j \u0026lt; n; j++) { for (int k = 0; k \u0026lt; n; k++) { local_C[i * n + j] += local_A[i * n + k] * B[k * n + j]; } } } } CMakeLists.txt包含mpi头文件，这里编译用的是Windows平台，对应mpi库为ms-mpi。\ncmake_minimum_required(VERSION 3.13) project(mpi LANGUAGES C CXX) set(CMAKE_C_STANDARD 11) set(CMAKE_CXX_STANDARD 20) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable MPI support find_package(MPI REQUIRED) set(SRC_LIST src/main.c src/mpi.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${MPI_CXX_INCLUDE_DIRS} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ${MPI_LIBRARIES} ) 在Windows平台上使用MSYS2的ucrt64编译，输出Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\mpi\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/c/mpi/build/mpi_gnu_gnu_15.1.0.exe -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.235279s(9.127Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.245679s(8.741Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.252880s(8.492Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.253803s(8.461Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.235253s(9.128Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.261678s(8.207Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.261258s(8.220Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.256560s(8.370Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.254379s(8.442Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.271509s(7.909Gflops) Average Gflops: 8.510, Max Gflops: 9.128 Average Time: 0.252828s, Min Time: 0.235253s PS D:\\example\\efficiency_v3\\c\\mpi\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/c/mpi/build/mpi_gnu_gnu_15.1.0.exe -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.317128s(6.772Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.316111s(6.793Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.320794s(6.694Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.334887s(6.413Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.328439s(6.538Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.326553s(6.576Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.321168s(6.686Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.322008s(6.669Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.330364s(6.500Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.314038s(6.838Gflops) Average Gflops: 6.648, Max Gflops: 6.838 Average Time: 0.323149s, Min Time: 0.314038s 计算机器CPU为AMD AI 9 365w，10核心20线程，这里使用了超线程，整体性能和openmp其实差不多。可以通过命令行控制计算进程数量。由于每个进程都需要分配局部存储空间，计算进程越多内存开销越大。\n1.2 fortran实现 fortran也提供了mpi实现，ms-mpi和openmpi都提供了fortran的接口。限于篇幅，这里主程序源码还是采用和上一节相同的main.c，矩阵乘法部分通过fortran实现，通过fortran和C混合编程的方式实现程序功能。\n矩阵乘法计算相关函数在fortran源文件mpi2c.f90里定：\nsubroutine matrix_multiply_float(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_float\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_float), intent(in) :: local_A(*) real(c_float), intent(in) :: B(*) real(c_float), intent(out) :: local_C(*) real(c_float), allocatable :: local_A_2d(:, :) real(c_float), allocatable :: B_2d(:, :) real(c_float), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j, k remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! 将一维数组转换为二维数组 do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! 初始化结果矩阵 local_C_2d = 0.0_c_float ! 矩阵乘法计算 do i = 1, rows_per_process do j = 1, n do k = 1, n local_C_2d(i, j) = local_C_2d(i, j) + local_A_2d(i, k) * B_2d(k, j) end do end do end do ! 将二维数组转换为一维数组 do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_double\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_double), intent(in) :: local_A(*) real(c_double), intent(in) :: B(*) real(c_double), intent(out) :: local_C(*) real(c_double), allocatable :: local_A_2d(:, :) real(c_double), allocatable :: B_2d(:, :) real(c_double), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j, k remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! 将一维数组转换为二维数组 do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! 初始化结果矩阵 local_C_2d = 0.0_c_double ! 矩阵乘法计算 do i = 1, rows_per_process do j = 1, n do k = 1, n local_C_2d(i, j) = local_C_2d(i, j) + local_A_2d(i, k) * B_2d(k, j) end do end do end do ! 将二维数组转换为一维数组 do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_double 由于C程序接口将二维矩阵存储在一维数组里，为了使fortran程序能正常读取存储在一维数组里的矩阵信息，需要定义一维数组到二维数组的转换，开辟新的存储空间。为了方便接口复用，这里把数组转换的相关内容定义到了计算函数中，会带来额外的性能开销。\nCMakeLists.txt没太大区别，包含C的mpi头文件，激活了fortran语言支持。\ncmake_minimum_required(VERSION 3.13) project(mpi-fortran LANGUAGES C Fortran) set(CMAKE_C_STANDARD 11) set(CMAKE_Fortran_STANDARD 2008) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_Fortran_COMPILER_FRONTEND_VARIANT}_${CMAKE_Fortran_COMPILER_ID}_${CMAKE_Fortran_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable MPI support find_package(MPI REQUIRED) set(SRC_LIST src/main.c src/mpi2c.f90 ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${MPI_CXX_INCLUDE_DIRS} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ${MPI_LIBRARIES} ) 在Windows下使用MSYS2的ucrt64工具链编译，Release程序运行效果如下：\nPS D:\\example\\efficiency_v3\\fortran\\mpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/mpi-fortran/build/mpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.078928s(27.208Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.089289s(24.051Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.082848s(25.921Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.080060s(26.823Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.084739s(25.342Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.083975s(25.573Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.085056s(25.248Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.083494s(25.720Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.082314s(26.089Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.082700s(25.967Gflops) Average Gflops: 25.794, Max Gflops: 27.208 Average Time: 0.083340s, Min Time: 0.078928s PS D:\\example\\efficiency_v3\\fortran\\mpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/mpi-fortran/build/mpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.126719s(16.947Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.127700s(16.817Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.126309s(17.002Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.124125s(17.301Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.121543s(17.668Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.129106s(16.634Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.129957s(16.525Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.128702s(16.686Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.127478s(16.846Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.125188s(17.154Gflops) Average Gflops: 16.958, Max Gflops: 17.668 Average Time: 0.126683s, Min Time: 0.121543s 以上是在和1.1相同的处理器上运行的结果，fortran数组采用了和C不同的列优先存储方式，缓存命中率提高，有利于提高矩阵运算性能。\n2. MPI加速分块矩阵运算 2.1 C实现 上面的结果揭示了缓存性能对矩阵运算存在较大的影响，为了提高缓存命中率，这里尝试使用分块矩阵，看是否能提升C语言程序的计算性能。\n主程序main.c和前面一样保持不变，这里只改动mpi.c，在原来的循环嵌套计算的基础上增加分块计算功能：\n#include \u0026lt;math.h\u0026gt; // 分块大小，可根据缓存大小调整 #define BLOCK_SIZE 8 // Parallel matrix multiply void matrix_multiply_float(int n, int rank, int size, float local_A[], float B[], float local_C[]) { // 计算每个进程处理的行数 int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // 初始化 local_C 为 0 for (int i = 0; i \u0026lt; rows_per_process; i++) { for (int j = 0; j \u0026lt; n; j++) { local_C[i * n + j] = 0.0f; } } // 分块矩阵乘法 for (int bi = 0; bi \u0026lt; rows_per_process; bi += BLOCK_SIZE) { for (int bj = 0; bj \u0026lt; n; bj += BLOCK_SIZE) { for (int bk = 0; bk \u0026lt; n; bk += BLOCK_SIZE) { // 块内计算 int end_i = fmin(bi + BLOCK_SIZE, rows_per_process); int end_j = fmin(bj + BLOCK_SIZE, n); int end_k = fmin(bk + BLOCK_SIZE, n); for (int i = bi; i \u0026lt; end_i; i++) { for (int j = bj; j \u0026lt; end_j; j++) { float sum = local_C[i * n + j]; for (int k = bk; k \u0026lt; end_k; k++) { sum += local_A[i * n + k] * B[k * n + j]; } local_C[i * n + j] = sum; } } } } } } // Parallel matrix multiply void matrix_multiply_double(int n, int rank, int size, double local_A[], double B[], double local_C[]) { // 计算每个进程处理的行数 int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // 初始化 local_C 为 0 for (int i = 0; i \u0026lt; rows_per_process; i++) { for (int j = 0; j \u0026lt; n; j++) { local_C[i * n + j] = 0.0; } } // 分块矩阵乘法 for (int bi = 0; bi \u0026lt; rows_per_process; bi += BLOCK_SIZE) { for (int bj = 0; bj \u0026lt; n; bj += BLOCK_SIZE) { for (int bk = 0; bk \u0026lt; n; bk += BLOCK_SIZE) { // 块内计算 int end_i = fmin(bi + BLOCK_SIZE, rows_per_process); int end_j = fmin(bj + BLOCK_SIZE, n); int end_k = fmin(bk + BLOCK_SIZE, n); for (int i = bi; i \u0026lt; end_i; i++) { for (int j = bj; j \u0026lt; end_j; j++) { double sum = local_C[i * n + j]; for (int k = bk; k \u0026lt; end_k; k++) { sum += local_A[i * n + k] * B[k * n + j]; } local_C[i * n + j] = sum; } } } } } } CMakeLists.txt文件和1.1中的相同。\n在Windows下使用MSYS2的ucrt64工具链编译，程序执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\blockmpi\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/c/blockmpi/build/blockmpi_gnu_gnu_15.1.0.exe -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.039390s(54.519Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.041973s(51.164Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.044693s(48.050Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.042822s(50.149Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.044406s(48.361Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.041931s(51.215Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.042498s(50.531Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.041639s(51.573Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.042505s(50.523Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.043471s(49.401Gflops) Average Gflops: 50.548, Max Gflops: 54.519 Average Time: 0.042533s, Min Time: 0.039390s PS D:\\example\\efficiency_v3\\c\\blockmpi\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/c/blockmpi/build/blockmpi_gnu_gnu_15.1.0.exe -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.041856s(51.307Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.045559s(47.136Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.044370s(48.400Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.043212s(49.697Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.041711s(51.485Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.043633s(49.217Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.042332s(50.730Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.041305s(51.991Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.041940s(51.203Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.040975s(52.410Gflops) Average Gflops: 50.358, Max Gflops: 52.410 Average Time: 0.042689s, Min Time: 0.040975s 这里分块大小设置为8，是根据大量测试得出的最优结果。在其他平台上实施，需要针对该平台硬件进行测试，才能确定最合适的分块大小。\n2.2 fortran实现 接下来用fortran实现分块矩阵mpi加速功能，看看性能是否有提升。\nmain.c和CMakeLists.txt基本和1.2相同，只改动mpi2c.f90文件，增加分块内容：\nsubroutine matrix_multiply_float(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_float\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_float), intent(in) :: local_A(*) real(c_float), intent(in) :: B(*) real(c_float), intent(out) :: local_C(*) real(c_float), allocatable :: local_A_2d(:, :) real(c_float), allocatable :: B_2d(:, :) real(c_float), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j, k, ii, jj, kk integer(c_int), parameter :: block_size = 8 ! 分块大小，可根据实际情况调整 remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! 将一维数组转换为二维数组 do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! 初始化结果矩阵 local_C_2d = 0.0_c_float ! 分块矩阵乘法计算 do ii = 1, rows_per_process, block_size do kk = 1, n, block_size do jj = 1, n, block_size do i = ii, min(ii + block_size - 1, rows_per_process) do k = kk, min(kk + block_size - 1, n) do j = jj, min(jj + block_size - 1, n) local_C_2d(i, j) = local_C_2d(i, j) + local_A_2d(i, k) * B_2d(k, j) end do end do end do end do end do end do ! 将二维数组转换为一维数组 do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_double\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_double), intent(in) :: local_A(*) real(c_double), intent(in) :: B(*) real(c_double), intent(out) :: local_C(*) real(c_double), allocatable :: local_A_2d(:, :) real(c_double), allocatable :: B_2d(:, :) real(c_double), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j, k, ii, jj, kk integer(c_int), parameter :: block_size = 8 ! 分块大小，可根据实际情况调整 remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! 将一维数组转换为二维数组 do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! 初始化结果矩阵 local_C_2d = 0.0_c_double ! 分块矩阵乘法计算 do ii = 1, rows_per_process, block_size do kk = 1, n, block_size do jj = 1, n, block_size do i = ii, min(ii + block_size - 1, rows_per_process) do k = kk, min(kk + block_size - 1, n) do j = jj, min(jj + block_size - 1, n) local_C_2d(i, j) = local_C_2d(i, j) + local_A_2d(i, k) * B_2d(k, j) end do end do end do end do end do end do ! 将二维数组转换为一维数组 do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_double Windows下使用MSYS2的ucrt64工具链编译输出Release程序，执行效果如下：\nPS D:\\example\\efficiency_v3\\fortran\\blockmpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/blockmpi-fortran/build/blockmpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.050200s(42.779Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.050326s(42.671Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.049548s(43.341Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.050283s(42.708Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.048627s(44.163Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.052763s(40.700Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.056050s(38.314Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.055364s(38.789Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.051921s(41.361Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.053460s(40.170Gflops) Average Gflops: 41.500, Max Gflops: 44.163 Average Time: 0.051854s, Min Time: 0.048627s PS D:\\example\\efficiency_v3\\fortran\\blockmpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/blockmpi-fortran/build/blockmpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.078406s(27.389Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.066016s(32.530Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.067229s(31.943Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.067948s(31.605Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.063610s(33.760Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.063506s(33.816Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.063981s(33.564Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.063603s(33.764Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.068714s(31.252Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.063268s(33.943Gflops) Average Gflops: 32.357, Max Gflops: 33.943 Average Time: 0.066628s, Min Time: 0.063268s 看上去比C实现的要慢一点，但实际上fortran计算函数内部增加了数组转换、开辟存储空间等功能增加了额外的运行时间。除去这部分开销，实际计算性能应该会更好一点。\n3. MPI并行配合底层优化加速 3.1 fortran编译器优化 虽然前面的分块矩阵已经有了很大的提升，但和之前提到的编译器底层优化和专业数学库比起来，这点性能还是有点不够。现在尝试将mpi配合fortran编译器的底层优化，看能否在性能上进一步提升。\nmain.c和CMakeLists.txt基本和1.2相同，这里改动mpi2c.f90文件，使用fortran内置函数matmul实现矩阵运算功能：\nsubroutine matrix_multiply_float(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_float\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_float), intent(in) :: local_A(*) real(c_float), intent(in) :: B(*) real(c_float), intent(out) :: local_C(*) real(c_float), allocatable :: local_A_2d(:, :) real(c_float), allocatable :: B_2d(:, :) real(c_float), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! 将一维数组转换为二维数组 do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! 使用 matmul 进行矩阵乘法计算 local_C_2d = matmul(local_A_2d, B_2d) ! 将二维数组转换为一维数组 do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, rank, size, local_A, B, local_C) bind(C, name=\u0026#34;matrix_multiply_double\u0026#34;) use, intrinsic :: iso_c_binding implicit none integer(c_int), value, intent(in) :: n, rank, size real(c_double), intent(in) :: local_A(*) real(c_double), intent(in) :: B(*) real(c_double), intent(out) :: local_C(*) real(c_double), allocatable :: local_A_2d(:, :) real(c_double), allocatable :: B_2d(:, :) real(c_double), allocatable :: local_C_2d(:, :) integer(c_int) :: rows_per_process integer(c_int) :: remainder integer(c_int) :: i, j remainder = mod(n, size) rows_per_process = n / size if (rank \u0026lt; remainder) then rows_per_process = rows_per_process + 1 end if allocate(local_A_2d(rows_per_process, n)) allocate(B_2d(n, n)) allocate(local_C_2d(rows_per_process, n)) ! 将一维数组转换为二维数组 do i = 1, rows_per_process do j = 1, n local_A_2d(i, j) = local_A((i - 1) * n + j) end do end do do i = 1, n do j = 1, n B_2d(i, j) = B((i - 1) * n + j) end do end do ! 使用 matmul 进行矩阵乘法计算 local_C_2d = matmul(local_A_2d, B_2d) ! 将二维数组转换为一维数组 do i = 1, rows_per_process do j = 1, n local_C((i - 1) * n + j) = local_C_2d(i, j) end do end do deallocate(local_A_2d, B_2d, local_C_2d) end subroutine matrix_multiply_double 使用MSYS2的ucrt64工具编译，在Windows下执行Release程序，效果如下：\nPS D:\\example\\efficiency_v3\\fortran\\matmul-mpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/matmul-mpi-fortran/build/matmul-mpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.797250s(172.391Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.551850s(249.051Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.560781s(245.085Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.555596s(247.372Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.564455s(243.490Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.552102s(248.938Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.588235s(233.646Gflops) Verification failed at iteration 7: expected 1032.420410, got 1032.418823 8 : 4096 x 4096 Matrix multiply wall time : 0.582861s(235.801Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.604666s(227.297Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.615205s(223.404Gflops) Average Gflops: 232.647, Max Gflops: 249.051 Average Time: 0.597300s, Min Time: 0.551850s PS D:\\example\\efficiency_v3\\fortran\\matmul-mpi-fortran\\build\u0026gt; mpiexec -n 20 D:/example/efficiency_v3/fortran/matmul-mpi-fortran/build/matmul-mpi-fortran_gnu_gnu_15.1.0.exe -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 1.493319s(92.036Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 1.031578s(133.232Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 1.074232s(127.942Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 1.051967s(130.649Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 1.135295s(121.060Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 1.071376s(128.283Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 1.059316s(129.743Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 1.061684s(129.454Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 1.042263s(131.866Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 1.054022s(130.395Gflops) Average Gflops: 125.466, Max Gflops: 133.232 Average Time: 1.107505s, Min Time: 1.031578s 相比前面的矩阵分块算法有很大的提升。注意到单精度浮点数运算时出现一个校验错误，主程序中单精度浮点数校验误差是0.001，这里数值误差增加到了0.0016，超过了前面设定的误差值。在矩阵规模增大时浮点数运算的误差会累积增加，比较合理的解决办法是采用更高精度的浮点数运算或者加大校验误差，前者意味着计算量增加性能下降时间变长，后者则可能导致计算结果可信度降低，究竟该接受哪种代价需要区分计算场合来决定。\n3.2 BLAS加速 之前演示的blas库底层都通过openmp实现并行加速，但设计上openmp库只适合单节点多核心加速，不适合超算上的多节点多核心的运行环境。有没有一种办法底层可以利用blas库搭配mpi实现跨节点多核心加速呢？下面演示一下实现方法。\n首先主程序文件main.c和前面一样直接复用，改动mpi.c增加调用openblas函数实现矩阵运算功能。由于openblas默认使用openmp加速，需要手动设置openblas为单线程运行。\n#include \u0026lt;cblas.h\u0026gt; #include \u0026lt;openblas_config.h\u0026gt; #include \u0026lt;mpi.h\u0026gt; void matrix_multiply_float(int n, int rank, int size, float local_A[], float B[], float local_C[]) { // 设置 OpenBLAS 使用单线程 openblas_set_num_threads(1); // 计算每个进程处理的行数 int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // 使用 OpenBLAS 进行局部矩阵乘法 // C = α * A * B + β * C，这里 α = 1.0，β = 1.0 cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, rows_per_process, n, n, 1.0, local_A, n, B, n, 1.0, local_C, n); } void matrix_multiply_double(int n, int rank, int size, double local_A[], double B[], double local_C[]) { // 设置 OpenBLAS 使用单线程 openblas_set_num_threads(1); // 计算每个进程处理的行数 int rows_per_process = n / size; int remainder = n % size; if (rank \u0026lt; remainder) { rows_per_process++; } // 使用 OpenBLAS 进行局部矩阵乘法 // C = α * A * B + β * C，这里 α = 1.0，β = 1.0 cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, rows_per_process, n, n, 1.0, local_A, n, B, n, 1.0, local_C, n); } CMakeLists.txt中还需要链接到openblas和openmp，否则无法通过编译。\ncmake_minimum_required(VERSION 3.13) project(openblas-mpi LANGUAGES C CXX) set(CMAKE_C_STANDARD 11) set(CMAKE_CXX_STANDARD 20) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable MPI support find_package(MPI REQUIRED) # Find OpenBLAS find_package(OpenMP REQUIRED) find_package(OpenBLAS CONFIG REQUIRED) set(SRC_LIST src/main.c src/mpi.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${MPI_CXX_INCLUDE_DIRS} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE OpenBLAS::OpenBLAS OpenMP::OpenMP_C ${MPI_LIBRARIES} ) 在Windows下编译，使用MSYS2的ucrt64工具链和其自带的openblas，编译输出Release程序，运行效果如下：\nPS D:\\example\\efficiency_v3\\c\\openblas-mpi\\build\u0026gt; mpiexec -n 10 D:/example/efficiency_v3/c/openblas-mpi/build/openblas-mpi_gnu_gnu_15.1.0.exe -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.162794s(844.252Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.178354s(770.595Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.201812s(681.023Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.191400s(718.073Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.186770s(735.872Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.200100s(686.850Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.182761s(752.013Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.181107s(758.882Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.182459s(753.258Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.190692s(720.739Gflops) Average Gflops: 742.156, Max Gflops: 844.252 Average Time: 0.185825s, Min Time: 0.162794s PS D:\\example\\efficiency_v3\\c\\openblas-mpi\\build\u0026gt; mpiexec -n 10 D:/example/efficiency_v3/c/openblas-mpi/build/openblas-mpi_gnu_gnu_15.1.0.exe -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.267384s(514.013Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.361675s(380.007Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.338016s(406.605Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.331837s(414.176Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.352789s(389.578Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.346735s(396.381Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.343613s(399.982Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.349749s(392.965Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.340508s(403.629Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.347642s(395.347Gflops) Average Gflops: 409.268, Max Gflops: 514.013 Average Time: 0.337995s, Min Time: 0.267384s 可以看到计算速度相比于前面的方法已经有了很大的提升。这里没有用到超线程，但是这个结果已经和单独使用openblas加速运算的效果十分接近了，双精度浮点运算性能甚至要更好一些。测试过程中使用超线程没能达成更好的效果，而且有很大几率会导致程序崩溃，可能是openmp和mpi混用所导致的。\n4. 总结 在单机上运行，编写mpi程序需要单独控制内存和消息广播，编码和调试更加麻烦，而且会带来额外的性能开销，跟openmp相比没有性能上的优势，反而openmp编码上更有优势。\nmpi存在的意义在于超算集群等机器上的跨节点多核调用，使用mpi开发的高性能计算程序尤其适合在这种场合下通过大集群多节点的调用进行加速。\n对高性能计算程序而言，单独使用mpi对循环加速其实没有多大意义，应配合编译器和专业数学库的优化实现计算加速。\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-06-25-matrix_multiplication-3/","tags":[{"LinkTitle":"C++","RelPermalink":"/zh-cn/tags/c++/"},{"LinkTitle":"Fortran","RelPermalink":"/zh-cn/tags/fortran/"}],"title":"矩阵乘法运算(三)-使用MPI并行加速"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/zh-cn/categories/code/"}],"content":"BLAS最初是采用fortran开发的线性代数库，后来移植到C/C++上，作为现代高性能计算的核心组件，已经形成了一套标准。有开源的实现如Netlib BLAS、GotoBLAS及其后继者OpenBLAS，商业上各个厂商针对自家平台都有相应的实现，比如Intel的MKL、NVIDIA的CUDA、AMD的AOCL和ROCm。其中有针对CPU平台进行优化的，也有采用GPU并行加速的。本文通过使用不同BLAS库实现矩阵运算，分析不同实现间的性能差异。\n1. CPU并行加速BLAS库 1.1 Intel MKL main.c文件同矩阵乘法运算(一)-使用OpenMP加速循环计算 中的2.1，blas.c引入mkl的blas库，并使用gemm函数执行矩阵乘法运算。\n#include \u0026lt;mkl_cblas.h\u0026gt; void matrix_multiply_float(int n, float A[], float B[], float C[]) { cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, n, n, n, 1.0, A, n, B, n, 0.0, C, n); } void matrix_multiply_double(int n, double A[], double B[], double C[]) { cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, n, n, n, 1.0, A, n, B, n, 0.0, C, n); } CMakeLists.txt包含了mkl和openmp库文件，mkl底层默认使用openmp进行并行化，因此要链接到openmp库。\ncmake_minimum_required(VERSION 3.13) project(mkl LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) set(MKL_LINK static) # Enable OpenMP find_package(OpenMP REQUIRED) # Enable MKL find_package(MKL CONFIG REQUIRED) set(SRC_LIST src/main.c src/blas.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_compile_options(${EXECUTE_FILE_NAME} PUBLIC $\u0026lt;TARGET_PROPERTY:MKL::MKL,INTERFACE_COMPILE_OPTIONS\u0026gt; ) target_include_directories(${EXECUTE_FILE_NAME} PUBLIC $\u0026lt;TARGET_PROPERTY:MKL::MKL,INTERFACE_INCLUDE_DIRECTORIES\u0026gt; ) target_link_libraries(${EXECUTE_FILE_NAME} PUBLIC OpenMP::OpenMP_C $\u0026lt;LINK_ONLY:MKL::MKL\u0026gt; ) 编译机器是AMD笔记本，处理器是AI 9 365w，在Windows下使用vs2022的clang-cl编译并运行，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\mkl\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/mkl/build/Release/mkl_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.218935s(627.761Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.211711s(649.183Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.215178s(638.722Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.223452s(615.072Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.202687s(678.085Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.203175s(676.455Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.225790s(608.702Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.204435s(672.287Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.217666s(631.421Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.217374s(632.270Gflops) Average Gflops: 642.996, Max Gflops: 678.085 Average Time: 0.214040s, Min Time: 0.202687s PS D:\\example\\efficiency_v3\\c\\mkl\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/mkl/build/Release/mkl_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.400238s(343.393Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.365257s(376.280Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.375613s(365.906Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.353108s(389.226Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.380444s(361.260Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.381736s(360.036Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.392378s(350.272Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.382949s(358.897Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.401440s(342.365Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.413794s(332.143Gflops) Average Gflops: 357.978, Max Gflops: 389.226 Average Time: 0.384696s, Min Time: 0.353108s 想要在AMD机器上正常运行Intel mkl程序，建议在环境变量中加入MKL_DEBUG_CPU_TYPE=5。\n在Intel工作站(Xeon Gold 6226R)上运行效果如下：\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./mkl_msvc_clang_19.1.5.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 4.505398s(1952.345Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 4.513220s(1948.962Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 4.472770s(1966.587Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 4.478043s(1964.272Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 4.487709s(1960.041Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 4.467270s(1969.009Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 4.399587s(1999.299Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 4.511694s(1949.621Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 4.477332s(1964.584Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 4.459314s(1972.521Gflops) Average Gflops: 1964.724, Max Gflops: 1999.299 Average Time: 4.477234s, Min Time: 4.399587s PS E:\\example\\efficiency_v3\\run\u0026gt; ./mkl_msvc_clang_19.1.5.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 10.416302s(844.455Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 10.291459s(854.698Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 10.229627s(859.865Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 10.319844s(852.347Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 10.277702s(855.842Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 10.134144s(867.966Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 10.602970s(829.588Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 10.421812s(844.008Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 10.096355s(871.215Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 10.168106s(865.067Gflops) Average Gflops: 854.505, Max Gflops: 871.215 Average Time: 10.295832s, Min Time: 10.096355s 对于blas来说，矩阵规模越小加速效果越不明显，但矩阵规模也不是越大越好，总有一个上限值。矩阵规模多少合适，取决于CPU和缓存的性能，不同平台适合的矩阵运算规模不一样。\n1.2 OpenBLAS 考虑到Intel的库在AMD平台上执行效率可能会差一点，这里用openblas替换mkl试试。这里只改动blas.c所引用的头文件，其余保持不变：\n#include \u0026lt;cblas.h\u0026gt; void matrix_multiply_float(int n, float A[], float B[], float C[]) { cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, n, n, n, 1.0, A, n, B, n, 0.0, C, n); } void matrix_multiply_double(int n, double A[], double B[], double C[]) { cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, n, n, n, 1.0, A, n, B, n, 0.0, C, n); } CMakeLists.txt包含openblas和openmp，没错，openblas底层也是通过openmp实现并行化的。\ncmake_minimum_required(VERSION 3.13) project(openblas LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable OpenBLAS and OpenMP find_package(OpenMP REQUIRED) find_package(OpenBLAS CONFIG REQUIRED) set(SRC_LIST src/main.c src/blas.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE OpenBLAS::OpenBLAS OpenMP::OpenMP_C ) 在AMD的机器上编译运行，Windows下使用MSYS2的ucrt64工具链编译输出Release程序，执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\openblas\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openblas/build/openblas_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.155257s(885.234Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.154143s(891.633Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.159396s(862.251Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.159194s(863.341Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.154769s(888.027Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.160298s(857.398Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.158431s(867.502Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.153623s(894.649Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.164457s(835.712Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.145844s(942.366Gflops) Average Gflops: 878.811, Max Gflops: 942.366 Average Time: 0.156541s, Min Time: 0.145844s PS D:\\example\\efficiency_v3\\c\\openblas\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openblas/build/openblas_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.342961s(400.743Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.366820s(374.677Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.331864s(414.143Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.324773s(423.185Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.370822s(370.634Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.387845s(354.365Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.353893s(388.363Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.344588s(398.851Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.359606s(382.193Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.379676s(361.990Gflops) Average Gflops: 386.914, Max Gflops: 423.185 Average Time: 0.356285s, Min Time: 0.324773s 在AMD平台上openblas运行性能确实比Intel mkl提升了不少。\n在Intel工作站上运行效果如下：\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./openblas_gnu_gnu_15.1.0.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 5.334670s(1648.854Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 5.288424s(1663.273Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 5.288376s(1663.288Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 5.280766s(1665.685Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 5.203108s(1690.546Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 5.283861s(1664.709Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 5.313183s(1655.522Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 5.247752s(1676.164Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 5.229815s(1681.913Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 5.205896s(1689.641Gflops) Average Gflops: 1669.960, Max Gflops: 1690.546 Average Time: 5.267585s, Min Time: 5.203108s PS E:\\example\\efficiency_v3\\run\u0026gt; ./openblas_gnu_gnu_15.1.0.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 12.076276s(728.378Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 12.136422s(724.768Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 12.155159s(723.651Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 12.246918s(718.229Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 12.240157s(718.626Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 12.205895s(720.643Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 12.001869s(732.894Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 12.127655s(725.292Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 12.195832s(721.238Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 12.124491s(725.481Gflops) Average Gflops: 723.920, Max Gflops: 732.894 Average Time: 12.151067s, Min Time: 12.001869s Intel平台上openblas表现不如mkl。\n1.3 AMD AOCL 接下来看看AMD自家的blas库表现如何。AOCL1是AMD针对自家平台推出的CPU优化库，包含了blas、lapack、fftw等数学运算相关的库文件，用于在AMD CPU上加速数学计算任务。和1.2中的一样，头文件同样都是cblas.h，这回连blas.c都不用更改。CMakeLists.txt如下所示，配置cmake时需要传递AOCL的安装路径AOCL_DIR参数，数据模型AOCL_BLAS_DATA默认采用LP64。演示用的AOCL是Windows下的5.1版本，其他版本可能需要根据安装包文件自行调整。\ncmake_minimum_required(VERSION 3.13) project(aocl LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # setting aocl directory if(DEFINED AOCL_DIR) message(STATUS \u0026#34;AOCL_DIR is set to: ${AOCL_DIR}\u0026#34;) else() message(FATAL_ERROR \u0026#34;AOCL_DIR is not defined. Please set it to the AOCL installation directory.\u0026#34;) endif() if(NOT DEFINED AOCL_BLAS_DATA) set(AOCL_BLAS_DATA LP64) message(STATUS \u0026#34;AOCL_BLAS_DATA is not defined. Use default value: ${AOCL_BLAS_DATA}\u0026#34;) else() message(STATUS \u0026#34;AOCL_BLAS_DATA is set to: ${AOCL_BLAS_DATA}\u0026#34;) endif() # Enable AOCL BLAS set(AOCL_BLIS_INCLUDE_DIRS ${AOCL_DIR}/amd-blis/include/${AOCL_BLAS_DATA}) set(AOCL_BLIS_LINK_DIR ${AOCL_DIR}/amd-blis/lib/${AOCL_BLAS_DATA}) set(AOCL_BLIS_LIBS AOCL-LibBlis-Win-MT.lib) # find OpenMP find_package(OpenMP REQUIRED) set(SRC_LIST src/main.c src/blas.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_include_directories(${EXECUTE_FILE_NAME} PRIVATE ${AOCL_BLIS_INCLUDE_DIRS} ) target_link_directories(${EXECUTE_FILE_NAME} PRIVATE ${AOCL_BLIS_LINK_DIR} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ${AOCL_BLIS_LIBS} OpenMP::OpenMP_C ) 在AMD机器上使用vs2022的clang-cl编译，这里传递的是参数是，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\aocl\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/aocl/build/Release/aocl_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.174192s(789.010Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.173520s(792.065Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.207043s(663.819Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.174526s(787.497Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.179918s(763.899Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.154489s(889.637Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.157877s(870.547Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.165565s(830.119Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.156284s(879.416Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.207975s(660.843Gflops) Average Gflops: 792.685, Max Gflops: 889.637 Average Time: 0.175139s, Min Time: 0.154489s PS D:\\example\\efficiency_v3\\c\\aocl\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/aocl/build/Release/aocl_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 0.307589s(446.827Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.348955s(393.859Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.292173s(470.403Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.296039s(464.259Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.291551s(471.407Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.316478s(434.277Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.306831s(447.931Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.302522s(454.311Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.291020s(472.266Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.301170s(456.351Gflops) Average Gflops: 451.189, Max Gflops: 472.266 Average Time: 0.305433s, Min Time: 0.291020s 在AMD的机器上单精度浮点数表现稍逊于openblas，双精度浮点数性能优于openblas，整体性能强于Intel mkl。\n在Intel工作站上运行效果如下：\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./aocl_msvc_clang_19.1.5.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 5.475668s(1606.396Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 5.130221s(1714.564Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 5.183950s(1696.794Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 5.265062s(1670.653Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 5.232872s(1680.930Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 4.979762s(1766.368Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 5.069269s(1735.180Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 5.059971s(1738.368Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 5.063930s(1737.009Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 5.029378s(1748.942Gflops) Average Gflops: 1709.521, Max Gflops: 1766.368 Average Time: 5.149008s, Min Time: 4.979762s PS E:\\example\\efficiency_v3\\run\u0026gt; ./aocl_msvc_clang_19.1.5.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 10.912167s(806.081Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 10.452268s(841.549Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 10.402997s(845.535Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 10.937584s(804.208Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 10.529708s(835.360Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 10.834290s(811.875Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 10.291204s(854.720Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 10.675223s(823.973Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 10.651213s(825.830Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 10.510095s(836.918Gflops) Average Gflops: 828.605, Max Gflops: 854.720 Average Time: 10.619675s, Min Time: 10.291204s aocl在Intel平台上的表现甚至要优于openblas，仅次于mkl。\n2. GPU并行加速BLAS库 2.1 CUDA cuda是Nvidia专门为自家显卡开发的高性能计算工具包，其中包含了cublas用于实现blas的功能。main.c主程序文件不变，新建cuda.cu文件用于实现cuda加速矩阵乘法运算。\n#include \u0026lt;cublas_v2.h\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; extern \u0026#34;C\u0026#34; void matrix_multiply_float(int n, float A[], float B[], float C[]) { cublasStatus_t status; cublasHandle_t handle; float *d_A = 0; float *d_B = 0; float *d_C = 0; float alpha = 1.0; float beta = 0.0; cudaMalloc((void **)\u0026amp;d_A, n * n * sizeof(d_A[0])); cudaMalloc((void **)\u0026amp;d_B, n * n * sizeof(d_B[0])); cudaMalloc((void **)\u0026amp;d_C, n * n * sizeof(d_C[0])); cublasSetVector(n * n, sizeof(*A), A, 1, d_A, 1); cublasSetVector(n * n, sizeof(*B), B, 1, d_B, 1); cublasSetVector(n * n, sizeof(*C), C, 1, d_C, 1); status = cublasCreate(\u0026amp;handle); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! CUBLAS initialization error\\n\u0026#34;); return; } status = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, n, n, n, \u0026amp;alpha, d_A, n, d_B, n, \u0026amp;beta, d_C, n); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! CUBLAS Sgemm error\\n\u0026#34;); return; } cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); status = cublasDestroy(handle); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! shutdown error (A)\\n\u0026#34;); return; } } extern \u0026#34;C\u0026#34; void matrix_multiply_double(int n, double A[], double B[], double C[]) { cublasStatus_t status; cublasHandle_t handle; double *d_A = 0; double *d_B = 0; double *d_C = 0; double alpha = 1.0; double beta = 0.0; cudaMalloc((void **)\u0026amp;d_A, n * n * sizeof(d_A[0])); cudaMalloc((void **)\u0026amp;d_B, n * n * sizeof(d_B[0])); cudaMalloc((void **)\u0026amp;d_C, n * n * sizeof(d_C[0])); cublasSetVector(n * n, sizeof(*A), A, 1, d_A, 1); cublasSetVector(n * n, sizeof(*B), B, 1, d_B, 1); cublasSetVector(n * n, sizeof(*C), C, 1, d_C, 1); status = cublasCreate(\u0026amp;handle); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! CUBLAS initialization error\\n\u0026#34;); return; } status = cublasDgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, n, n, n, \u0026amp;alpha, d_A, n, d_B, n, \u0026amp;beta, d_C, n); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! CUBLAS Dgemm error\\n\u0026#34;); return; } cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); status = cublasDestroy(handle); if (status != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \u0026#34;!!!! shutdown error (A)\\n\u0026#34;); return; } } CMakeLists.txt文件如下，cmake会自动调用nvcc对cuda源文件进行编译，需要链接到cublas库和cuda的运行时。\ncmake_minimum_required(VERSION 3.13) project(cuda LANGUAGES C) set(CMAKE_C_STANDARD 11) enable_language(CUDA) set(CMAKE_CUDA_ARCHITECTURES OFF) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) set(SRC_LIST src/main.c src/cuda.cu ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST} ) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE cublas cudart ) 这里编译用的cuda工具包版本12.9，编译器是vs2022的msvc，在NVIDIA RTX A4000单卡工作站上运行效果如下：\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./cuda_msvc_msvc_19.44.35209.0.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 2.473336s(3556.369Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 2.124057s(4141.175Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 2.141353s(4107.727Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 2.164315s(4064.146Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 2.146332s(4098.198Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 2.083143s(4222.510Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 2.092037s(4204.559Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 2.200659s(3997.026Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 2.104675s(4179.311Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 2.177780s(4039.018Gflops) Average Gflops: 4061.004, Max Gflops: 4222.510 Average Time: 2.170769s, Min Time: 2.083143s PS E:\\example\\efficiency_v3\\run\u0026gt; ./cuda_msvc_msvc_19.44.35209.0.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 31.435958s(279.810Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 32.104813s(273.981Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 33.338183s(263.844Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 33.624088s(261.601Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 33.429806s(263.121Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 33.508910s(262.500Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 33.479590s(262.730Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 33.880561s(259.621Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 33.770107s(260.470Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 33.821256s(260.076Gflops) Average Gflops: 264.775, Max Gflops: 279.810 Average Time: 33.239327s, Min Time: 31.435958s 只能说单精度浮点运算性能确实很强大，双精度浮点运算性能不如CPU加速。\n为了确保main.c文件通用，cuda.cu将显存分配程序写到了计算函数内部，实际上扣除掉这一部分的开销，真实的计算性能应该会更高。不过这也符合的编程习惯，一般高性能计算程序都是将不同的数学库包装到单独的模块当中，和主程序分开，根据运行时切换合适的模块。因此运行时间主要统计主程序调用不同模块的计算时间。主程序编写者不需要关心每个模块到底如何执行，只需要关心输出结果和性能表现。\n2.2 OpenCL opencl不光支持Nvidia，还支持AMD和Intel的产品。虽然显卡厂商都宣称支持opencl，但是Nvidia有cuda作为护城河。AMD在rocm发布之前长期缺乏同样功能的套件，只能依靠opencl实现类似的生态。并且现阶段rocm只支持AMD少数高端显卡，不像cuda支持相对广泛。\n使用opencl实现的blas库主要有两个：一个是clblas，已经停止开发很久了；另一个是clblast，现阶段可作为clblas的替代品。\n和前面一样main.c通用，新建clblast.c文件调用clblast实现矩阵乘法运算。\n// Reference : https://github.com/CNugteren/CLBlast/blob/master/samples/sgemm.c #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;clblast_c.h\u0026gt; #define CL_TARGET_OPENCL_VERSION 120 #define CL_USE_DEPRECATED_OPENCL_1_2_APIS void matrix_multiply_float(int n, float A[], float B[], float C[]) { const size_t platform_id = 0; const size_t device_id = 0; const float alpha = 0.7f; const float beta = 1.0f; cl_uint num_platforms; clGetPlatformIDs(0, NULL, \u0026amp;num_platforms); cl_platform_id *platforms = (cl_platform_id *)malloc(num_platforms * sizeof(cl_platform_id)); clGetPlatformIDs(num_platforms, platforms, NULL); cl_platform_id platform = platforms[platform_id]; cl_uint num_devices; clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, 0, NULL, \u0026amp;num_devices); cl_device_id *devices = (cl_device_id *)malloc(num_devices * sizeof(cl_device_id)); clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, num_devices, devices, NULL); cl_device_id device = devices[device_id]; cl_context context = clCreateContext(NULL, 1, \u0026amp;device, NULL, NULL, NULL); cl_command_queue queue = clCreateCommandQueue(context, device, 0, NULL); cl_event event = NULL; cl_mem device_a = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(float), NULL, NULL); cl_mem device_b = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(float), NULL, NULL); cl_mem device_c = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(float), NULL, NULL); clEnqueueWriteBuffer(queue, device_a, CL_TRUE, 0, n * n * sizeof(float), A, 0, NULL, NULL); clEnqueueWriteBuffer(queue, device_b, CL_TRUE, 0, n * n * sizeof(float), B, 0, NULL, NULL); clEnqueueWriteBuffer(queue, device_c, CL_TRUE, 0, n * n * sizeof(float), C, 0, NULL, NULL); CLBlastStatusCode status = CLBlastSgemm(CLBlastLayoutRowMajor, CLBlastTransposeNo, CLBlastTransposeNo, n, n, n, alpha, device_a, 0, n, device_b, 0, n, beta, device_c, 0, n, \u0026amp;queue, \u0026amp;event); if (status == CLBlastSuccess) { clWaitForEvents(1, \u0026amp;event); clReleaseEvent(event); } else { fprintf(stderr, \u0026#34;Error! CLBlast failed with status %d\\n\u0026#34;, status); } free(platforms); free(devices); clReleaseMemObject(device_a); clReleaseMemObject(device_b); clReleaseMemObject(device_c); clReleaseCommandQueue(queue); clReleaseContext(context); } void matrix_multiply_double(int n, double A[], double B[], double C[]) { const size_t platform_id = 0; const size_t device_id = 0; const double alpha = 0.7f; const double beta = 1.0f; cl_uint num_platforms; clGetPlatformIDs(0, NULL, \u0026amp;num_platforms); cl_platform_id *platforms = (cl_platform_id *)malloc(num_platforms * sizeof(cl_platform_id)); clGetPlatformIDs(num_platforms, platforms, NULL); cl_platform_id platform = platforms[platform_id]; cl_uint num_devices; clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, 0, NULL, \u0026amp;num_devices); cl_device_id *devices = (cl_device_id *)malloc(num_devices * sizeof(cl_device_id)); clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, num_devices, devices, NULL); cl_device_id device = devices[device_id]; cl_context context = clCreateContext(NULL, 1, \u0026amp;device, NULL, NULL, NULL); cl_command_queue queue = clCreateCommandQueue(context, device, 0, NULL); cl_event event = NULL; cl_mem device_a = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(double), NULL, NULL); cl_mem device_b = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(double), NULL, NULL); cl_mem device_c = clCreateBuffer(context, CL_MEM_READ_WRITE, n * n * sizeof(double), NULL, NULL); clEnqueueWriteBuffer(queue, device_a, CL_TRUE, 0, n * n * sizeof(double), A, 0, NULL, NULL); clEnqueueWriteBuffer(queue, device_b, CL_TRUE, 0, n * n * sizeof(double), B, 0, NULL, NULL); clEnqueueWriteBuffer(queue, device_c, CL_TRUE, 0, n * n * sizeof(double), C, 0, NULL, NULL); CLBlastStatusCode status = CLBlastDgemm(CLBlastLayoutRowMajor, CLBlastTransposeNo, CLBlastTransposeNo, n, n, n, alpha, device_a, 0, n, device_b, 0, n, beta, device_c, 0, n, \u0026amp;queue, \u0026amp;event); if (status == CLBlastSuccess) { clWaitForEvents(1, \u0026amp;event); clReleaseEvent(event); } else { fprintf(stderr, \u0026#34;Error! CLBlast failed with status %d\\n\u0026#34;, status); } free(platforms); free(devices); clReleaseMemObject(device_a); clReleaseMemObject(device_b); clReleaseMemObject(device_c); clReleaseCommandQueue(queue); clReleaseContext(context); } 和cuda一样，扣掉显存分配和预热的开销，真正的计算时间只会更短。\nCMakeLists.txt文件如下，需要链接到opencl：\ncmake_minimum_required(VERSION 3.13) project(opencl LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable clBlast find_package(clBlast REQUIRED) set(SRC_LIST src/main.c src/clblast.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE clBlast opencl ) 编译工具链用的是MSYS2的ucrt64，编译机器配置是AMD Radeon 880M核显，在编译机器上运行效果如下：\nPS D:\\example\\efficiency_v3\\run\u0026gt; ./opencl_gnu_gnu_15.1.0.exe -l 10 -n 12 Using float precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 1.117597s(122.977Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 0.186813s(735.703Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 0.162170s(847.499Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 0.168798s(814.219Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 0.159344s(862.531Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 0.162423s(846.179Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 0.180195s(762.723Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 0.156903s(875.949Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 0.170804s(804.659Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 0.150039s(916.018Gflops) Average Gflops: 758.846, Max Gflops: 916.018 Average Time: 0.261509s, Min Time: 0.150039s PS D:\\example\\efficiency_v3\\run\u0026gt; ./opencl_gnu_gnu_15.1.0.exe -l 10 -n 12 -double Using double precision for matrix multiplication. 1 : 4096 x 4096 Matrix multiply wall time : 2.102548s(65.368Gflops) 2 : 4096 x 4096 Matrix multiply wall time : 1.159928s(118.489Gflops) 3 : 4096 x 4096 Matrix multiply wall time : 1.173824s(117.087Gflops) 4 : 4096 x 4096 Matrix multiply wall time : 1.161465s(118.332Gflops) 5 : 4096 x 4096 Matrix multiply wall time : 1.159717s(118.511Gflops) 6 : 4096 x 4096 Matrix multiply wall time : 1.172199s(117.249Gflops) 7 : 4096 x 4096 Matrix multiply wall time : 1.157075s(118.781Gflops) 8 : 4096 x 4096 Matrix multiply wall time : 1.178716s(116.601Gflops) 9 : 4096 x 4096 Matrix multiply wall time : 1.162563s(118.221Gflops) 10 : 4096 x 4096 Matrix multiply wall time : 1.175523s(116.917Gflops) Average Gflops: 112.556, Max Gflops: 118.781 Average Time: 1.260356s, Min Time: 1.157075s 单精度浮点运算性能与CPU加速性能相当，双精度浮点运算性能只有CPU加速性能的1/4左右。\n在NVIDIA RTX A4000单卡工作站上运行效果如下：\nPS E:\\example\\efficiency_v3\\run\u0026gt; ./opencl_gnu_gnu_15.1.0.exe -l 10 -n 14 Using float precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 6.913470s(1272.312Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 6.030729s(1458.545Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 6.100249s(1441.924Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 6.059375s(1451.650Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 6.045835s(1454.901Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 6.013252s(1462.785Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 6.010330s(1463.496Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 6.043099s(1455.560Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 6.087911s(1444.846Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 6.010101s(1463.552Gflops) Average Gflops: 1436.957, Max Gflops: 1463.552 Average Time: 6.131435s, Min Time: 6.010101s PS E:\\example\\efficiency_v3\\run\u0026gt; ./opencl_gnu_gnu_15.1.0.exe -l 10 -n 14 -double Using double precision for matrix multiplication. 1 : 16384 x 16384 Matrix multiply wall time : 30.209699s(291.168Gflops) 2 : 16384 x 16384 Matrix multiply wall time : 32.432424s(271.213Gflops) 3 : 16384 x 16384 Matrix multiply wall time : 32.288050s(272.426Gflops) 4 : 16384 x 16384 Matrix multiply wall time : 32.304304s(272.289Gflops) 5 : 16384 x 16384 Matrix multiply wall time : 32.392134s(271.550Gflops) 6 : 16384 x 16384 Matrix multiply wall time : 32.370801s(271.729Gflops) 7 : 16384 x 16384 Matrix multiply wall time : 32.440887s(271.142Gflops) 8 : 16384 x 16384 Matrix multiply wall time : 32.515872s(270.517Gflops) 9 : 16384 x 16384 Matrix multiply wall time : 32.635461s(269.526Gflops) 10 : 16384 x 16384 Matrix multiply wall time : 32.786553s(268.284Gflops) Average Gflops: 272.984, Max Gflops: 291.168 Average Time: 32.237619s, Min Time: 30.209699s 单精度浮点数运算跟cuda相比差太多了，只有cuda的1/3左右，还不如mkl；双精度反而比cuda好一点。\n3. 总结 GPU加速主要优势在于大规模单精度浮点数计算，由于显存预热时间长，对于小规模计算GPU加速优势不明显。超大规模计算就需要考虑显存大小和内存交换能力的限制。对于双精度浮点数计算，GPU加速效果并不明显，甚至不如CPU加速性能提升来得显著。\n如果只考虑CPU加速库，最好根据cpu平台选择对应厂商的数学库，比如Intel平台适合mkl，AMD平台AOCL是首选，当然openblas也可以作为通用设备跨平台备选项。比较合理的办法是针对不同平台开发特定的计算模块，再通过运行时灵活调度，确保在各个平台上都能达到最优性能。\nAMD Optimizing CPU Libraries (AOCL) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-06-24-matrix_multiplication-2/","tags":[{"LinkTitle":"C++","RelPermalink":"/zh-cn/tags/c++/"}],"title":"矩阵乘法运算(二)-基于BLAS库的加速运算"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/zh-cn/categories/code/"}],"content":"说到矩阵，只要是理工科都会想到被线性代数课支配的恐惧。矩阵乘法运算，往大了说各种工业和科研数值计算离不开它，往小了说各种跑分软件也会用到它，矩阵乘法运算的耗时也是评判计算机浮点运算性能的重要指标。本文的目的是通过矩阵乘法运算验证各种实现方式的性能差异，并对比不同计算平台的性能差异，为高性能计算开发提供参考。\n1. 矩阵乘法算法 矩阵乘法运算也成为矩阵点乘，可以用下图表示1，通常由A矩阵对应行和B矩阵对应列的元素相乘并累加，形成C矩阵对应行列的值。要求A矩阵的列数要与B矩阵的行数相等，C矩阵的尺寸则相当于B矩阵的行数×A矩阵的列数。 用C语言表示一个M×N矩阵点乘一个N×T矩阵，一般写成3层循环嵌套的形式。\n... for (int i = 0; i \u0026lt; M; i++) { for (int j = 0; j \u0026lt; T; j++) { C[i,j] = 0.0; for (int k = 0; k \u0026lt; N; k++) { C[i,j] += A[i,k] * B[k,j]; } } } ... 通常认为一个N×N维矩阵的乘法次数为N的3次方，用O(n3)表示计算复杂度。也有一些算法，将其中某些乘法运算转化成加法，减少了乘法运算次数。比如Strassen算法2，理论上计算复杂度只有O(n2.807)，降低了大矩阵相乘的耗时。更新的算法比如Coppersmith-Winograd方法，计算复杂度更是号称只有O(n2.3727)。这种算法上的差异不在本文讨论之列。\n2. 循环嵌套并行化 现代计算机处理器一般都为多核心，为充分利用处理器性能，使用openmp等并行库对计算程序进行并行优化不失为一个好的选项。使用单核心进行循环嵌套求解没有实际意义，这里不做演示，有兴趣的可以在下面的代码中单独去掉并行库后进行编译计算。\n2.1 C实现 C实现很简单，通过三层循环嵌套实现矩阵相乘算法。这里通过openmp将前两层循环合并分配到不同线程中展开计算3，以充分利用多核心处理器的性能。以下是openmp.c的示例代码。\n#include \u0026lt;omp.h\u0026gt; void matrix_multiply_float(int n, float A[], float B[], float C[]) { #pragma omp parallel for collapse(2) shared(A, B, C) for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { C[i * n + j] = 0; for (int k = 0; k \u0026lt; n; k++) { C[i * n + j] += A[i * n + k] * B[k * n + j]; } } } } void matrix_multiply_double(int n, double A[], double B[], double C[]) { #pragma omp parallel for collapse(2) shared(A, B, C) for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { C[i * n + j] = 0; for (int k = 0; k \u0026lt; n; k++) { C[i * n + j] += A[i * n + k] * B[k * n + j]; } } } } 在main.c中定义一些命令行开关，用户可以自定义矩阵尺寸、循环次数、计算精度等。这里定义了一个性能参数，GFLOPs，用来衡量单位时间内执行浮点运算的能力，1GFLOPs相当于1秒内执行109次浮点运算。\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;time.h\u0026gt; #include \u0026lt;math.h\u0026gt; #define MAX(a, b) ((a) \u0026gt; (b) ? (a) : (b)) #define MIN(a, b) ((a) \u0026lt; (b) ? (a) : (b)) extern void matrix_multiply_float(int n, float A[], float B[], float C[]); extern void matrix_multiply_double(int n, double A[], double B[], double C[]); // Initialize matrix void initialize_matrix_float(int n, float matrix[]) { srand((unsigned)time(NULL)); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { matrix[i * n + j] = rand() / (float)(RAND_MAX); } } } void initialize_matrix_double(int n, double matrix[]) { srand((unsigned)time(NULL)); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { matrix[i * n + j] = rand() / (double)(RAND_MAX); } } } // Execute matrix multiply and print results int execute_float(int dim, int loop_num, double *ave_gflops, double *max_gflops, double *ave_time, double *min_time) { // Use volatile to prevent compiler optimizations volatile float *a, *b, *c; struct timespec start_ns, end_ns; double cpu_time; for (int i = 0; i \u0026lt; loop_num; i++) { a = (float *)malloc(dim * dim * sizeof(float)); b = (float *)malloc(dim * dim * sizeof(float)); c = (float *)malloc(dim * dim * sizeof(float)); if (a == NULL || b == NULL || c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } initialize_matrix_float(dim, a); initialize_matrix_float(dim, b); timespec_get(\u0026amp;start_ns, TIME_UTC); matrix_multiply_float(dim, a, b, c); timespec_get(\u0026amp;end_ns, TIME_UTC); cpu_time = (end_ns.tv_sec - start_ns.tv_sec) + (end_ns.tv_nsec - start_ns.tv_nsec) / 1e9; double gflops = 1e-9 * dim * dim * dim * 2 / cpu_time; printf(\u0026#34;%d\\t: %d x %d Matrix multiply wall time : %.6fs(%.3fGflops)\\n\u0026#34;, i + 1, dim, dim, cpu_time, gflops); free(a); free(b); free(c); *ave_gflops += gflops; *max_gflops = MAX(*max_gflops, gflops); *ave_time += cpu_time; *min_time = MIN(*min_time, cpu_time); } *ave_gflops /= loop_num; *ave_time /= loop_num; return 1; } int execute_double(int dim, int loop_num, double *ave_gflops, double *max_gflops, double *ave_time, double *min_time) { // Use volatile to prevent compiler optimizations volatile double *a, *b, *c; struct timespec start_ns, end_ns; double cpu_time; for (int i = 0; i \u0026lt; loop_num; i++) { a = (double *)malloc(dim * dim * sizeof(double)); b = (double *)malloc(dim * dim * sizeof(double)); c = (double *)malloc(dim * dim * sizeof(double)); if (a == NULL || b == NULL || c == NULL) { fprintf(stderr, \u0026#34;Memory allocation failed\\n\u0026#34;); return 0; } initialize_matrix_double(dim, a); initialize_matrix_double(dim, b); timespec_get(\u0026amp;start_ns, TIME_UTC); matrix_multiply_double(dim, a, b, c); timespec_get(\u0026amp;end_ns, TIME_UTC); cpu_time = (end_ns.tv_sec - start_ns.tv_sec) + (end_ns.tv_nsec - start_ns.tv_nsec) / 1e9; double gflops = 1e-9 * dim * dim * dim * 2 / cpu_time; printf(\u0026#34;%d\\t: %d x %d Matrix multiply wall time : %.6fs(%.3fGflops)\\n\u0026#34;, i + 1, dim, dim, cpu_time, gflops); free(a); free(b); free(c); *ave_gflops += gflops; *max_gflops = MAX(*max_gflops, gflops); *ave_time += cpu_time; *min_time = MIN(*min_time, cpu_time); } *ave_gflops /= loop_num; *ave_time /= loop_num; return 1; } int main(int argc, char *argv[]) { int n = 10;\t// Default matrix size exponent int loop_num = 5;\t// Number of iterations for averaging double ave_gflops = 0.0, max_gflops = 0.0; // Average and maximum Gflops double ave_time = 0.0, min_time = 1e9;\t// Average and minimum time int use_double = 0;\t// Default to float precision // Help message if (argc == 1 || (argc == 2 \u0026amp;\u0026amp; (strcmp(argv[1], \u0026#34;-h\u0026#34;) == 0 || strcmp(argv[1], \u0026#34;--help\u0026#34;) == 0))) { printf(\u0026#34;Usage: %s [-n SIZE] [-l LOOP_NUM] [-float|-double]\\n\u0026#34;, argv[0]); printf(\u0026#34; -n SIZE Specify matrix size, like 2^SIZE (default: 10)\\n\u0026#34;); printf(\u0026#34; -l LOOP_NUM Specify number of iterations (default: 5)\\n\u0026#34;); printf(\u0026#34; -float Use float precision (default)\\n\u0026#34;); printf(\u0026#34; -double Use double precision\\n\u0026#34;); printf(\u0026#34; -h, --help Show this help message\\n\u0026#34;); return 0; } // Parse -n, -l, -float, -double options int double_flag = 0, float_flag = 0; for (int argi = 1; argi \u0026lt; argc; ++argi) { if (strcmp(argv[argi], \u0026#34;-n\u0026#34;) == 0 \u0026amp;\u0026amp; argi + 1 \u0026lt; argc) { n = atoi(argv[argi + 1]); argi++; } else if (strcmp(argv[argi], \u0026#34;-l\u0026#34;) == 0 \u0026amp;\u0026amp; argi + 1 \u0026lt; argc) { loop_num = atoi(argv[argi + 1]); argi++; } else if (strcmp(argv[argi], \u0026#34;-double\u0026#34;) == 0) { double_flag = 1; } else if (strcmp(argv[argi], \u0026#34;-float\u0026#34;) == 0) { float_flag = 1; } } if (double_flag \u0026amp;\u0026amp; float_flag) { fprintf(stderr, \u0026#34;Error: Cannot specify both -double and -float options.\\n\u0026#34;); return 1; } use_double = double_flag ? 1 : 0; int dim = (int)pow(2, n); if (use_double) { printf(\u0026#34;Using double precision for matrix multiplication.\\n\u0026#34;); execute_double(dim, loop_num, \u0026amp;ave_gflops, \u0026amp;max_gflops, \u0026amp;ave_time, \u0026amp;min_time); } else { printf(\u0026#34;Using float precision for matrix multiplication.\\n\u0026#34;); execute_float(dim, loop_num, \u0026amp;ave_gflops, \u0026amp;max_gflops, \u0026amp;ave_time, \u0026amp;min_time); } printf(\u0026#34;Average Gflops: %.3f, Max Gflops: %.3f\\n\u0026#34;, ave_gflops, max_gflops); printf(\u0026#34;Average Time: %.6fs, Min Time: %.6fs\\n\u0026#34;, ave_time, min_time); return 0; } 这里用到了cmake，CMakeLists.txt告诉编译器怎么包含openmp的头文件并链接到它。\ncmake_minimum_required(VERSION 3.13) project(openmp LANGUAGES C) set(CMAKE_C_STANDARD 11) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_C_COMPILER_FRONTEND_VARIANT}_${CMAKE_C_COMPILER_ID}_${CMAKE_C_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) find_package(OpenMP REQUIRED) set(SRC_LIST src/main.c src/openmp.c ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE OpenMP::OpenMP_C ) Windows下使用vs2022的clang-cl编译出来的Release程序，执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\openmp\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/Release/openmp_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.362688s(5.921Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.304758s(7.047Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.314348s(6.832Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.294248s(7.298Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.294496s(7.292Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.306986s(6.995Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.320405s(6.702Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.278521s(7.710Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.294080s(7.302Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.338626s(6.342Gflops) Average Gflops: 6.944, Max Gflops: 7.710 Average Time: 0.310916s, Min Time: 0.278521s PS D:\\example\\efficiency_v3\\c\\openmp\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/Release/openmp_msvc_clang_19.1.5.exe\u0026#34; -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.353909s(6.068Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.319001s(6.732Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.329514s(6.517Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.381114s(5.635Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.349447s(6.145Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.370087s(5.803Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.349626s(6.142Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.370023s(5.804Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.364005s(5.900Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.344441s(6.235Gflops) Average Gflops: 6.098, Max Gflops: 6.732 Average Time: 0.353117s, Min Time: 0.319001s 使用MSYS2的clang64工具链编译，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_clang_20.1.7.exe\u0026#34; -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.344437s(6.235Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.327010s(6.567Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.367141s(5.849Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.358603s(5.988Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.362182s(5.929Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.338947s(6.336Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.334758s(6.415Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.331219s(6.484Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.343316s(6.255Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.324451s(6.619Gflops) Average Gflops: 6.268, Max Gflops: 6.619 Average Time: 0.343206s, Min Time: 0.324451s PS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_clang_20.1.7.exe\u0026#34; -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.447118s(4.803Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.423630s(5.069Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.365229s(5.880Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.355163s(6.046Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.443296s(4.844Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.310319s(6.920Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.366405s(5.861Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.369704s(5.809Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.382898s(5.608Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.367266s(5.847Gflops) Average Gflops: 5.669, Max Gflops: 6.920 Average Time: 0.383103s, Min Time: 0.310319s 切换成MSYS2的ucrt64工具链编译，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.308967s(6.951Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.267709s(8.022Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.278587s(7.708Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.263047s(8.164Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.262595s(8.178Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.264196s(8.128Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.273148s(7.862Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.262910s(8.168Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.277290s(7.745Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.274961s(7.810Gflops) Average Gflops: 7.874, Max Gflops: 8.178 Average Time: 0.273341s, Min Time: 0.262595s PS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.314685s(6.824Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.290665s(7.388Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.304162s(7.060Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.315041s(6.817Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.306290s(7.011Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.356495s(6.024Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.310081s(6.926Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.301022s(7.134Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.317229s(6.769Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.307106s(6.993Gflops) Average Gflops: 6.895, Max Gflops: 7.388 Average Time: 0.312278s, Min Time: 0.290665s Windows下gcc编译器的表现要好一点，三者运算性能基本都在同一量级，差异比较小可以忽略不记。\n以上均是在AMD AI 9 365w处理器计算的结果，计算过程中基本跑满了所有核心和超线程。本文展示的计算结果，如果没有特殊说明，都是在该处理器上运行输出的结果。\n2.2 fortran实现 fortran有内置的矩阵运算函数matmul，这里先不考虑内置函数的算法优化。将C实现的3层循环嵌套用Fortran实现，再通过openmp加速。\nmain.f90实现功能和C实现的main.c一致。\nprogram main implicit none external matrix_multiply_float, matrix_multiply_double integer :: n = 10 integer :: loop_num = 5 real :: ave_gflops = 0.0, max_gflops = 0.0 real :: ave_time = 0.0, min_time = 1e9 logical :: use_double = .false. integer :: argi, i, dim character(len=100) :: arg logical :: double_set = .false., float_set = .false. if (command_argument_count() == 0) then call print_help() stop end if argi = 1 do while (argi \u0026lt;= command_argument_count()) call get_command_argument(argi, arg) if (trim(arg) == \u0026#39;-n\u0026#39; .and. argi + 1 \u0026lt;= command_argument_count()) then argi = argi + 1 call get_command_argument(argi, arg) read(arg, *) n else if (trim(arg) == \u0026#39;-l\u0026#39; .and. argi + 1 \u0026lt;= command_argument_count()) then argi = argi + 1 call get_command_argument(argi, arg) read(arg, *) loop_num else if (trim(arg) == \u0026#39;-double\u0026#39;) then double_set = .true. use_double = .true. else if (trim(arg) == \u0026#39;-float\u0026#39;) then float_set = .true. use_double = .false. else if (trim(arg) == \u0026#39;-h\u0026#39; .or. trim(arg) == \u0026#39;--help\u0026#39;) then call print_help() stop end if if (double_set .and. float_set) then print *, \u0026#34;Error: Cannot specify both -double and -float options.\u0026#34; stop end if argi = argi + 1 end do dim = 2**n if (use_double) then call perform_double(dim, loop_num, ave_gflops, max_gflops, ave_time, min_time) else call perform_float(dim, loop_num, ave_gflops, max_gflops, ave_time, min_time) end if ave_gflops = ave_gflops / loop_num ave_time = ave_time / loop_num print \u0026#39;(A, F8.3, A, F8.3)\u0026#39;, \u0026#39;Average Gflops: \u0026#39;, ave_gflops, \u0026#39;, Max Gflops: \u0026#39;, max_gflops print \u0026#39;(A, F10.6, A, F10.6, A)\u0026#39;, \u0026#39;Average Time: \u0026#39;, ave_time, \u0026#39;s, Min Time: \u0026#39;, min_time, \u0026#39;s\u0026#39; contains subroutine print_help() print *, \u0026#39;Usage: program_name [-n SIZE] [-l LOOP_NUM] [-float|-double]\u0026#39; print *, \u0026#39; -n SIZE Specify matrix size, like 2^SIZE (default: 10)\u0026#39; print *, \u0026#39; -l LOOP_NUM Specify number of iterations (default: 5)\u0026#39; print *, \u0026#39; -float Use real*32 precision (default)\u0026#39; print *, \u0026#39; -double Use real*64 precision\u0026#39; print *, \u0026#39; -h, --help Show this help message\u0026#39; end subroutine print_help subroutine initialize_matrix_float(n, matrix) integer, intent(in) :: n real, intent(out) :: matrix(n, n) integer :: i, j real :: rand call random_seed() do i = 1, n do j = 1, n call random_number(rand) matrix(i, j) = rand end do end do end subroutine initialize_matrix_float subroutine initialize_matrix_double(n, matrix) integer, intent(in) :: n real*8, intent(out) :: matrix(n, n) integer :: i, j real*8 :: rand call random_seed() do i = 1, n do j = 1, n call random_number(rand) matrix(i, j) = rand end do end do end subroutine initialize_matrix_double subroutine perform_double(dim, loop_num, ave_gflops, max_gflops, ave_time, min_time) integer, intent(in) :: dim, loop_num real, intent(inout) :: ave_gflops, max_gflops, ave_time real, intent(inout) :: min_time real*8, allocatable :: a_double(:, :), b_double(:, :), c_double(:, :) real :: gflops integer*8 :: i, start_count(1), end_count(1), count_rate(1) real :: elapsed_time print *, \u0026#39;Using real*64 precision for matrix multiplication.\u0026#39; allocate(a_double(dim, dim), b_double(dim, dim), c_double(dim, dim)) do i = 1, loop_num call initialize_matrix_double(dim, a_double) call initialize_matrix_double(dim, b_double) call system_clock(count=start_count(1), count_rate=count_rate(1)) call matrix_multiply_double(dim, a_double, b_double, c_double) call system_clock(count=end_count(1)) elapsed_time = real(end_count(1) - start_count(1)) / real(count_rate(1)) gflops = 1e-9 * dim * dim * dim * 2 / elapsed_time print \u0026#39;(I8, A, I0, A, I0, A, F10.6, A, F8.3, A)\u0026#39;, i, \u0026#39; : \u0026#39;, dim, \u0026#39; x \u0026#39;, dim, \u0026#39; Matrix multiply wall time : \u0026#39;, elapsed_time, \u0026#39;s(\u0026#39;, gflops, \u0026#39;Gflops)\u0026#39; ave_gflops = ave_gflops + gflops max_gflops = max(max_gflops, gflops) ave_time = ave_time + elapsed_time min_time = min(min_time, elapsed_time) end do deallocate(a_double, b_double, c_double) end subroutine perform_double subroutine perform_float(dim, loop_num, ave_gflops, max_gflops, ave_time, min_time) integer, intent(in) :: dim, loop_num real, intent(inout) :: ave_gflops, max_gflops, ave_time real, intent(inout) :: min_time real*4, allocatable :: a_float(:, :), b_float(:, :), c_float(:, :) real :: gflops integer*8 :: i, start_count(1), end_count(1), count_rate(1) real :: elapsed_time print *, \u0026#39;Using real*32 precision for matrix multiplication.\u0026#39; allocate(a_float(dim, dim), b_float(dim, dim), c_float(dim, dim)) do i = 1, loop_num call initialize_matrix_float(dim, a_float) call initialize_matrix_float(dim, b_float) call system_clock(count=start_count(1), count_rate=count_rate(1)) call matrix_multiply_float(dim, a_float, b_float, c_float) call system_clock(count=end_count(1)) elapsed_time = real(end_count(1) - start_count(1)) / real(count_rate(1)) gflops = 1e-9 * dim * dim * dim * 2 / elapsed_time print \u0026#39;(I8, A, I0, A, I0, A, F10.6, A, F8.3, A)\u0026#39;, i, \u0026#39; : \u0026#39;, dim, \u0026#39; x \u0026#39;, dim, \u0026#39; Matrix multiply wall time : \u0026#39;, elapsed_time, \u0026#39;s(\u0026#39;, gflops, \u0026#39;Gflops)\u0026#39; ave_gflops = ave_gflops + gflops max_gflops = max(max_gflops, gflops) ave_time = ave_time + elapsed_time min_time = min(min_time, elapsed_time) end do deallocate(a_float, b_float, c_float) end subroutine perform_float end program main omp.f90是3层循环算法的具体实现，同样用openmp展开前两层循环实现并行化。\nsubroutine matrix_multiply_float(n, a, b, c) implicit none integer, intent(in) :: n real*4, intent(in) :: a(n, n), b(n, n) real*4, intent(out) :: c(n, n) integer :: i, j, k c = 0.0 !$omp parallel do private(i, j, k) shared(a, b, c, n) collapse(2) do i = 1, n do j = 1, n do k = 1, n c(i, j) = c(i, j) + a(i, k) * b(k, j) end do end do end do !$omp end parallel do end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, a, b, c) implicit none integer, intent(in) :: n real*8, intent(in) :: a(n, n), b(n, n) real*8, intent(out) :: c(n, n) integer :: i, j, k c = 0.0d0 !$omp parallel do private(i, j, k) shared(a, b, c, n) collapse(2) do i = 1, n do j = 1, n do k = 1, n c(i, j) = c(i, j) + a(i, k) * b(k, j) end do end do end do !$omp end parallel do end subroutine matrix_multiply_double CMakeLists.txt文件如下：\ncmake_minimum_required(VERSION 3.13) project(openmp-fortran LANGUAGES Fortran) set(CMAKE_Fortran_STANDARD 2008) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_Fortran_COMPILER_FRONTEND_VARIANT}_${CMAKE_Fortran_COMPILER_ID}_${CMAKE_Fortran_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) # Enable OpenMP find_package(OpenMP REQUIRED) set(SRC_LIST src/main.f90 src/omp.f90 ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE OpenMP::OpenMP_Fortran ) 使用Intel oneAPI的ifx编译，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/Release/openmp-fortran_msvc_intelllvm_2025.1.1.exe\u0026#34; -l 10 -n 10 Using real*32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.299000s( 7.182Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.284000s( 7.562Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.337000s( 6.372Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.310000s( 6.927Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.321000s( 6.690Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.358000s( 5.999Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.290000s( 7.405Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.312000s( 6.883Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.301000s( 7.134Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.290000s( 7.405Gflops) Average Gflops: 6.956, Max Gflops: 7.562 Average Time: 0.310200s, Min Time: 0.284000s PS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\\Release\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/Release/openmp-fortran_msvc_intelllvm_2025.1.1.exe\u0026#34; -l 10 -n 10 -double Using real*64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.391000s( 5.492Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.362000s( 5.932Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.334000s( 6.430Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.393000s( 5.464Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.331000s( 6.488Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.357000s( 6.015Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.366000s( 5.867Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.345000s( 6.225Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.341000s( 6.298Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.359000s( 5.982Gflops) Average Gflops: 6.019, Max Gflops: 6.488 Average Time: 0.357900s, Min Time: 0.331000s 使用MSYS2的ucrt64工具链编译，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/openmp-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using real*32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.224624s( 9.560Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.224959s( 9.546Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.228968s( 9.379Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.238853s( 8.991Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.216879s( 9.902Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.212407s( 10.110Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.233809s( 9.185Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.240426s( 8.932Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.241291s( 8.900Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.222414s( 9.655Gflops) Average Gflops: 9.416, Max Gflops: 10.110 Average Time: 0.228463s, Min Time: 0.212407s PS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/openmp-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using real*64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.258885s( 8.295Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.299699s( 7.165Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.298025s( 7.206Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.291631s( 7.364Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.271050s( 7.923Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.330989s( 6.488Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.278846s( 7.701Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.281121s( 7.639Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.288999s( 7.431Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.318669s( 6.739Gflops) Average Gflops: 7.395, Max Gflops: 8.295 Average Time: 0.291792s, Min Time: 0.258885s 在AMD处理器上运行的结果，Intel ifx版本比gcc版本要慢一些，多次尝试也是如此。\n将生成的程序拷贝到Intel工作站(Xeon Gold 6226R)上运行，两者运行时间基本相同。很令人怀疑是不是Intel的编译器针对AMD平台做了某些负优化。 2.3 rust实现 rust下没有openmp原生实现，但有类似功能的并行库rayon，这里用rust+rayon实现3层循环嵌套算法。\nmain.rs实现和main.c相同的功能。\nmod matmul; use matmul::{matrix_multiply_double, matrix_multiply_float}; use clap::{Arg, ArgAction, Command}; use rand::prelude::*; use std::time::Instant; fn initialize_matrix_float(n: usize, matrix: \u0026amp;mut [f32]) { let mut rng = rand::rng(); for i in 0..n * n { matrix[i] = rng.random::\u0026lt;f32\u0026gt;(); } } fn initialize_matrix_double(n: usize, matrix: \u0026amp;mut [f64]) { let mut rng = rand::rng(); for i in 0..n * n { matrix[i] = rng.random::\u0026lt;f64\u0026gt;(); } } fn execute_float(dim: usize, loop_num: usize) -\u0026gt; (f64, f64, f64, f64) { let mut ave_gflops: f64 = 0.0; let mut max_gflops: f64 = 0.0; let mut ave_time: f64 = 0.0; let mut min_time = f64::MAX; for i in 0..loop_num { let mut a = vec![0.0; dim * dim]; let mut b = vec![0.0; dim * dim]; let mut c = vec![0.0; dim * dim]; initialize_matrix_float(dim, \u0026amp;mut a); initialize_matrix_float(dim, \u0026amp;mut b); let start = Instant::now(); matrix_multiply_float(dim, \u0026amp;a, \u0026amp;b, \u0026amp;mut c); let cpu_time = start.elapsed().as_secs_f64(); let gflops = 2.0 * (dim * dim * dim) as f64 / cpu_time / 1e9; println!( \u0026#34;{}\\t: {} x {} Matrix multiply wall time : {:.6}s({:.3}Gflops)\u0026#34;, i + 1, dim, dim, cpu_time, gflops ); ave_gflops += gflops; max_gflops = max_gflops.max(gflops); ave_time += cpu_time; min_time = min_time.min(cpu_time); } ave_gflops /= loop_num as f64; ave_time /= loop_num as f64; (ave_gflops, max_gflops, ave_time, min_time) } fn execute_double(dim: usize, loop_num: usize) -\u0026gt; (f64, f64, f64, f64) { let mut ave_gflops: f64 = 0.0; let mut max_gflops: f64 = 0.0; let mut ave_time: f64 = 0.0; let mut min_time = f64::MAX; for i in 0..loop_num { let mut a = vec![0.0; dim * dim]; let mut b = vec![0.0; dim * dim]; let mut c = vec![0.0; dim * dim]; initialize_matrix_double(dim, \u0026amp;mut a); initialize_matrix_double(dim, \u0026amp;mut b); let start = Instant::now(); matrix_multiply_double(dim, \u0026amp;a, \u0026amp;b, \u0026amp;mut c); let cpu_time = start.elapsed().as_secs_f64(); let gflops = 2.0 * (dim * dim * dim) as f64 / cpu_time / 1e9; println!( \u0026#34;{}\\t: {} x {} Matrix multiply wall time : {:.6}s({:.3}Gflops)\u0026#34;, i + 1, dim, dim, cpu_time, gflops ); ave_gflops += gflops; max_gflops = max_gflops.max(gflops); ave_time += cpu_time; min_time = min_time.min(cpu_time); } ave_gflops /= loop_num as f64; ave_time /= loop_num as f64; (ave_gflops, max_gflops, ave_time, min_time) } fn main() { let matches = Command::new(\u0026#34;rayon-rs\u0026#34;) .version(\u0026#34;0.1.0\u0026#34;) .author(\u0026#34;AndrewMoa\u0026#34;) .about(\u0026#34;Matrix multiplication benchmark\u0026#34;) .arg( Arg::new(\u0026#34;size\u0026#34;) .short(\u0026#39;n\u0026#39;) .long(\u0026#34;size\u0026#34;) .help(\u0026#34;Matrix size exponent (size = 2^n)\u0026#34;) .default_value(\u0026#34;10\u0026#34;), ) .arg( Arg::new(\u0026#34;loops\u0026#34;) .short(\u0026#39;l\u0026#39;) .long(\u0026#34;loops\u0026#34;) .help(\u0026#34;Number of iterations\u0026#34;) .default_value(\u0026#34;5\u0026#34;), ) .arg( Arg::new(\u0026#34;f64\u0026#34;) .short(\u0026#39;d\u0026#39;) .long(\u0026#34;f64\u0026#34;) .help(\u0026#34;Use float64 precision\u0026#34;) .action(ArgAction::SetTrue), ) .arg( Arg::new(\u0026#34;f32\u0026#34;) .short(\u0026#39;f\u0026#39;) .long(\u0026#34;f32\u0026#34;) .help(\u0026#34;Use float32 precision (default)\u0026#34;) .action(ArgAction::SetTrue), ) .get_matches(); let n: usize = matches .get_one::\u0026lt;String\u0026gt;(\u0026#34;size\u0026#34;) .unwrap() .parse() .expect(\u0026#34;Invalid size exponent\u0026#34;); let loop_num: usize = matches .get_one::\u0026lt;String\u0026gt;(\u0026#34;loops\u0026#34;) .unwrap() .parse() .expect(\u0026#34;Invalid loop count\u0026#34;); let use_double = matches.get_flag(\u0026#34;f64\u0026#34;); let use_float = matches.get_flag(\u0026#34;f32\u0026#34;); if use_double \u0026amp;\u0026amp; use_float { eprintln!(\u0026#34;Error: Cannot specify both --f64 and --f32\u0026#34;); std::process::exit(1); } let dim = 2usize.pow(n as u32); if use_double { println!(\u0026#34;Using f64 precision for matrix multiplication.\u0026#34;); let (ave_gflops, max_gflops, ave_time, min_time) = execute_double(dim, loop_num); println!( \u0026#34;Average Gflops: {:.3}, Max Gflops: {:.3}\u0026#34;, ave_gflops, max_gflops ); println!(\u0026#34;Average Time: {:.6}s, Min Time: {:.6}s\u0026#34;, ave_time, min_time); } else { println!(\u0026#34;Using f32 precision for matrix multiplication.\u0026#34;); let (ave_gflops, max_gflops, ave_time, min_time) = execute_float(dim, loop_num); println!( \u0026#34;Average Gflops: {:.3}, Max Gflops: {:.3}\u0026#34;, ave_gflops, max_gflops ); println!(\u0026#34;Average Time: {:.6}s, Min Time: {:.6}s\u0026#34;, ave_time, min_time); } } matmul.rs实现具体算法，这里用2层循环代替了3层循环，再通过rayon展开最外层循环实现并行化，实际效果和openmp是类似的。\nuse rayon::prelude::*; pub fn matrix_multiply_float(n: usize, a: \u0026amp;[f32], b: \u0026amp;[f32], c: \u0026amp;mut [f32]) { c.par_iter_mut().enumerate().for_each(|(idx, c_ij)| { let i = idx / n; let j = idx % n; *c_ij = 0.0; for k in 0..n { *c_ij += a[i * n + k] * b[k * n + j]; } }); } pub fn matrix_multiply_double(n: usize, a: \u0026amp;[f64], b: \u0026amp;[f64], c: \u0026amp;mut [f64]) { c.par_iter_mut().enumerate().for_each(|(idx, c_ij)| { let i = idx / n; let j = idx % n; *c_ij = 0.0; for k in 0..n { *c_ij += a[i * n + k] * b[k * n + j]; } }); } Cargo.toml文件如下所示。\n[package] name = \u0026#34;rayon-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] clap = { version = \u0026#34;4.5.40\u0026#34;, features = [\u0026#34;derive\u0026#34;] } rand = \u0026#34;0.9.1\u0026#34; rayon = \u0026#34;1.10.0\u0026#34; Windows下使用msvc工具链编译，Release程序效果如下：\nPS D:\\example\\efficiency_v3\\rust\\rayon-rs\u0026gt; cargo run --release -- -l 10 -n 10 Finished `release` profile [optimized] target(s) in 0.04s Running `target\\release\\rayon-rs.exe -l 10 -n 10` Using f32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.183645s(11.694Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.189942s(11.306Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.186613s(11.508Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.190823s(11.254Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.197830s(10.855Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.194282s(11.053Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.199380s(10.771Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.198576s(10.814Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.190201s(11.291Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.195518s(10.984Gflops) Average Gflops: 11.153, Max Gflops: 11.694 Average Time: 0.192681s, Min Time: 0.183645s PS D:\\example\\efficiency_v3\\rust\\rayon-rs\u0026gt; cargo run --release -- -l 10 -n 10 -d Finished `release` profile [optimized] target(s) in 0.04s Running `target\\release\\rayon-rs.exe -l 10 -n 10 -d` Using f64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.188721s(11.379Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.186753s(11.499Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.206919s(10.378Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.197388s(10.879Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.196899s(10.907Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.206537s(10.398Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.200781s(10.696Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.191448s(11.217Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.192770s(11.140Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.178019s(12.063Gflops) Average Gflops: 11.056, Max Gflops: 12.063 Average Time: 0.194624s, Min Time: 0.178019s 3. 分块矩阵并行化 直接求解大矩阵时，为提高求解效率，可以将大矩阵分割为小块进行求解，这样可以提高缓存命中率，一定程度上可以提高计算性能。\n3.1 分块矩阵C实现 在2.1实现的基础上更改openmp.c文件内容如下，其他文件内容保持不变。\n#include \u0026lt;omp.h\u0026gt; #include \u0026lt;math.h\u0026gt; // 分块大小，根据缓存大小调整 #define BLOCK_SIZE 8 void matrix_multiply_float(int n, float A[], float B[], float C[]) { #pragma omp parallel for collapse(2) shared(A, B, C) for (int i_block = 0; i_block \u0026lt; n; i_block += BLOCK_SIZE) { for (int j_block = 0; j_block \u0026lt; n; j_block += BLOCK_SIZE) { for (int k_block = 0; k_block \u0026lt; n; k_block += BLOCK_SIZE) { int i_end = fmin(i_block + BLOCK_SIZE, n); int j_end = fmin(j_block + BLOCK_SIZE, n); int k_end = fmin(k_block + BLOCK_SIZE, n); for (int i = i_block; i \u0026lt; i_end; i++) { for (int j = j_block; j \u0026lt; j_end; j++) { for (int k = k_block; k \u0026lt; k_end; k++) { C[i * n + j] += A[i * n + k] * B[k * n + j]; } } } } } } } void matrix_multiply_double(int n, double A[], double B[], double C[]) { #pragma omp parallel for collapse(2) shared(A, B, C) for (int i_block = 0; i_block \u0026lt; n; i_block += BLOCK_SIZE) { for (int j_block = 0; j_block \u0026lt; n; j_block += BLOCK_SIZE) { for (int k_block = 0; k_block \u0026lt; n; k_block += BLOCK_SIZE) { int i_end = fmin(i_block + BLOCK_SIZE, n); int j_end = fmin(j_block + BLOCK_SIZE, n); int k_end = fmin(k_block + BLOCK_SIZE, n); for (int i = i_block; i \u0026lt; i_end; i++) { for (int j = j_block; j \u0026lt; j_end; j++) { for (int k = k_block; k \u0026lt; k_end; k++) { C[i * n + j] += A[i * n + k] * B[k * n + j]; } } } } } } } Windows下使用MSYS2的ucrt64工具链编译，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using float precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.058133s(36.941Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.060944s(35.237Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.060845s(35.294Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.060216s(35.663Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.061135s(35.127Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.060427s(35.539Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.059523s(36.078Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.060110s(35.726Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.062199s(34.526Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.059708s(35.966Gflops) Average Gflops: 35.610, Max Gflops: 36.941 Average Time: 0.060324s, Min Time: 0.058133s PS D:\\example\\efficiency_v3\\c\\openmp\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/c/openmp/build/openmp_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using double precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.065988s(32.544Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.056852s(37.773Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.059504s(36.090Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.053153s(40.402Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.057459s(37.374Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.053974s(39.787Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.055030s(39.024Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.053222s(40.349Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.054025s(39.750Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.053405s(40.211Gflops) Average Gflops: 38.330, Max Gflops: 40.402 Average Time: 0.056261s, Min Time: 0.053153s 相比于循环嵌套算法，分块之后性能有了很大的提升。分块大小和硬件平台有关，直接影响缓存命中率，进而影响计算性能，需要通过大量测试才能确定分块大小在多少最为合适。\n3.2 分块矩阵fortran实现 在2.2内容的基础上，仅仅变更omp.f90文件，增加分块矩阵算法。\nsubroutine matrix_multiply_float(n, a, b, c) implicit none integer, intent(in) :: n real*4, intent(in) :: a(n, n), b(n, n) real*4, intent(out) :: c(n, n) integer :: i, j, k, bi, bj, bk, block_size real*4 :: temp ! 定义块大小，根据缓存情况调整 block_size = 8 c = 0.0 !$omp parallel do private(bi, bj, bk, i, j, k, temp) shared(a, b, c, n, block_size) do bi = 1, n, block_size do bj = 1, n, block_size do bk = 1, n, block_size do i = bi, min(bi + block_size - 1, n) do j = bj, min(bj + block_size - 1, n) temp = c(i, j) do k = bk, min(bk + block_size - 1, n) temp = temp + a(i, k) * b(k, j) end do c(i, j) = temp end do end do end do end do end do !$omp end parallel do end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, a, b, c) implicit none integer, intent(in) :: n real*8, intent(in) :: a(n, n), b(n, n) real*8, intent(out) :: c(n, n) integer :: i, j, k, bi, bj, bk, block_size real*8 :: temp block_size = 8 c = 0.0d0 !$omp parallel do private(bi, bj, bk, i, j, k, temp) shared(a, b, c, n, block_size) do bi = 1, n, block_size do bj = 1, n, block_size do bk = 1, n, block_size do i = bi, min(bi + block_size - 1, n) do j = bj, min(bj + block_size - 1, n) temp = c(i, j) do k = bk, min(bk + block_size - 1, n) temp = temp + a(i, k) * b(k, j) end do c(i, j) = temp end do end do end do end do end do !$omp end parallel do end subroutine matrix_multiply_double Windows下使用MSYS2的ucrt64工具链编译，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/openmp-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using real*32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.091248s( 23.535Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.082953s( 25.888Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.080934s( 26.534Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.078231s( 27.451Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.076461s( 28.086Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.078698s( 27.288Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.077900s( 27.567Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.079131s( 27.138Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.083838s( 25.615Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.078336s( 27.414Gflops) Average Gflops: 26.651, Max Gflops: 28.086 Average Time: 0.080773s, Min Time: 0.076461s PS D:\\example\\efficiency_v3\\fortran\\openmp-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/openmp-fortran/build/openmp-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using real*64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.087275s( 24.606Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.080483s( 26.682Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.079304s( 27.079Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.080373s( 26.719Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.076651s( 28.016Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.080497s( 26.678Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.097153s( 22.104Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.085253s( 25.190Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.085751s( 25.043Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.098205s( 21.867Gflops) Average Gflops: 25.399, Max Gflops: 28.016 Average Time: 0.085094s, Min Time: 0.076651s 性能提升取决于分块大小，可以通过调整分块大小慢慢试出最合适的参数。\n3.3 分块矩阵rust实现 同样地，在2.3的基础上，这里仅仅改动matmul.rs文件。\nuse rayon::prelude::*; // 定义分块大小 const BLOCK_SIZE: usize = 8; pub fn matrix_multiply_float(n: usize, a: \u0026amp;[f32], b: \u0026amp;[f32], c: \u0026amp;mut [f32]) { c.par_chunks_mut(n).enumerate().for_each(|(i, c_row)| { for bj in (0..n).step_by(BLOCK_SIZE) { for bk in (0..n).step_by(BLOCK_SIZE) { for j in bj..(bj + BLOCK_SIZE).min(n) { for k in bk..(bk + BLOCK_SIZE).min(n) { c_row[j] += a[i * n + k] * b[k * n + j]; } } } } }); } pub fn matrix_multiply_double(n: usize, a: \u0026amp;[f64], b: \u0026amp;[f64], c: \u0026amp;mut [f64]) { c.par_chunks_mut(n).enumerate().for_each(|(i, c_row)| { for bj in (0..n).step_by(BLOCK_SIZE) { for bk in (0..n).step_by(BLOCK_SIZE) { for j in bj..(bj + BLOCK_SIZE).min(n) { for k in bk..(bk + BLOCK_SIZE).min(n) { c_row[j] += a[i * n + k] * b[k * n + j]; } } } } }); } Windows下使用msvc工具链编译，Release程序效果如下：\nPS D:\\example\\efficiency_v3\\rust\\rayon-rs\u0026gt; cargo run --release -- -l 10 -n 10 Finished `release` profile [optimized] target(s) in 0.03s Running `target\\release\\rayon-rs.exe -l 10 -n 10` Using f32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.113868s(18.859Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.116698s(18.402Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.125060s(17.172Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.125669s(17.088Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.116527s(18.429Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.114784s(18.709Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.117369s(18.297Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.114227s(18.800Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.118140s(18.177Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.115756s(18.552Gflops) Average Gflops: 18.249, Max Gflops: 18.859 Average Time: 0.117810s, Min Time: 0.113868s PS D:\\example\\efficiency_v3\\rust\\rayon-rs\u0026gt; cargo run --release -- -l 10 -n 10 -d Finished `release` profile [optimized] target(s) in 0.03s Running `target\\release\\rayon-rs.exe -l 10 -n 10 -d` Using f64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.120614s(17.805Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.128292s(16.739Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.122600s(17.516Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.118252s(18.160Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.127342s(16.864Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.121794s(17.632Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.130288s(16.483Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.137106s(15.663Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.130548s(16.450Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.138826s(15.469Gflops) Average Gflops: 16.878, Max Gflops: 18.160 Average Time: 0.127566s, Min Time: 0.118252s 提升幅度相比C和fortran比较有限，需要时间调整分块大小试出最合适的参数。\n4. 编译器黑魔法 前面讲了简单的循环实现算法，并且用了一些并行库实现计算加速。下面不使用并行库，改用fortran的matmul函数、以及rust的ndarray库和mathru库实现矩阵乘法运算，说明编译器优化的重要性。\n4.1 fortran内置函数matmul main.f90和2.2中的内容一致，具体实现的matmul.f90文件内容如下。\nsubroutine matrix_multiply_float(n, a, b, c) implicit none integer :: n real*4, intent(in) :: a(n, n), b(n, n) real*4, intent(out) :: c(n, n) c = matmul(a, b) end subroutine matrix_multiply_float subroutine matrix_multiply_double(n, a, b, c) implicit none integer :: n real*8, intent(in) :: a(n, n), b(n, n) real*8, intent(out) :: c(n, n) c = matmul(a, b) end subroutine matrix_multiply_double CMakeLists.txt移除了openmp相关代码。\ncmake_minimum_required(VERSION 3.13) project(matmul-fortran LANGUAGES Fortran) set(CMAKE_Fortran_STANDARD 2008) set(EXECUTE_FILE_NAME ${PROJECT_NAME}_${CMAKE_Fortran_COMPILER_FRONTEND_VARIANT}_${CMAKE_Fortran_COMPILER_ID}_${CMAKE_Fortran_COMPILER_VERSION}) string(TOLOWER ${EXECUTE_FILE_NAME} EXECUTE_FILE_NAME) set(SRC_LIST src/main.f90 src/matmul.f90 ) add_executable(${EXECUTE_FILE_NAME} ${SRC_LIST}) target_link_libraries(${EXECUTE_FILE_NAME} PRIVATE ) Windows下使用MSYS2的ucrt64工具链编译，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\fortran\\matmul-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/matmul-fortran/build/matmul-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 Using real*32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.049251s( 43.603Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.048811s( 43.996Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.048778s( 44.026Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.048698s( 44.098Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.046176s( 46.506Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.046269s( 46.413Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.046297s( 46.385Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.046634s( 46.049Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.046198s( 46.484Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.046255s( 46.428Gflops) Average Gflops: 45.399, Max Gflops: 46.506 Average Time: 0.047337s, Min Time: 0.046176s PS D:\\example\\efficiency_v3\\fortran\\matmul-fortran\\build\u0026gt; .\u0026#34;D:/example/efficiency_v3/fortran/matmul-fortran/build/matmul-fortran_gnu_gnu_15.1.0.exe\u0026#34; -l 10 -n 10 -double Using real*64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.094482s( 22.729Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.093445s( 22.981Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.090847s( 23.638Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.089272s( 24.055Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.088472s( 24.273Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.088759s( 24.195Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.088489s( 24.268Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.088787s( 24.187Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.089572s( 23.975Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.088757s( 24.195Gflops) Average Gflops: 23.850, Max Gflops: 24.273 Average Time: 0.090088s, Min Time: 0.088472s 没有用到超线程，但吊打前面的循环嵌套+openmp方法，这也许能解释为什么C火了几十年但仍旧取代不了fortran。\n4.2 rust通过ndarray库实现 ndarray是rust用于处理数组的库，内置了矩阵转置函数。与2.3中实现的通过一维数组存储的矩阵不同，使用ndarray库需要将一维数组转换成二维。\nmain.rs文件同2.3中的一致，matmul.rs文件如下所示。\nuse ndarray::{Array2, ArrayView2}; pub fn matrix_multiply_float(n: usize, a: \u0026amp;[f32], b: \u0026amp;[f32], c: \u0026amp;mut [f32]) { let a = ArrayView2::from_shape((n, n), a).unwrap(); let b = ArrayView2::from_shape((n, n), b).unwrap(); let mut c_mat = Array2::\u0026lt;f32\u0026gt;::zeros((n, n)); c_mat.assign(\u0026amp;a.dot(\u0026amp;b)); c.copy_from_slice(c_mat.as_slice().unwrap()); } pub fn matrix_multiply_double(n: usize, a: \u0026amp;[f64], b: \u0026amp;[f64], c: \u0026amp;mut [f64]) { let a = ArrayView2::from_shape((n, n), a).unwrap(); let b = ArrayView2::from_shape((n, n), b).unwrap(); let mut c_mat = Array2::\u0026lt;f64\u0026gt;::zeros((n, n)); c_mat.assign(\u0026amp;a.dot(\u0026amp;b)); c.copy_from_slice(c_mat.as_slice().unwrap()); } Cargo.toml文件如下所示。\n[package] name = \u0026#34;ndarray-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] clap = { version = \u0026#34;4.5.40\u0026#34;, features = [\u0026#34;derive\u0026#34;] } ndarray = \u0026#34;0.16.1\u0026#34; rand = \u0026#34;0.9.1\u0026#34; Windows下使用msvc工具链编译，Release程序效果如下：\nPS D:\\example\\efficiency_v3\\rust\\ndarray-rs\u0026gt; cargo run --release -- -l 10 -n 10 Finished `release` profile [optimized] target(s) in 0.04s Running `target\\release\\ndarray-rs.exe -l 10 -n 10` Using f32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.019414s(110.617Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.020280s(105.891Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.019639s(109.346Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.020076s(106.966Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.019702s(109.000Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.019438s(110.479Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.019336s(111.061Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.018605s(115.426Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.018340s(117.096Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.019040s(112.787Gflops) Average Gflops: 110.867, Max Gflops: 117.096 Average Time: 0.019387s, Min Time: 0.018340s PS D:\\example\\efficiency_v3\\rust\\ndarray-rs\u0026gt; cargo run --release -- -l 10 -n 10 -d Finished `release` profile [optimized] target(s) in 0.04s Running `target\\release\\ndarray-rs.exe -l 10 -n 10 -d` Using f64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.041449s(51.811Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.039612s(54.213Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.039634s(54.183Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.040022s(53.657Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.038563s(55.687Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.037992s(56.524Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.038128s(56.324Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.038157s(56.280Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.038750s(55.419Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.038102s(56.361Gflops) Average Gflops: 55.046, Max Gflops: 56.524 Average Time: 0.039041s, Min Time: 0.037992s 同样没有用到超线程，吊打前面的循环嵌套+rayon实现。如果不考虑一维数组和二维数组之间转换所带来的时间差，实际计算性能其实只会更高。\n4.3 rust通过mathru库实现 mathru是另一个rust数学库，提供了很多常用的线性代数计算函数，底层算法可以选择采用intel mkl或者openblas实现。\n同样地main.rs不表，只更改matmul.rs文件，如下所示。\nuse mathru::algebra::linear::matrix::General; pub fn matrix_multiply_float(n: usize, a: \u0026amp;[f32], b: \u0026amp;[f32], c: \u0026amp;mut [f32]) { let a_mat = General::new(n, n, a.to_vec()); let b_mat = General::new(n, n, b.to_vec()); let c_mat = \u0026amp;a_mat * \u0026amp;b_mat; c.copy_from_slice(\u0026amp;c_mat.convert_to_vec()); } pub fn matrix_multiply_double(n: usize, a: \u0026amp;[f64], b: \u0026amp;[f64], c: \u0026amp;mut [f64]) { let a_mat = General::new(n, n, a.to_vec()); let b_mat = General::new(n, n, b.to_vec()); let c_mat = a_mat * b_mat; c.copy_from_slice(\u0026amp;c_mat.convert_to_vec()); } Cargo.toml文件如下所示，这里使用mathru的默认实现，没有使用intel mkl或者openblas。\n[package] name = \u0026#34;mathru-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] clap = { version = \u0026#34;4.5.40\u0026#34;, features = [\u0026#34;derive\u0026#34;] } mathru = \u0026#34;0.15.5\u0026#34; rand = \u0026#34;0.9.1\u0026#34; Windows下使用msvc工具链编译，Release程序执行效果如下：\nPS D:\\example\\efficiency_v3\\rust\\mathru-rs\u0026gt; cargo run --release -- -l 10 -n 10 Finished `release` profile [optimized] target(s) in 0.05s Running `target\\release\\mathru-rs.exe -l 10 -n 10` Using f32 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.009277s(231.472Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.007753s(277.002Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.009030s(237.817Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.007585s(283.111Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.008559s(250.904Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.009235s(232.530Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.008425s(254.900Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.007451s(288.214Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.007609s(282.237Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.008545s(251.318Gflops) Average Gflops: 258.950, Max Gflops: 288.214 Average Time: 0.008347s, Min Time: 0.007451s PS D:\\example\\efficiency_v3\\rust\\mathru-rs\u0026gt; cargo run --release -- -l 10 -n 10 -d Finished `release` profile [optimized] target(s) in 0.05s Running `target\\release\\mathru-rs.exe -l 10 -n 10 -d` Using f64 precision for matrix multiplication. 1 : 1024 x 1024 Matrix multiply wall time : 0.016261s(132.063Gflops) 2 : 1024 x 1024 Matrix multiply wall time : 0.016281s(131.904Gflops) 3 : 1024 x 1024 Matrix multiply wall time : 0.015308s(140.282Gflops) 4 : 1024 x 1024 Matrix multiply wall time : 0.016065s(133.672Gflops) 5 : 1024 x 1024 Matrix multiply wall time : 0.016702s(128.578Gflops) 6 : 1024 x 1024 Matrix multiply wall time : 0.014126s(152.019Gflops) 7 : 1024 x 1024 Matrix multiply wall time : 0.017731s(121.116Gflops) 8 : 1024 x 1024 Matrix multiply wall time : 0.020181s(106.409Gflops) 9 : 1024 x 1024 Matrix multiply wall time : 0.015631s(137.388Gflops) 10 : 1024 x 1024 Matrix multiply wall time : 0.015181s(141.459Gflops) Average Gflops: 132.489, Max Gflops: 152.019 Average Time: 0.016347s, Min Time: 0.014126s 没有用到多线程，比ndarray快了不少，看来底层是进行了某些优化的。同样地，如果不考虑一维和二维数组之间的转换，实际计算性能应该会更高。\n5. 总结 使用openmp、rayon等并行库加速的的C、fortran和rust矩阵乘法程序在使用相同算法的前提下，根据不同平台、编译器的实现，矩阵乘法的性能会有所些许区别，但总体上都在同一数量级。这种简单循环并行加速的计算方法适合矩阵规模较小的场合，随着矩阵规模的增加，整体计算性能愈发呈现下降的趋势。\n即使采用分块矩阵+并行库的方法提高缓存命中率，性能提升依旧有限。而且矩阵分块大小与计算平台缓存大小有关，需要大量测试才能调出合适的分块参数。\n使用fortran的matmul和rust的数学库实现的矩阵乘法功能，即使没有采用并行加速，其矩阵乘法性能依旧领先于简单循环并行加速的实现。这恰恰说明了底层算法优化的重要性。\n在高性能计算领域除了openmp并行加速方法之外，还有blas、cuda、mpi等，将在以后的文章进一步演示。\nMatrix multiplication \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n矩阵乘法的Strassen算法详解 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n矩阵乘法并行化（使用OpenMP） \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-06-23-matrix_multiplication-1/","tags":[{"LinkTitle":"C++","RelPermalink":"/zh-cn/tags/c++/"},{"LinkTitle":"Fortran","RelPermalink":"/zh-cn/tags/fortran/"},{"LinkTitle":"Rust","RelPermalink":"/zh-cn/tags/rust/"}],"title":"矩阵乘法运算(一)-使用OpenMP加速循环计算"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/zh-cn/categories/code/"}],"content":"最近打算用rust把以前写的代码重构一下，涉及到GUI界面怎么选择的问题。rust正式发布不过十年光景，在GUI开发这方面还不如老牌的C/C++，有诸如wxWidgets、qt、gtk+等众多知名又久经考验的GUI界面库。本文选取几款rust的GUI库，简单实现一个边界层计算器，做个横向对比。\n1. slint slint 近期的宣传不可谓不卖力，号称要打造成下一代gui工具包，看来野心不小。slint通过自定义的声明式语言定义ui界面，在vscode下编程可以通过插件预览，也可以通过官方的slintpad 网站预览。\ncargo配置文件如下，slint-build用于将.slint界面文件翻译成.rs文件。\n[package] name = \u0026#34;sLayers-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] slint = \u0026#34;1.8.0\u0026#34; [build-dependencies] slint-build = \u0026#34;1.8.0\u0026#34; [profile.release] strip = true opt-level = \u0026#34;z\u0026#34; lto = true codegen-units = 1 panic = \u0026#34;abort\u0026#34; ui界面文件dialog.slint。slint可以把需要调用的参数定义到ui界面中，并自动隐式生成set_和get_方法用于在rs文件中设置这些参数的值。同样的，界面中定义的回调函数也会自动隐式生成on_方法，用于在rs文件中调用。需要注意的是，在rs中更新了ui界面中的参数，关联的控件并不会自动更新显示，需要手动更新控件显示。\nimport { Button, LineEdit, SpinBox, CheckBox, GridBox, VerticalBox } from \u0026#34;std-widgets.slint\u0026#34;; export component Dialog inherits Window { title: \u0026#34;Layers\u0026#34;; in-out property \u0026lt;string\u0026gt; tt: \u0026#34;3.0\u0026#34;; in-out property \u0026lt;int\u0026gt; nl: 5; in-out property \u0026lt;string\u0026gt; ft: \u0026#34;0.3\u0026#34;; in-out property \u0026lt;string\u0026gt; gr: \u0026#34;1.5\u0026#34;; callback calculate_first_thickness(); callback calculate_num_layers(); callback calculate_growth_rate(); callback calculate_total_thickness(); callback calculate_value(); VerticalBox { Text { text: \u0026#34;Calculate fluid boundary layer parameters.\\nCalculate the selected parameters based on the others.\u0026#34;; } GridBox { Row { b_t := CheckBox { text: \u0026#34;Total thickness (mm)\u0026#34;; checked: true; enabled: !self.checked; toggled() =\u0026gt; { if self.checked { b_n.checked = false; b_f.checked = false; b_g.checked = false; } } } e_t := LineEdit { text: root.tt; input-type: decimal; read-only: b_t.checked; } } Row { b_n := CheckBox { text: \u0026#34;Number of layers\u0026#34;; checked: false; enabled: !self.checked; toggled() =\u0026gt; { if self.checked { b_t.checked = false; b_f.checked = false; b_g.checked = false; } } } e_n := SpinBox { value: root.nl; minimum: 1; } } Row { b_f := CheckBox { text: \u0026#34;First thickness (mm)\u0026#34;; checked: false; enabled: !self.checked; toggled() =\u0026gt; { if self.checked { b_n.checked = false; b_t.checked = false; b_g.checked = false; } } } e_f := LineEdit { text: root.ft; input-type: decimal; read-only: b_f.checked; } } Row { b_g := CheckBox { text: \u0026#34;Growth rate\u0026#34;; checked: false; enabled: !self.checked; toggled() =\u0026gt; { if self.checked { b_n.checked = false; b_f.checked = false; b_t.checked = false; } } } e_g := LineEdit { text: root.gr; input-type: decimal; read-only: b_g.checked; } } } Button { text: \u0026#34;Calculate\u0026#34;; clicked =\u0026gt; { root.tt = e_t.text; root.nl = e_n.value; root.ft = e_f.text; root.gr = e_g.text; if b_t.checked { root.calculate_total_thickness(); } else if b_n.checked { root.calculate_num_layers(); } else if b_f.checked { root.calculate_first_thickness(); } else if b_g.checked { root.calculate_growth_rate(); } e_t.text = root.tt; e_g.text = root.gr; e_f.text = root.ft; e_n.value = root.nl; } } } } build.rs构建脚本调用slint-build将.slint文件翻译成.rs文件。\nfn main() { slint_build::compile(\u0026#34;ui/dialog.slint\u0026#34;).expect(\u0026#34;Slint build failed\u0026#34;); } main.rs文件，包含边界层计算的算法实现，通过回调函数更新ui界面参数。\n// Prevent console window in addition to Slint window in Windows release builds when, e.g., starting the app via file manager. Ignored on other platforms. #![cfg_attr(not(debug_assertions), windows_subsystem = \u0026#34;windows\u0026#34;)] use slint::SharedString; use std::error::Error; slint::include_modules!(); fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt; { let ui = Dialog::new()?; ui.on_calculate_total_thickness({ let ui_handle = ui.as_weak(); move || { let ui = ui_handle.unwrap(); let n = ui.get_nl(); let f = ui.get_ft().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let g = ui.get_gr().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let mut v = 0.0; for i in 0..n { v = v + f * g.powi(i); } ui.set_tt(SharedString::from(v.to_string())); } }); ui.on_calculate_first_thickness({ let ui_handle = ui.as_weak(); move || { let ui = ui_handle.unwrap(); let n = ui.get_nl(); let t = ui.get_tt().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let g = ui.get_gr().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let mut fi = t / (n as f64); loop { let mut v: f64 = 0.0; for i in 0..n { v = v + fi * g.powi(i); } if (v - t).abs() \u0026lt; 1e-6 { break; } else { fi = fi - (v - t) * 0.01; } } ui.set_ft(SharedString::from(fi.to_string())); } }); ui.on_calculate_growth_rate({ let ui_handle = ui.as_weak(); move || { let ui = ui_handle.unwrap(); let n = ui.get_nl(); let t = ui.get_tt().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let f = ui.get_ft().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let mut gi = f / t; let mut v = 0.0; while (v - t).abs() \u0026gt; 1e-6 { gi = gi - (v - t) * 0.01; v = 0.0; for i in 0..n { v = v + f * gi.powi(i); } } ui.set_gr(SharedString::from(gi.to_string())); } }); ui.on_calculate_num_layers({ let ui_handle = ui.as_weak(); move || { let ui = ui_handle.unwrap(); let f: f64 = ui.get_ft().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let t = ui.get_tt().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let g = ui.get_gr().to_string().parse::\u0026lt;f64\u0026gt;().unwrap(); let mut i = 1; let mut v = 0.0; loop { v = v + f * g.powi(i); if v \u0026gt; t { break; } i = i + 1; } ui.set_nl(i + 1); } }); ui.run()?; Ok(()) } 最终界面效果如下。根据系统主题颜色不同，显示界面颜色也会有所改变。 生成的release二进制包大小3MB左右。 运行内存在110MB左右。 优点：\n通过声明式语言实现ui界面，甚至可以把一些简单的计算逻辑包含在.slint文件中。 rust原生界面，没有unsafe操作。 支持跨平台跨设备编译，也支持手机和嵌入式开发。 缺点：\n不支持多窗口和文件拖放功能，也不支持文件对话框和消息对话框。 不能调用窗口句柄，也没有提供相关操作方法。 不支持事件控制，对窗口和控件的操作缺乏更细粒度的支持。 2. egui egui 属于eframe 的一部分，目标是打造简单快速又易用的GUI库。eframe是一个跨平台的应用开发框架，支持Windows、Linus、MacOS及Android等多平台应用开发。egui不光支持本地应用开发，甚至可以将代码编译成wasm在浏览器中运行。\ncargo配置文件如下，这里的依赖主要是eframe。\n[package] name = \u0026#34;eLayers-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] eframe = \u0026#34;0.31.1\u0026#34; [profile.release] strip = true opt-level = \u0026#34;z\u0026#34; lto = true codegen-units = 1 panic = \u0026#34;abort\u0026#34; main.rs文件，ui入口包含在eframe的app结构体中。控件组装和按钮的回调函数都是通过闭包实现的。好处是控件和参数关联实时更新，不需要手动更新控件显示。\n#![cfg_attr(not(debug_assertions), windows_subsystem = \u0026#34;windows\u0026#34;)] // hide console window on Windows in release use eframe::egui; fn main() -\u0026gt; eframe::Result { let options = eframe::NativeOptions { viewport: egui::ViewportBuilder::default().with_inner_size([320.0, 240.0]), ..Default::default() }; eframe::run_native( \u0026#34;Layers\u0026#34;, options, Box::new(|_| { // This gives us image support: Ok(Box::\u0026lt;LayersApp\u0026gt;::default()) }), ) } #[derive(PartialEq)] enum Flags { Total, First, Number, Growth, } struct LayersApp { tt: f64, ft: f64, gr: f64, nl: i32, checked: Flags, } impl Default for LayersApp { fn default() -\u0026gt; Self { Self { tt: 3.0, ft: 0.3, gr: 1.5, nl: 5, checked: Flags::Total, } } } impl eframe::App for LayersApp { fn update(\u0026amp;mut self, ctx: \u0026amp;egui::Context, _frame: \u0026amp;mut eframe::Frame) { egui::CentralPanel::default().show(ctx, |ui| { ui.heading(\u0026#34;Layers\u0026#34;); ui.label(\u0026#34;Calculate fluid boundary layer parameters.\\nCalculate the selected parameters based on the others.\u0026#34;); ui.vertical(|ui| { egui::Grid::new(\u0026#34;\u0026#34;) .num_columns(2) .striped(true) .show(ui, |ui| { ui.radio_value(\u0026amp;mut self.checked, Flags::Total, \u0026#34;Total thickness (mm)\u0026#34;); ui.add(egui::DragValue::new(\u0026amp;mut self.tt).max_decimals(6).speed(0.1)); ui.end_row(); ui.radio_value(\u0026amp;mut self.checked, Flags::Number, \u0026#34;Number of layers\u0026#34;); ui.add(egui::DragValue::new(\u0026amp;mut self.nl).speed(1)); ui.end_row(); ui.radio_value(\u0026amp;mut self.checked, Flags::First, \u0026#34;First thickness (mm)\u0026#34;); ui.add(egui::DragValue::new(\u0026amp;mut self.ft).max_decimals(6).speed(0.1)); ui.end_row(); ui.radio_value(\u0026amp;mut self.checked, Flags::Growth, \u0026#34;Growth rate\u0026#34;); ui.add(egui::DragValue::new(\u0026amp;mut self.gr).max_decimals(6).speed(0.1)); ui.end_row(); }); }); if ui.button(\u0026#34;Calculate\u0026#34;).clicked() { match self.checked { Flags::Total =\u0026gt; { let mut v = 0.0; for i in 0..self.nl { v = v + self.ft * self.gr.powi(i); } self.tt = v; } Flags::Number =\u0026gt; { let mut i = 1; let mut v = 0.0; loop { v = v + self.ft * self.gr.powi(i); if v \u0026gt; self.tt { break; } i = i + 1; } self.nl = i + 1; } Flags::First =\u0026gt; { let mut f = self.tt / (self.nl as f64); loop { let mut v: f64 = 0.0; for i in 0..self.nl { v = v + f * self.gr.powi(i); } if (v - self.tt).abs() \u0026lt; 1e-6 { break; } else { f = f - (v - self.tt) * 0.01; } } self.ft = f; } Flags::Growth =\u0026gt; { let mut g = self.ft / self.tt; let mut v = 0.0; while (v - self.tt).abs() \u0026gt; 1e-6 { g = g - (v - self.tt) * 0.01; v = 0.0; for i in 0..self.nl { v = v + self.ft * g.powi(i); } } self.gr = g; } } } }); } } 界面效果如下，和传统的ui界面相比，egui所呈现的风格更类似于网页前端。这里截图显示的系统是浅色主题，如果系统是深色主题，界面背景色会以深色显示。界面偶尔会出现闪烁的情况，因为控件显示和参数关联，可能和界面刷新频率太高有关。 生成的release二进制包大小不到3MB。 运行内存在110MB左右。 优点：\n支持Web和Native应用开发。 rust原生，没有unsafe操作。 支持跨平台。 缺点：\n没有ui工具，无法快速预览ui界面，没有实现界面和实现方法的分离。 不能调用窗口句柄，也不支持事件控制。 开发中，api还不稳定，随时都有可能出现重大调整。 3. fltk-rs fltk 原本是C++开发的一个轻量级的GUI库，fltk-rs 是fltk的rust绑定，底层还是通过C++实现的。fltk的界面实现十分简陋，控件组装都是通过较为原始的坐标进行操作，不支持自动排列组合。虽然提供了丰富的控件，但原生主题效果都是类似于Motif 的极简风，和现代ui界面差距甚远。不过好在fltk-theme 提供了丰富的界面主题用于美化，用户可以在此基础上定制自己的主题风格。\ncargo配置文件如下。这里fltk通过\u0026quot;fltk-bundled\u0026quot;特征下载编译好的lib文件，避免重新编译fltk报错。fltk-rs可以使用fltk提供的界面工具fluid进行ui界面的快速开发和预览，rust下通过fl2rust将fluid生成的.fl文件翻译成.rs文件。\n[package] name = \u0026#34;fLayers-rs\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2024\u0026#34; [dependencies] fltk = { version = \u0026#34;1.5.9\u0026#34;, features = [\u0026#34;fltk-bundled\u0026#34;] } [build-dependencies] fl2rust = \u0026#34;0.7.0\u0026#34; [profile.release] strip = true opt-level = \u0026#34;z\u0026#34; lto = true codegen-units = 1 panic = \u0026#34;abort\u0026#34; ui文件widget.fl，通过fluid生成的，尽量避免手动编辑。\n# data file for the Fltk User Interface Designer (fluid) version 1.0403 header_name {.h} code_name {.cxx} class Widget {open } { Function {make_window()} {open } { Fl_Window window { label Layers open xywh {668 341 360 200} type Double visible } { Fl_Group {} { label {Calculate fluid boundary layer parameters.} open xywh {0 20 360 5} align 5 } {} Fl_Group {} { label {Calculate the selected parameters based on the others.} open xywh {0 40 360 5} align 5 } {} Fl_Round_Button rb_tt { label {Total thickness (mm)} xywh {0 50 170 25} down_box ROUND_DOWN_BOX value 1 } Fl_Value_Input vi_tt { xywh {180 50 170 25} maximum 1e+06 step 0.1 value 3 } Fl_Round_Button rb_nl { label {Number of layers} xywh {0 80 170 25} down_box ROUND_DOWN_BOX } Fl_Value_Input vi_nl { xywh {180 80 170 25} maximum 1e+06 step 1 value 5 } Fl_Round_Button rb_ft { label {First thickness (mm)} xywh {0 110 170 25} down_box ROUND_DOWN_BOX } Fl_Value_Input vi_ft { xywh {180 110 170 25} maximum 1e+06 step 0.1 value 0.3 } Fl_Round_Button rb_gr { label {Growth rate} xywh {0 140 170 25} down_box ROUND_DOWN_BOX } Fl_Value_Input vi_gr {selected xywh {180 140 170 25} maximum 1e+06 step 0.1 value 1.5 } Fl_Button bn_calc { label Calculate xywh {10 170 340 22} } } } } build.rs构建脚本，告诉fl2rust怎么把ui界面翻译成rs文件。\n// build.rs fn main() { use std::env; use std::path::PathBuf; println!(\u0026#34;cargo:rerun-if-changed=ui/widget.fl\u0026#34;); let g = fl2rust::Generator::default(); let out_path = PathBuf::from(env::var(\u0026#34;OUT_DIR\u0026#34;).unwrap()); g.in_out(\u0026#34;ui/widget.fl\u0026#34;, out_path.join(\u0026#34;widget.rs\u0026#34;).to_str().unwrap()) .expect(\u0026#34;Failed to generate rust from fl file!\u0026#34;); } main.rs文件，通过闭包实现控件刷新及按钮功能。\n// src/main.rs // Prevent console window #![cfg_attr(not(debug_assertions), windows_subsystem = \u0026#34;windows\u0026#34;)] #![allow(unused_variables)] #![allow(unused_mut)] #![allow(unused_imports)] #![allow(clippy::needless_update)] use fltk::{prelude::*, *}; include!(concat!(env!(\u0026#34;OUT_DIR\u0026#34;), \u0026#34;/widget.rs\u0026#34;)); fn main() { let app = app::App::default(); show_window(); app.run().unwrap(); } fn show_window() { let mut win = Widget::make_window(); win.rb_tt.deactivate(); let mut ui = win.clone(); win.rb_tt.set_callback(move |_| { if ui.rb_tt.value() { ui.rb_tt.deactivate(); ui.rb_ft.activate(); ui.rb_ft.set_value(false); ui.rb_nl.activate(); ui.rb_nl.set_value(false); ui.rb_gr.activate(); ui.rb_gr.set_value(false); } }); let mut ui = win.clone(); win.rb_ft.set_callback(move |_| { if ui.rb_ft.value() { ui.rb_ft.deactivate(); ui.rb_tt.activate(); ui.rb_tt.set_value(false); ui.rb_nl.activate(); ui.rb_nl.set_value(false); ui.rb_gr.activate(); ui.rb_gr.set_value(false); } }); let mut ui = win.clone(); win.rb_nl.set_callback(move |_| { if ui.rb_nl.value() { ui.rb_nl.deactivate(); ui.rb_tt.activate(); ui.rb_tt.set_value(false); ui.rb_ft.activate(); ui.rb_ft.set_value(false); ui.rb_gr.activate(); ui.rb_gr.set_value(false); } }); let mut ui = win.clone(); win.rb_gr.set_callback(move |_| { if ui.rb_gr.value() { ui.rb_gr.deactivate(); ui.rb_tt.activate(); ui.rb_tt.set_value(false); ui.rb_ft.activate(); ui.rb_ft.set_value(false); ui.rb_nl.activate(); ui.rb_nl.set_value(false); } }); win.vi_tt.set_precision(6); win.vi_ft.set_precision(6); win.vi_nl.set_precision(0); win.vi_gr.set_precision(6); let mut ui = win.clone(); win.bn_calc.set_callback(move |_| { let tt = ui.vi_tt.value(); let ft = ui.vi_ft.value(); let nl = ui.vi_nl.value() as i32; let gr = ui.vi_gr.value(); if ui.rb_tt.value() { let mut v = 0.0; for i in 0..nl { v = v + ft * gr.powi(i); } ui.vi_tt.set_value(v); } else if ui.rb_ft.value() { let mut f = tt / (nl as f64); loop { let mut v: f64 = 0.0; for i in 0..nl { v = v + f * gr.powi(i); } if (v - tt).abs() \u0026lt; 1e-6 { break; } else { f = f - (v - tt) * 0.01; } } ui.vi_ft.set_value(f); } else if ui.rb_nl.value() { let mut i = 1; let mut v = 0.0; loop { v = v + ft * gr.powi(i); if v \u0026gt; tt { break; } i = i + 1; } ui.vi_nl.set_value(i as f64 + 1.0); } else { let mut gi = ft / tt; let mut v = 0.0; while (v - tt).abs() \u0026gt; 1e-6 { gi = gi - (v - tt) * 0.01; v = 0.0; for i in 0..nl { v = v + ft * gi.powi(i); } } ui.vi_gr.set_value(gi); } }) } 界面效果如下。界面配色是固定的，不会随着系统主题变化而改变。 生成的release二进制包只有600多KB大小。 运行内存不到20MB。 优点：\napi较为成熟，提供了简单的事件控制功能，支持提取窗口句柄。 提供了ui工具，可以快速开发和预览界面，支持界面和实现分离。 支持跨平台。 缺点：\n原生界面主题风格太老，不适合现代ui风格。 底层通过C++实现，非rust原生，包含许多unsafe操作。 控件排列组合方式较为原始，没有自动排列控件的组装器。 4. 对比和总结 slint和egui基本可以算得上rust原生GUI，两者内存消耗、生成的二进制包大小基本在同一量级，不过两者ui实现方式和界面效果大相径庭。生态上slint要比egui完善，有ui工具，而且支持界面和实现分离，代价是slint的依赖也更多，构建过程耗时也更长。和传统的C++界面库相比，两者共同的问题是太新功能太少，不支持对显示内容进行更细粒度的控制。像窗口句柄和文件拖放等，虽然有很多办法可以绕开ui库的功能限制来实现，但无形中也增加了学习成本，对开发者来说增加了额外的负担。\nfltk-rs在内存消耗和二进制发布包大小上优于前两者，但底层依赖于C++，不可避免地会用到很多unsafe操作。fltk的发展历程远早于前两者，因此api功能更加完善，对于刚从C/C++转到rust的开发者来说不失为一个好的选择。正是由于fltk的历史比较久远，原生界面风格比较古老，不太符合现代审美风格。而且fltk的控件排列组合方式较为原始，好在有ui工具加持，开发效率算不上太糟糕。\n简而言之：\n从C/C++代码转向rust重构，不追求华丽和现代的界面效果，适合采用fltk-rs 对应用窗口和事件控制有更多要求的，对应用程序资源控制敏感的，适合采用fltk-rs 需要兼顾Web和Native应用开发，适合采用egui 完全采用rust从头开发，追求跨平台和嵌入式功能的，适合采用slint 至于其他rust的界面库，像Tauri、gtk-rs、CXX-qt等，暂时想不出有什么特别的优势，只能说有缘再会吧。\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-06-11-rust_gui/","tags":[{"LinkTitle":"Rust","RelPermalink":"/zh-cn/tags/rust/"}],"title":"Rust图形界面库初探"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/categories/cfd/"}],"content":"虽然STAR-CCM+官方文档专门说明，Windows下不支持 FORTRAN1。但实际上，只要编译器支持，在Windows下使用Fortran编译出来的用户程序一样能正常在STAR-CCM+中正常加载运行。\n1. 构建CMake工程 首先我们参考官方文档当中的教程案例2，编写CMake项目，项目结构如下：\nSTARCCM_FORTRAN_SAMPLE │ CMakeLists.txt\t# CMake配置文件 │ README.md\t# 说明文件，非必须 ├───.vscode │ launch.json\t# 启动调试模式时自动生成的文件，非必须 │ settings.json\t# 定义CMake相关变量 └───src initVelocity.f StarReal.f.in sutherlandViscosity.f uflib.f zeroGradT.f CMake配置文件CMakeLists.txt主要内容如下：\ncmake_minimum_required(VERSION 3.10) # Project name project(UserFortran LANGUAGES Fortran) set(CMAKE_Fortran_STANDARD 2008) # Check for STARCCM_USER_LIB_DIR if(NOT DEFINED STARCCM_USER_LIB_DIR) message(FATAL_ERROR \u0026#34;STARCCM_USER_LIB_DIR is not defined. Please specify the path to the STAR-CCM+ UserFunctions library directory.\u0026#34;) # For example, in Windows : C:/Program Files/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/win64/clang17.0vc14.2-r8/lib # In Linux : /opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/gnu11.4-r8/lib\u0026#34; else() message(STATUS \u0026#34;STARCCM_USER_LIB_DIR location : \u0026#34; ${STARCCM_USER_LIB_DIR}) endif() # Check for STARCCM_STD_LIB_DIR if(NOT DEFINED STARCCM_STD_LIB_DIR) message(STATUS \u0026#34;STARCCM_STD_LIB_DIR undefined. using system standard library. \u0026#34;) # For example, in Linux : /opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64\u0026#34; else() message(STATUS \u0026#34;STARCCM_STD_LIB_DIR location : \u0026#34; ${STARCCM_STD_LIB_DIR}) endif() # STAR-CCM+ output precision if(USE_DOUBLE_PRECISION) message(STATUS \u0026#34;Using double precision for STAR-CCM+\u0026#34;) set(STAR_REAL \u0026#34;1D0\u0026#34;) else() message(STATUS \u0026#34;Using float precision for STAR-CCM+\u0026#34;) set(STAR_REAL \u0026#34;1.0\u0026#34;) endif() # generate the StarReal.f file configure_file(src/StarReal.f.in ${CMAKE_BINARY_DIR}/StarReal.f @ONLY) # Include directories include_directories(${PROJECT_SOURCE_DIR}/include ) # Link with STARCCM LIB directory link_directories(${STARCCM_USER_LIB_DIR} ${STARCCM_STD_LIB_DIR} ) # Specify the source files set(SOURCES ${CMAKE_BINARY_DIR}/StarReal.f src/initVelocity.f src/sutherlandViscosity.f src/zeroGradT.f src/uflib.f ) # Add library add_library(${CMAKE_PROJECT_NAME} SHARED ${SOURCES} ) # Link library target_link_libraries(${CMAKE_PROJECT_NAME} UserFunctions) # Install target install(TARGETS ${CMAKE_PROJECT_NAME} RUNTIME DESTINATION bin LIBRARY DESTINATION lib ARCHIVE DESTINATION lib/static ) 其中有两个关键设置：\n通过STARCCM_USER_LIB_DIR和STARCCM_STD_LIB_DIR指定STAR-CCM+自带的库文件搜索路径，其中STARCCM_STD_LIB_DIR是非必须的，用于在Linux下指定链接到STAR-CCM+自带的标准库文件 通过USE_DOUBLE_PRECISION指定精度，在配置文件StarReal.f.in基础上生成StarReal.f，给编译器指定StarReal类型精度 StarReal.f.in同官方案例的StarReal.f，只是将原来的数值改为了@STAR_REAL@标记，用来告诉CMake如何替换字符：\nmodule StarRealMod integer, parameter :: StarInt = kind(1) integer, parameter :: StarReal = kind(@STAR_REAL@) integer, parameter :: CoordReal = kind(1D0) integer, parameter :: StarIntSize = StarInt integer, parameter :: StarRealSize = StarReal integer, parameter :: CoordRealSize = CoordReal end module StarRealMod zeroGradT.f定义了一个边界配置函数：\nC Set boundary temperature equal to cell temperature subroutine zeroGradT(result,size,fc,T) use StarRealMod implicit none integer, intent(in) :: size real(StarReal), intent(out) :: result(size) integer, intent(in) :: fc(2,*) real(StarReal), intent(in) :: T(*) integer i C Loop through all entities applying T_boundary = T_cell C fc(1,i) is the cell next to i do i = 1,size result(i) = T(fc(1,i)) end do return end initVelocity.f定义了一个区域配置函数：\nC Initial velocity based on uniform swirl subroutine initVelocity(result,size,centroid) use StarRealMod implicit none integer, intent(in) :: size real(StarReal), intent(out) :: result(3,size) real(CoordReal), intent(in) :: centroid(3,*) integer i real(CoordReal) dr(3) C Angular velocity and origin of rotation real(CoordReal), parameter :: omega(3) = (/0.0,0.0,100.0/) real(CoordReal), parameter :: origin(3) = (/0.0,0.0,0.0/) C Loop through all entities applying u = omega x (centroid - origin) do i = 1,size dr(1) = centroid(1,i) - origin(1) dr(2) = centroid(2,i) - origin(2) dr(3) = centroid(3,i) - origin(3) result(1,i) = omega(2)*dr(3) - omega(3)*dr(2) result(2,i) = omega(3)*dr(1) - omega(1)*dr(3) result(3,i) = omega(1)*dr(2) - omega(2)*dr(1) end do return end sutherlandViscosity.f定义了一个场函数：\nC Dynamic viscosity based on Sutherland\u0026#39;s law subroutine sutherlandViscosity(result,size,T) use StarRealMod implicit none integer, intent(in) :: size real(StarReal), intent(out) :: result(size) real(StarReal), intent(in) :: T(*) integer i C Reference viscosity, Sutherland constant and reference temperature real(StarReal), parameter :: v0 = 1.716E-5 real(StarReal), parameter :: Cs = 110.0 real(StarReal), parameter :: T0 = 273.15 C Loop through all entities applying Sutherland\u0026#39;s law do i = 1,size result(i) = v0 * (T(i)/T0)**1.5 * (T0 + Cs)/(T(i) + Cs) end do return end uflib.f作为导入函数，用于注册上面用户定义的函数：\nsubroutine uflib() use StarRealMod implicit none C Register user functions here external zeroGradT,initVelocity,sutherlandViscosity call uffunc(zeroGradT, \u0026#34;BoundaryProfile\u0026#34;, \u0026amp; \u0026#34;Zero Gradient Temperature\u0026#34;) call ufarg (zeroGradT, \u0026#34;Face\u0026#34;, \u0026amp; \u0026#34;FaceCellIndex\u0026#34;, 2*StarIntSize) call ufarg (zeroGradT, \u0026#34;Cell\u0026#34;, \u0026amp; \u0026#34;Temperature\u0026#34;, StarRealSize) call uffunc(initVelocity, \u0026#34;RegionProfile\u0026#34;, \u0026amp; \u0026#34;Initial Velocity\u0026#34;) call ufarg(initVelocity, \u0026#34;Cell\u0026#34;, \u0026amp; \u0026#34;Centroid\u0026#34;, 3*CoordRealSize) call uffunc(sutherlandViscosity, \u0026#34;ScalarFieldFunction\u0026#34;, \u0026amp; \u0026#34;Sutherland Viscosity\u0026#34;) call ufarg(sutherlandViscosity, \u0026#34;Cell\u0026#34;, \u0026amp; \u0026#34;Temperature\u0026#34;, StarRealSize) return end 官方文档自带的Fortran代码都是固定格式，不知是不是排版的原因，代码前6个空格没有正确显示，复制粘贴的时候一定要注意。\n2. Windows下编译动态库 Windwos下使用msys2的clang64工具链编译，编译器是LLVM flang。如果VSCode上没有找到该选项，可以使用CMake拓展的扫描工具包功能扫描msys2的clang64目录找到该选项。 编译输出动态库之后，在clang64工具链下用ldd命令扫描该动态库的依赖，除了系统自带的动态库之外，没有链接到其他第三方动态库。 如果用msys2的mingw64或者ucrt64工具链编译，编译器一般都是gfortran，编译出来的动态库文件最好用ldd扫描一遍，确保不会多出一堆第三方动态库依赖。\n使用STAR-CCM+加载，可以正常识别，编译语言显示是Fortran。 3. Linux下编译动态库 Linux下通过以下命令编译fortran动态链接库：\n# 进入项目目录 cd starccm_fortran_sample # 建立工作目录 mkdir -p build \u0026amp;\u0026amp; cd build # 配置编译文件，链接STAR-CCM+库 cmake .. -G \u0026#34;Ninja\u0026#34; -DUSE_DOUBLE_PRECISION=ON \\ -DSTARCCM_USER_LIB_DIR=${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/gnu11.4-r8/lib \\ -DSTARCCM_STD_LIB_DIR=${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64 # 编译动态库 cmake --build . --config Release # 安装 cmake --install . --prefix $PWD/../../UserLib 使用ldd扫描生成的动态库文件，没有其他第三方动态库依赖。 使用STAR-CCM+加载，可以正常识别。 4. 计算案例演示 接下来用一个STAR-CCM+案例演示3下，看Fortran编写的用户程序能否载入STAR-CCM+正常计算。\n模拟充分发展的圆管层流，圆管长3米、直径0.2米，一端为进气口，另一端为出口。物性参数和入口条件如下所示：\n密度：1.0 kg/m^3 粘度：2×10^-3 N-s/m^2 管道层流入口充分发展满足下面的关系：uum=2[1−(rr0)2] 其中： um 表示入口的平均速度， r0 表示管径 建立一个二维轴对称模型。 新建CMake项目，CMakeLists.txt文件基本和前面的内容一样，只改动下面部分：\n# Specify the source files set(SOURCES ${CMAKE_BINARY_DIR}/StarReal.f src/parabolicVelocity.f src/uflib.f ) 源码文件只有3个，其中StarReal.f.in内容和前面的一样。入口速度的实现在parabolicVelocity.f文件中，内容如下所示：\nC Initial velocity subroutine parabolicVelocity(result,size,centroid) use StarRealMod implicit none integer, intent(in) :: size real(StarReal), intent(out) :: result(size) real(CoordReal), intent(in) :: centroid(3,*) integer i real(CoordReal) radius real(CoordReal), parameter :: R = 0.1 real(CoordReal), parameter :: origin(3) = (/0.0,0.0,0.0/) C Loop through all entities applying uniform velocity do i = 1,size radius = sqrt((centroid(1,i)- origin(1))**2 + \u0026amp; (centroid(2,i) - origin(2))**2 + \u0026amp; (centroid(3,i) - origin(3))**2) result(i) = 2* (1-(radius/R)*(radius/R)) end do return end 注册函数所在的uflib.f文件内容如下所示：\nsubroutine uflib() use StarRealMod implicit none external parabolicVelocity C Register user functions here call uffunc(parabolicVelocity, \u0026#34;BoundaryProfile\u0026#34;, \u0026amp; \u0026#34;Parabolic Velocity\u0026#34;) call ufarg(parabolicVelocity, \u0026#34;Face\u0026#34;, \u0026amp; \u0026#34;Centroid\u0026#34;, 3*CoordRealSize) return end Windows下使用msys2的clang64工具链编译，输出动态链接库文件。载入用户程序： 入口条件选择用户程序，函数选择上面定义的函数parabolicVelocity。 计算完成输出云图。 入口处速度分布。 使用用户程序 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFortran 用户接口参考 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSTAR-CCM+二次开发——User Code \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-30-use-vscode-develop-starccm-user-library-by-fortran/","tags":[{"LinkTitle":"Fortran","RelPermalink":"/zh-cn/tags/fortran/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/zh-cn/tags/star-ccm+/"}],"title":"使用VSCode开发STAR-CCM+用户程序：通过Fortran构建动态库"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/categories/cfd/"}],"content":"STAR-CCM+用户程序(用户库)1是通过外部编译器(通常是C/C++，也支持Fortran)按一定的规则编译构建的动态链接库，将构建好的动态链接库注册到sim文件中，可以实现某些自定义功能。一个用户程序通常包含一个或多个用户自定义的函数，这些自定义函数一般用于实现特殊的配置或场函数。\n用户程序需要先注册动态链接库才能调用，而动态链接库的格式往往与操作系统、硬件平台等密切相关，因此用户程序通常都是针对特定平台进行编译，无法实现跨平台运行。\n本文尝试用VSCode编写STAR-CCM+动态链接库，利用开源的热力学库CoolProp为STAR-CCM+拓展物性参数计算功能；通过CMake构建用户程序，确保在不同平台下能正确生成动态链接库文件。\n1. CoolProp介绍 CoolProp 是一个开源跨平台的热力学库，定位上类似于NIST的REFPROP，包含各种流体物性参数，同时支持多种编程语言，也支持通过MATLAB、Excel甚至javascript调用进行物性参数计算2。\nWindows平台通过以下命令下载编译、安装CoolProp3：\n# 推荐使用PowerShell，需要先安装cmake、git及clang-cl编译器 # 下载CoolProp源码及其依赖 git clone https://github.com/CoolProp/CoolProp --recursive cd CoolProp # 建立工作目录 mkdir build \u0026amp;\u0026amp; cd build # 配置静态库编译文件 cmake .. -DCOOLPROP_STATIC_LIBRARY=ON -G \u0026#34;Visual Studio 17 2022\u0026#34; -T ClangCL,host=x64 -A x64 # 编译静态库 cmake --build . --config Release # 安装静态库 cmake --install . --prefix $PWD/../../CoolPropLib Linux平台通过以下命令编译、安装CoolProp：\n# 需要先安装cmake、git及gcc编译器 # 下载CoolProp源码及其依赖 git clone https://github.com/CoolProp/CoolProp --recursive cd CoolProp # 建立工作目录 mkdir -p build \u0026amp;\u0026amp; cd build # 配置静态库编译文件，默认架构为64位 cmake .. -DCOOLPROP_STATIC_LIBRARY=ON -DCOOLPROP_FPIC=ON -G \u0026#34;Ninja\u0026#34; # 编译静态库，使用32个核心加速编译 cmake --build . --config Release -- -j32 # 安装静态库 cmake --install . --prefix $PWD/../../CoolPropLib 得益于软件良好的封装，CoolProp相关函数的调用十分简单，以下是官方示例代码4：\n#include \u0026#34;CoolProp.h\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;stdlib.h\u0026gt; using namespace CoolProp; int main() { // First type (slowest, due to most string processing, exposed in DLL) std::cout \u0026lt;\u0026lt; PropsSI(\u0026#34;Dmolar\u0026#34;, \u0026#34;T\u0026#34;, 298, \u0026#34;P\u0026#34;, 1e5, \u0026#34;Propane[0.5]\u0026amp;Ethane[0.5]\u0026#34;) \u0026lt;\u0026lt; std::endl; // Default backend is HEOS std::cout \u0026lt;\u0026lt; PropsSI(\u0026#34;Dmolar\u0026#34;, \u0026#34;T\u0026#34;, 298, \u0026#34;P\u0026#34;, 1e5, \u0026#34;HEOS::Propane[0.5]\u0026amp;Ethane[0.5]\u0026#34;) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; PropsSI(\u0026#34;Dmolar\u0026#34;, \u0026#34;T\u0026#34;, 298, \u0026#34;P\u0026#34;, 1e5, \u0026#34;REFPROP::Propane[0.5]\u0026amp;Ethane[0.5]\u0026#34;) \u0026lt;\u0026lt; std::endl; // Vector example std::vector\u0026lt;double\u0026gt; z(2, 0.5); // Second type (C++ only, a bit faster, allows for vector inputs and outputs) std::vector\u0026lt;std::string\u0026gt; fluids; fluids.push_back(\u0026#34;Propane\u0026#34;); fluids.push_back(\u0026#34;Ethane\u0026#34;); std::vector\u0026lt;std::string\u0026gt; outputs; outputs.push_back(\u0026#34;Dmolar\u0026#34;); std::vector\u0026lt;double\u0026gt; T(1, 298), p(1, 1e5); std::cout \u0026lt;\u0026lt; PropsSImulti(outputs, \u0026#34;T\u0026#34;, T, \u0026#34;P\u0026#34;, p, \u0026#34;\u0026#34;, fluids, z)[0][0] \u0026lt;\u0026lt; std::endl; // Default backend is HEOS std::cout \u0026lt;\u0026lt; PropsSImulti(outputs, \u0026#34;T\u0026#34;, T, \u0026#34;P\u0026#34;, p, \u0026#34;HEOS\u0026#34;, fluids, z)[0][0] \u0026lt;\u0026lt; std::endl; // Comment me out if REFPROP is not installed std::cout \u0026lt;\u0026lt; PropsSImulti(outputs, \u0026#34;T\u0026#34;, T, \u0026#34;P\u0026#34;, p, \u0026#34;REFPROP\u0026#34;, fluids, z)[0][0] \u0026lt;\u0026lt; std::endl; // All done return return EXIT_SUCCESS; } 正确情况下，以上示例代码的输出结果：\n40.8269 40.8269 40.8269 40.8269 40.8269 40.8269 CoolProp还支持Python调用，可以作为计算器使用，方便地进行各种工质物性参数计算：\nIn [1]: from CoolProp.CoolProp import PropsSI In [2]: PropsSI(\u0026#39;T\u0026#39;,\u0026#39;P\u0026#39;,101325,\u0026#39;Q\u0026#39;,0,\u0026#39;Water\u0026#39;) Out[2]: 373.1242958476844 In [3]: H_V = PropsSI(\u0026#39;H\u0026#39;,\u0026#39;P\u0026#39;,101325,\u0026#39;Q\u0026#39;,1,\u0026#39;Water\u0026#39;) In [4]: H_L = PropsSI(\u0026#39;H\u0026#39;,\u0026#39;P\u0026#39;,101325,\u0026#39;Q\u0026#39;,0,\u0026#39;Water\u0026#39;) In [5]: H_V - H_L Out[5]: 2256471.5924066794 2. 编写STAR-CCM+用户程序 首先新建一个VSCode工作目录，文件结构如下：\nstarccm_coolprop\t# 工作目录名称 │ CMakeLists.txt\t# CMake配置文件 │ README.md\t# 说明文件，帮助其他用户更好地了解项目，非必须 ├───.vscode\t# VSCode的配置文件目录 │ launch.json\t# 启动调试模式时自动生成的文件，非必须 │ settings.json\t# 定义CMake相关变量 ├───include\t# 头文件包含目录 │ heos.h\t# CoolProp实现相关头文件 │ uclib.h\t# STAR-CCM+用户程序定义头文件 └───src\t# 源代码目录 heos.cpp\t# CoolProp实现相关源代码 uclib.cpp\t# STAR-CCM+用户程序链接源代码 为了方便跨平台，构建工具采用CMake，编辑CMake配置文件CMakeLists.txt内容如下：\n# 定义构建项目所需最小CMake版本 cmake_minimum_required(VERSION 3.10) # 项目名称及开发语言 project(upcp LANGUAGES CXX) # 编译器需满足C++17规范 set(CMAKE_CXX_STANDARD 17) # 检查COOLPROP_SRC_DIR定义，用于搜索CoolProp源码文件 if(NOT DEFINED COOLPROP_SRC_DIR) message(FATAL_ERROR \u0026#34;COOLPROP_SRC_DIR is not defined. Please specify the path to the CoolProp source directory.\u0026#34;) else() message(STATUS \u0026#34;COOLPROP_SRC_DIR location : \u0026#34; ${COOLPROP_SRC_DIR}) endif() # 检查COOLPROP_LIB_DIR定义，用于搜索预编译的CoolProp库文件 if(NOT DEFINED COOLPROP_LIB_DIR) message(FATAL_ERROR \u0026#34;COOLPROP_LIB_DIR is not defined. Please specify the path to the CoolProp library directory.\u0026#34;) # For example: [CoolPropLib_Dir]/static_library/[platform]/[architecture]_[compiler]_[version] else() message(STATUS \u0026#34;COOLPROP_LIB_DIR location : \u0026#34; ${COOLPROP_LIB_DIR}) endif() # 检查STARCCM_LIB_DIR，用于链接到STAR-CCM+的UserFunctions库文件 if(NOT DEFINED STARCCM_LIB_DIR) message(FATAL_ERROR \u0026#34;STARCCM_LIB_DIR is not defined. Please specify the path to the STAR-CCM+ UserFunctions library directory.\u0026#34;) # For example, in Windows : C:/Program Files/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/win64/clang17.0vc14.2-r8/lib # In Linux : /opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/gnu11.4-r8/lib\u0026#34; else() message(STATUS \u0026#34;STARCCM_LIB_DIR location : \u0026#34; ${STARCCM_LIB_DIR}) endif() # 定义STAR-CCM+软件精度 if(USE_DOUBLE_PRECISION) add_definitions(-DDOUBLE_PRECISION) message(STATUS \u0026#34;Using double precision for STAR-CCM+\u0026#34;) else() message(STATUS \u0026#34;Using float precision for STAR-CCM+\u0026#34;) endif() # 使用CoolProp静态库还是动态库 if(USE_SHARED_COOLPROP) add_definitions(-DCOOLPROP_LIB) message(STATUS \u0026#34;Using shared CoolProp library\u0026#34;) else() message(STATUS \u0026#34;Using static CoolProp library\u0026#34;) endif() # 头文件包含目录 include_directories(${PROJECT_SOURCE_DIR}/include ${COOLPROP_SRC_DIR}/include ${COOLPROP_SRC_DIR}/externals/fmtlib/include ) # 添加STAR-CCM+和CoolProp的库文件目录 link_directories(${STARCCM_LIB_DIR} ${COOLPROP_LIB_DIR} ) # 生成动态链接库 add_library(${CMAKE_PROJECT_NAME} SHARED src/uclib.cpp src/heos.cpp ) # 链接到STAR-CCM+和CoolProp的库文件 target_link_libraries(${CMAKE_PROJECT_NAME} UserFunctions CoolProp) # 安装目标 install(TARGETS ${CMAKE_PROJECT_NAME} RUNTIME DESTINATION bin LIBRARY DESTINATION lib ARCHIVE DESTINATION lib/static ) heos.h定义了3个用户函数，分别是空气的密度、粘度和导热率函数，代码如下：\n#ifndef __HEOS_H__ #define __HEOS_H__ void USERFUNCTION_EXPORT airDensity(CoordReal *, int, CoordReal *, CoordReal *); void USERFUNCTION_EXPORT airViscosity(CoordReal *, int, CoordReal *, CoordReal *); void USERFUNCTION_EXPORT airConductivity(CoordReal *, int, CoordReal *, CoordReal *); #endif /* __HEOS_H__ */ heos.cpp是这3个用户函数的实现，通过输入压力和温度，调用CoolProp库计算输出空气的密度、粘度和导热率。\n#include \u0026#34;CoolProp.h\u0026#34; #include \u0026#34;AbstractState.h\u0026#34; #include \u0026#34;crossplatform_shared_ptr.h\u0026#34; //以上3个头文件包含在CoolProp源代码的include目录里 #include \u0026#34;uclib.h\u0026#34; #include \u0026#34;heos.h\u0026#34; /* 通过输入压力和温度，调用CoolProp计算输出空气密度 */ void USERFUNCTION_EXPORT airDensity(CoordReal *result, int size, CoordReal *P, CoordReal *T) { /* 为了加快迭代速度，这里用到了CoolProp的低级接口，避免反复调用字符串处理函数 */ /* 使用智能指针初始化流体参数 */ shared_ptr\u0026lt;CoolProp::AbstractState\u0026gt; heos(CoolProp::AbstractState::factory(\u0026#34;HEOS\u0026#34;, \u0026#34;Air\u0026#34;)); /* 遍历所有单元 */ for (int i = 0; i != size; ++i) { /* 从STAR-CCM+获取的压力是表压，需要手动转换成绝对压力 */ heos-\u0026gt;update(CoolProp::PT_INPUTS, P[i] + 101325.0, T[i]); // 默认为SI单位制 result[i] = heos-\u0026gt;rhomass(); // 单位：kg/m^3 } } /* 通过输入压力和温度，调用CoolProp计算输出空气粘度 */ void USERFUNCTION_EXPORT airViscosity(CoordReal *result, int size, CoordReal *P, CoordReal *T) { shared_ptr\u0026lt;CoolProp::AbstractState\u0026gt; heos(CoolProp::AbstractState::factory(\u0026#34;HEOS\u0026#34;, \u0026#34;Air\u0026#34;)); for (int i = 0; i != size; ++i) { heos-\u0026gt;update(CoolProp::PT_INPUTS, P[i] + 101325.0, T[i]); result[i] = heos-\u0026gt;viscosity(); // 单位：Pa-s } } /* 通过输入压力和温度，调用CoolProp计算输出空气导热率 */ void USERFUNCTION_EXPORT airConductivity(CoordReal *result, int size, CoordReal *P, CoordReal *T) { shared_ptr\u0026lt;CoolProp::AbstractState\u0026gt; heos(CoolProp::AbstractState::factory(\u0026#34;HEOS\u0026#34;, \u0026#34;Air\u0026#34;)); for (int i = 0; i != size; ++i) { heos-\u0026gt;update(CoolProp::PT_INPUTS, P[i] + 101325.0, T[i]); result[i] = heos-\u0026gt;conductivity(); // 单位：W/m-K } } uclib.h头文件是STAR-CCM+链接库的头文件5，定义了UserFunctions库所使用的变量和函数类型。该文件属于通用型头文件。\n#ifndef UCLIB_H #define UCLIB_H #ifdef DOUBLE_PRECISION typedef double Real; #else typedef float Real; #endif typedef double CoordReal; #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif #if defined(WIN32) || defined(_WINDOWS) || defined(_WINNT) #define USERFUNCTION_EXPORT __declspec(dllexport) #define USERFUNCTION_IMPORT __declspec(dllimport) #else #define USERFUNCTION_EXPORT #define USERFUNCTION_IMPORT #endif extern void USERFUNCTION_IMPORT ucarg(void *, char *, char *, int); extern void USERFUNCTION_IMPORT ucfunc(void *, char *, char *); extern void USERFUNCTION_IMPORT ucfunction(void *, char *, char *, int, ...); void USERFUNCTION_EXPORT uclib(); #ifdef __cplusplus } #endif #endif uclib.cpp用于注册用户函数，使得STAR-CCM+能够正确识别动态链接库，并将其加载为用户程序。\n#include \u0026#34;uclib.h\u0026#34; #include \u0026#34;heos.h\u0026#34; void USERFUNCTION_EXPORT uclib() { /* 将airDensity注册为场函数 */ /* 这里使用了reinterpret_cast\u0026lt;\u0026gt;关键字，用于提示C++强制类型转换，避免编译器报错 */ ucfunc(reinterpret_cast\u0026lt;void*\u0026gt;(airDensity), \u0026#34;ScalarFieldFunction\u0026#34;, \u0026#34;HEOS::Air Density (kg/m^3)\u0026#34;); // 自定义场函数默认无量纲，最好在名称后面加上单位 ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airDensity), \u0026#34;Cell\u0026#34;, \u0026#34;Pressure\u0026#34;, sizeof(CoordReal)); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airDensity), \u0026#34;Cell\u0026#34;, \u0026#34;Temperature\u0026#34;, sizeof(CoordReal)); /* 将airViscosity注册为场函数 */ ucfunc(reinterpret_cast\u0026lt;void*\u0026gt;(airViscosity), \u0026#34;ScalarFieldFunction\u0026#34;, \u0026#34;HEOS::Air Viscosity (Pa-s)\u0026#34;); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airViscosity), \u0026#34;Cell\u0026#34;, \u0026#34;Pressure\u0026#34;, sizeof(CoordReal)); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airViscosity), \u0026#34;Cell\u0026#34;, \u0026#34;Temperature\u0026#34;, sizeof(CoordReal)); /* 将airConductivity注册为场函数 */ ucfunc(reinterpret_cast\u0026lt;void*\u0026gt;(airConductivity), \u0026#34;ScalarFieldFunction\u0026#34;, \u0026#34;HEOS::Air Thermal conductivity (W/m-K)\u0026#34;); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airConductivity), \u0026#34;Cell\u0026#34;, \u0026#34;Pressure\u0026#34;, sizeof(CoordReal)); ucarg(reinterpret_cast\u0026lt;void*\u0026gt;(airConductivity), \u0026#34;Cell\u0026#34;, \u0026#34;Temperature\u0026#34;, sizeof(CoordReal)); } 这个项目新建了3个场函数，通过ucfunc函数将用户函数注册为STAR-CCM+的用户自定义场函数，再通过ucarg将STAR-CCM+的场函数(或变量)注册为用户函数的输入参数，这样调用用户自定义场函数时，就能自动调用压力和温度更新输出变量。\n除了用户自定义场函数之外，用户程序还支持用户自定义的边界条件及区域配置，具体可以参考官方文档6。\n3. 编译动态链接库 3.1 通过VSCode+CMake编译项目 如果VSCode中安装了CMake拓展 ，可以直接调用CMake编译生成动态链接库文件。\n首先要在settings.json文件中定义CMake调用STAR-CCM+和CoolProp相关库和头文件的搜索路径。\n{ \u0026#34;cmake.configureArgs\u0026#34;: [ \u0026#34;-DSTARCCM_LIB_DIR=[starccm_install_dir]/STAR-CCM+19.06.009-R8/star/lib/win64/clang17.0vc14.2-r8/lib\u0026#34;, \u0026#34;-DCOOLPROP_SRC_DIR=E:/Development/starccm/CoolProp\u0026#34;, \u0026#34;-DCOOLPROP_LIB_DIR=E:/Development/starccm/CoolPropLib/static_library/Windows/64bit_Clang_19.1.1\u0026#34;, \u0026#34;-DUSE_DOUBLE_PRECISION=ON\u0026#34; ] } 通过VSCode左侧的CMake按钮，调出CMake配置面板，选择构建工具链(这里选择clang-cl)和编译模式(Debug或Release)。 输出窗口当中显示CMake配置信息。 通过Ctrl+Shift+P快捷键调出命令面板，输入CMake: Build生成项目。 生成完毕，输出窗口显示生成的动态链接库路径。 将它拷贝到STAR-CCM+工程目录，通过图形界面加载，加载成功后可以看到动态链接库信息。 在场函数里也能看到生成的用户自定义场函数。 调用用户自定义场函数更新标量场并显示。 3.2 通过CMake+命令行编译项目 由于STAR-CCM+用户程序依赖于操作系统平台，不同操作系统编译的动态链接库不能通用。当工程文件需要用到超算进行计算时，需要将用户程序源码文件上传到超算上重新编译。不同的超算平台Linux发行版不一样，系统依赖也有所区别，建议计算前先在所使用的超算平台上编译一份专门在该平台上运行的动态链接库，后续再在该平台上调用编译的动态链接库进行相关计算。\nLinux平台下，通过以下命令编译用户程序动态链接库。\n# 进入项目目录 cd starccm_coolprop # 建立工作目录 mkdir -p build \u0026amp;\u0026amp; cd build # 配置编译文件，链接到CoolProp动态库 cmake .. -G \u0026#34;Ninja\u0026#34; -DUSE_DOUBLE_PRECISION=ON \\ -DCOOLPROP_LIB_DIR=${HOME}/Share/code/CoolPropLib/static_library/Linux/64bit_GNU_13.3.0 \\ -DCOOLPROP_SRC_DIR=${HOME}/Share/code/CoolProp \\ -DSTARCCM_LIB_DIR=${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/gnu11.4-r8/lib # 编译动态库 cmake --build . --config Release # 安装 cmake --install . --prefix $PWD/../../UserLib 编译完成后输出动态链接库，通过图形界面加载。 自动生成的用户自定义场函数。 通过用户自定义场函数更新云图。 4. 常见问题 4.1 动态链接库的依赖问题 如果编译配置使用的是CoolProp动态库，最终输出的动态链接库会依赖CoolProp动态库文件，需要将CoolProp.dll(Linux下为libCoolProp.so.X)所在文件夹添加到系统环境变量(PATH或LD_LIBRARY_PATH)中。为了省掉这个麻烦，本文演示所采用的都是CoolProp静态库，编译出来的动态链接库不依赖CoolProp动态库文件，但是不排除某些工程仍然有需要链接到CoolProp动态库的。\n另一个常见的依赖问题是，在Windows平台下使用GCC编译器(MinGW)进行编译时，不管CoolProp链接的是静态库还是动态库，最终输出的动态链接库都需要链接到MinGW自带的libstdc++-6.dll、libgcc_s_seh-1.dll和libwinpthread-1.dll这3个文件(不同的MinGW版本可能有所区别)。因此最好将MinGW的运行时路径添加到系统环境变量中，确保STAR-CCM+能搜索到所有依赖并正常调用动态链接库中的函数。\n如果在Linux上运行依赖其他动态链接库，启动STAR-CCM+时要在命令行中加入-ldlibpath选项，将依赖的动态库文件所在路径添加到该关键字后面。\nstarccm+ -ldlibpath [path-to--shared-file] 4.2 GLIBCXX版本问题 STAR-CCM+自带了一套C++标准库，如果用户Linux系统自带的C++标准库比STAR-CCM+自带的版本新，那么编译输出的动态链接库在载入STAR-CCM+时会输出如下错误信息：\nLoading user library /home/xxxx/Share/code/UserLib/lib/libupcp.so Could not load user library /home/xxxx/Share/code/UserLib/lib/libupcp.so /home/xxxx/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64/libstdc++.so.6: version `GLIBCXX_3.4.32\u0026#39; not found (required by /home/xxxx/Share/code/UserLib/lib/libupcp.so) 因为STAR-CCM+自带的C++标准链接库，GLIBCXX版本比系统上的版本低。 编译时，如果不作特别指定，编译器默认链接的时系统上的链接库。 而在载入STAR-CCM+时，载入动态链接库会和STAR-CCM+自带的标准库产生链接，从而报错。\n一个解决办法是，编译时通过LD_LIBRARY_PATH显式指定链接库搜索路径为STAR-CCM+自带路径。\nexport LD_LIBRARY_PATH=${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64:$LD_LIBRARY_PATH 但这种做法会导致系统工具链出现问题，无法正常开展编译工作。 另一个解决办法是，编辑CMakeLists.txt文件，增加以下内容，通过link_directories关键字设置链接库搜索路径。\n# Link with STARCCM CXX LIB link_directories( \u0026#34;${HOME}/opt/Siemens/19.06.009-R8/STAR-CCM+19.06.009-R8/star/lib/linux-x86_64-2.28/system/gnu11.4-64\u0026#34; ) 然后按照3.2 的方法正常编译。查看编译的动态链接库依赖，链接到STAR-CM+自带的库，问题解决了。 4.3 计算效率问题 我们建立一个基础的STAR-CCM+模型，说明一下计算效率问题。\n新建一个sim文件，二维轴对称问题，模拟空气在管路中加热情况。 基础模型状态方程采用理想气体，比热容为常数，导热率和动力粘度基于Sutherland’s Law。 网格量3629。4个核心并行求解3000步，耗时85秒。 然后我们把物理模型中的理想气体换成用户定义的EOS，加载编译好的动态链接库，通过动态链接库生成的用户定义场函数定义气体密度、动力粘度和导热率，比热容为常数保持不变。 网格量不变。同样4个核心并行求解3000步，耗时235秒，增加了近2倍时长。 问题出现在我们注册的用户函数上。因为每个函数每一步计算都要从CoolProp中调取函数、遍历网格单元获取温度和压力数值、再通过CoolProp函数计算输出结果。每个函数内部都通过循环执行以上操作，循环操作独立与求解器迭代计算本身，网格量越大，循环执行的时间就越长。况且我们还定义了3个相同功能的函数，相当于在求解器迭代计算之外又额外执行了3次循环操作。\n对于某些较大的模型以及对求解时间比较敏感的项目，一般不建议通过用户程序+CoolProp的方式定义物性参数。但是后处理方面，通过调用CoolProp程序更新云图、输出结果，还是十分有用的。\n使用用户程序 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAvailable Wrappers \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStatic Library \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC++ Sample Code \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n类型定义(C) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n用户程序示例 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-29-use-vscode-develop-starccm-user-library-with-coolprop/","tags":[{"LinkTitle":"C++","RelPermalink":"/zh-cn/tags/c++/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/zh-cn/tags/star-ccm+/"}],"title":"使用VSCode开发STAR-CCM+用户程序：通过CoolProp计算物性参数"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/categories/cfd/"}],"content":"STAR-CCM+模拟助手(Simulation Assistant)相当于一个插件，在插件内部封装了java宏命令。使用的时候，通过调用宏命令，执行一些流程性的操作。相比于直接执行java宏文件，可以更好地与用户进行交互，对不熟悉java的用户来说明显更加友好。因为模拟助手大多应用于前后处理当中，许多咨询公司针对这方面开发的定制化插件也更倾向于采用相同的方式。\n官方文档采用NetBeans演示了创建模拟助手项目的流程，但是演示的NetBeans版本已经很老旧了。官方文档更新缓慢，描述又过于笼统。这里采用VSCode重新实现并演示一遍，相关配置可以参考使用VSCode调试STAR-CCM+宏 ，以下演示材料来自于STAR-CCM+官方的教程文件。\n1. 建立模拟助手项目 参考使用VSCode调试STAR-CCM+宏 中的内容建立java项目，项目类型一样选择No build tools，名称根据自己需要起一个。\n项目创建完成后，编辑[项目文件夹]/.vscode下的settings.json文件，将下面这些目录添加到项目依赖中1：\n[STAR-CCM+_Installation]/star/lib/java/platform/core [STAR-CCM+_Installation]/star/lib/java/platform/core/locale [STAR-CCM+_Installation]/star/lib/java/platform/lib [STAR-CCM+_Installation]/star/lib/java/platform/modules [STAR-CCM+_Installation]/star/lib/java/platform/modules/ext [STAR-CCM+_Installation]/star/lib/java/platform/modules/locale 因为模拟助手需要加载到STAR-CCM+中，需要保证两者jdk版本一致。编辑[项目文件夹]/.vscode下的settings.json文件，增加一行设置，指定jdk为STAR-CCM+自带的版本。\n\u0026#34;java.jdt.ls.java.home\u0026#34;: \u0026#34;[STAR-CCM+_Installation]/jdk/[platform]/jdk[version]\u0026#34; 在JAVA PROJECTS里确认jdk版本，确保和STAR-CCM+自带的版本一致。 2. 项目开发 将自动生成的[项目文件夹]/src下面的源文件删除，创建文件夹Assistant，将官方教程中的java源文件复制粘贴到其中2。 在[项目文件夹]/src下面创建文件夹XHTML文件夹，将官方教程中的XHTML文件复制粘贴到其中。 接下来按照官方教程3的指导，补充完整其他部分。\n整个模拟助手项目非常简单，基本操作就分为6步：\n导入几何文件 建立分析域 建立分析物理 生成体网格 建立显示场景 设置求解步数并运行求解。 每一步的操作对应一个java文件，每一个java文件对应一个XHTML。java文件类似于前面说过的宏，可以采用录制宏+复制粘贴的方式快速编辑。XHTML文件则提供用户交互说明和操作入口，允许用户根据提示调用相应的操作命令。 在编写自己的项目文件时，建议在官方提供的教程文件基础上进行修改，将自己录制的宏片段复制粘贴到其中并进行编辑，尽量符合原始文件的模板要求。具体的编写规则，可以参考官方文档4。\n3. 发布和测试 模拟助手可以通过jar打包的形式发布，使用VSCode JAVA PROJECTS的功能打包输出jar文件。 选择bin目录即可，不要重复打包其他依赖项。 打包完成的jar文件在项目文件夹中。 在STAR-CCM+中新建或加载sim文件之后，通过文件菜单加载模拟助手。 运行完整测试。 因为教程案例比较老了，很多API在新版本中已经弃用，存在一些报错，需要自己调整代码。\n4. 调试模拟助手 模拟助手的调试和使用VSCode调试STAR-CCM+宏 中java宏的调试过程一摸一样，唯一不同的是加载方式从\u0026quot;播放宏\u0026quot;变成了\u0026quot;加载模拟助手\u0026quot;。\n项目文件夹的.vscode目录下launch.json文件代码如下：\n{ \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Debug (Attach)\u0026#34;, \u0026#34;projectName\u0026#34;: \u0026#34;starccm\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;hostName\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 8765 } ] } 通过命令行方式启动STAR-CCM+主程序：\n\u0026lt;InstallationDirectory\u0026gt;/star/bin/starccm+ -jvmargs \u0026#39;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8765\u0026#39; 在进入调试、开始执行之前先在源码中增加断点。等代码执行到断点，VSCode中会提示中断，可以在中断处检查变量。 需要注意的是，如果是通过jar打包发布模拟助手，在进入调试之前要确保jar打包的版本和当前源码一致。\n创建 NetBeans 项目 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n将 Java 包、Java 类和 XHTML 文件添加到模拟助手项目 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n模拟助手：管道内流模拟辅助 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n开发模拟助手 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-22-use-vscode-coding-starccm-assistant/","tags":[{"LinkTitle":"Java","RelPermalink":"/zh-cn/tags/java/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/zh-cn/tags/star-ccm+/"}],"title":"使用VSCode开发STAR-CCM+模拟助手"},{"categories":[],"content":"Hi，我是Andrew Moa，一个工程师，曾服务于国内制冷一线企业和龙头车企，曾担任研究员、仿真工程师。\n工作联系： Andrew.Moa2005@163.com ","permalink":"https://andrewmoa.site/zh-cn/about/","tags":[],"title":"关于"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/categories/cfd/"}],"content":"前面讲过STAR-CCM+宏文件的录制和编写，宏文件的本质就是java文件，因此可以用java编程的方法来对它进行开发和调试。如果涉及到复杂的业务场景，需要增加额外的功能，程序本身比较复杂，很难等到整个程序编写完成后再对它进行测试，免不了要在开发过程中进行调试。官方文档采用的开发工具是古早版本的NetBeans，很多功能已经发生变化，加之官方文档描述过于简略，大多数人阅读完后对于STAR-CCM+的调试过程还是一头雾水。VSCode 作为新兴IDE的佼佼者，不仅可以通过拓展支持java编程，还可以通过copilot 拓展集成强大的AI编程能力，本文便采用VSCode演示一下STAR-CCM+宏文件的调试过程。\n1. VSCode配置 首先要在VSCode中安装支持java的拓展，至少要安装下面几个：\nLanguage Support for Java(TM) by Red Hat Debugger for Java Project Manager for Java 也可以直接安装这个拓展包，一次性把所有需要用到的java拓展都装齐了：Extension Pack for Java 下载一个JDK并安装，如果不想下载JDK的话，也可以在STAR-CCM+安装路径中找到安装包附带的JDK，把它添加到环境变量中。 2. 建立java项目 在VSCode命令面板(Ctrl+Shift+P)中输入Java: Create Java Project，创建一个新的java项目。 项目类型选择No build tools。 在弹出对话框选择项目文件夹位置，然后输入项目名称(例如：starccm)，新建项目完成，可以看到java项目的结构。 将[STAR-CCM+_Installation]/star/lib/java/platform/modules/ext文件夹复制到新建项目的lib文件夹中1。 将之前录制或编写的java宏复制到项目生成的java文件中，可以看到语法检查和代码高亮提示已经生效了，这时候修改代码就能通过自动完成补充代码片段，也可以通过Ctrl+I调用copilot自动填写和纠正代码错误。 3. 配置调试 在项目文件夹的.vscode目录下修改launch.json文件，编辑代码如下。\n{ \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Debug (Attach)\u0026#34;, \u0026#34;projectName\u0026#34;: \u0026#34;starccm\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;hostName\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 8765 } ] } 如果.vscode下面没有launch.json这个文件，可以点击VSCode左侧的运行与调试按钮，根据提示生成该文件，然后按照上面的内容进行编辑。 通过命令行方式启动STAR-CCM+主程序，注意要在starccm+后面附加启动参数，address后面的端口号要与上面launch.json文件中填写的端口号一致。\n\u0026lt;InstallationDirectory\u0026gt;/star/bin/starccm+ -jvmargs \u0026#39;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8765\u0026#39; 上面配置完成之后，接下来就可以在STAR-CCM+中正常打开或者新建sim文件，用于测试我们编写的java程序。\n接着点击VSCode左侧的运行与调试按钮，进入调试模式。成功进入调试模式后，会在调式页面中显示调用的堆栈和断点信息。 如果进入调试模式失败，会弹出对话框提醒。需要确认STAR-CCM+有没有正常启动、有没有附加启动参数，以及launch.json文件编写有没有错误。 4. 调试过程 下面编写一个简单的java文件，用于演示调试过程。\n// 下面这一行是录制时自动生成的，没什么作用，可以注释掉。 // package macro; import star.common.*; public class testCCMDebug extends StarMacro { // 宏操作入口 public void execute() { pringMsg(); } // 演示打印消息的函数 private void pringMsg() { Simulation simulation_0 = getActiveSimulation(); simulation_0.println(\u0026#34;Hello, starccm+!\u0026#34;); simulation_0.println(\u0026#34;This is a macro example.\u0026#34;); simulation_0.println(\u0026#34;This macro is used to test STAR-CCM+ debug mode.\u0026#34;); String fileName = simulation_0.getPresentationName() + \u0026#34;.sim\u0026#34;; simulation_0.println(\u0026#34;The simulation file name is: \u0026#34; + fileName); } } 然后在其中加入断点。 关键的地方来了2，一定要在STAR-CCM+中使用“播放宏”加载java文件。等执行到断点时，会在VSCode中提示中断，这样就可以通过VSCode控制调试的执行进程。 最终执行效果： 调试执行到断点，过程中STAR-CCM+会进入暂停状态，这时候点击取消按钮（进度条旁边红色的X）是没有用的。 VSCode调试执行完成之后，想要再次进入调试，需要在STAR-CCM+中使用\u0026quot;播放宏\u0026quot;重新加载java文件。\n使用 IDE 调试宏 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n学习STAR-CCM+编程语言：在Eclipse中进行二次开发调试 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-18-use-vscode-debug-starccm-marco/","tags":[{"LinkTitle":"Java","RelPermalink":"/zh-cn/tags/java/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/zh-cn/tags/star-ccm+/"}],"title":"使用VSCode调试STAR-CCM+宏"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/categories/cfd/"}],"content":"STAR-CCM+宏本质上就是一个java文件，语法和普通的java没有什么区别。使用宏可以帮助我们简化处理过程，尤其是一些重复的流程性的操作。通过编写宏文件完成一些流程操作可以极大地解放人力资源，甚至可以在求解过程中完成某些特定的操作。\n1. 录制宏 STAR-CCM+宏本质就是分析过程中的各种操作命令的集合，与其从头开始讲解复杂繁琐的java语法+API，不如直接从工程问题上手。\nSTAR-CCM+宏操作的入口在左上角工具栏里，从左到右依次是“播放宏”、“开始录制”、“暂停录制”和“停止录制”按钮，相关操作选项也可以在“文件”菜单中找到。 点击“开始录制”会弹出对话框，提示宏的保存位置。如果过程中用到一些场景化的图形操作，可以勾选上“包括图形命令”；但一般情况下不建议勾选，尤其是涉及到超算提交的场合。 接下来就可以在输出窗口中看到录制java过程的代码，此时可以正常地执行分析操作，STAR-CCM+会把相关操作转换成对应的java代码，并显示在输出窗口中。 当分析操作完成后点击“停止录制”，可以在输出窗口中看到完整的java代码，同时该代码也被保存到java文件中。\n下面是录制的宏代码示例，作用是将原有网格清除并重新划分体网格。\n// Simcenter STAR-CCM+ macro: reGenerateMesh.java // Written by Simcenter STAR-CCM+ 19.06.009 // 上面两行注释是录制时生成的，记录保存宏的文件名、STAR-CCM+版本信息 package macro; import java.util.*; import star.common.*; import star.meshing.*; // 类名和文件名要一致 public class reGenerateMesh extends StarMacro { //这个函数是整个宏的入口\tpublic void execute() { execute0(); } //这个函数执行网格清除和划分操作 private void execute0() { Simulation simulation_0 = getActiveSimulation(); MeshPipelineController meshPipelineController_0 = simulation_0.get(MeshPipelineController.class); //清除原来的网格 meshPipelineController_0.clearGeneratedMeshes(); //生成体网格 meshPipelineController_0.generateVolumeMesh(); } } 具体代码的作用可以查看STAR-CCM+的帮助文件，不过大部分的API的命名都比较直观，根据其命名也能推测出来。 如果要执行的操作比较复杂，流程很多。建议将流程进行分解，分别录制不同java文件，再将其组合起来。 如果录制的操作需要运行求解器，可以换一个简单点的模型进行录制，也可以把参数调小一点，节省时间和资源。\n2. 编写宏 有了上面录制的java文件，下面的编写就简单多了。\n接下来将这些模块组合起来，编写一个新的java宏文件，作用是在读取sim文件之后自动划分网格，设置参数并运行求解，求解完成后输出相应的绘图和场景文件，并保存成指定文件名的结果文件。\n这里建议使用IDE工具，借助语法检查、代码高亮提示和自动完成可以减少出错的几率，对初学者十分友好。STAR-CCM+官方教程和帮助文件里演示的是NetBeans，也有人喜欢用Eclipse或IntelliJ IDEA，在这里区别都不大，根据个人喜好选择吧。\n// Simcenter STAR-CCM+ macro: meshAndRun.java // Written by Simcenter STAR-CCM+ 19.06.009 package macro; import java.util.*; import star.common.*; import star.meshing.*; import star.base.neo.*; import star.vis.*; public class meshAndRun extends StarMacro { // 分析的输入参数 double env_temp = 25.0; // Unit: C double fan_speed = 2450.0; // Unit: rpm // 一些设置参数 boolean autoSave = true; int maxStep = 10; int autoSaveStep = 1000; boolean saveAsResultsFile = true; // 这里最好写完整路径，否则Windows下会默认保存到${HOME}目录 String resultsFileName = \u0026#34;final_results.sim\u0026#34;; // 宏操作入口 public void execute() { // 执行分析操作 if (autoSave) { enableAutoSave(); } else { disableAutoSave(); } generateMesh(); modifyParameters(); modifyMaxStep(); run(); exportPlot(); exportScene(); if (saveAsResultsFile) { saveAs(); } } // 生成体网格 private void generateMesh() { Simulation simulation_0 = getActiveSimulation(); MeshPipelineController meshPipelineController_0 = simulation_0.get(MeshPipelineController.class); // meshPipelineController_0.clearGeneratedMeshes(); meshPipelineController_0.generateVolumeMesh(); } // 设置自动保存 private void enableAutoSave() { Simulation simulation_0 = getActiveSimulation(); AutoSave autoSave_0 = simulation_0.getSimulationIterator().getAutoSave(); autoSave_0.setAutoSaveBatch(true); autoSave_0.setAutoSaveMesh(true); AutoSaveFileSet autoSaveFileSet_0 = ((AutoSaveFileSet) autoSave_0.getAutoSaveFileSetManager() .getObject(\u0026#34;Auto Save File Set 1\u0026#34;)); StarUpdate starUpdate_0 = autoSaveFileSet_0.getStarUpdate(); IterationUpdateFrequency iterationUpdateFrequency_0 = starUpdate_0.getIterationUpdateFrequency(); IntegerValue integerValue_0 = iterationUpdateFrequency_0.getIterationFrequencyQuantity(); integerValue_0.getQuantity().setValue(autoSaveStep); starUpdate_0.setEnabled(true); } // 取消自动保存 private void disableAutoSave() { Simulation simulation_0 = getActiveSimulation(); AutoSave autoSave_0 = simulation_0.getSimulationIterator().getAutoSave(); autoSave_0.setAutoSaveMesh(false); autoSave_0.setAutoSaveBatch(false); } // 设置分析的输入参数 private void modifyParameters() { Simulation simulation_0 = getActiveSimulation(); ScalarGlobalParameter scalarGlobalParameter_0 = ((ScalarGlobalParameter) simulation_0 .get(GlobalParameterManager.class).getObject(\u0026#34;env_temp\u0026#34;)); Units units_0 = ((Units) simulation_0.getUnitsManager().getObject(\u0026#34;C\u0026#34;)); scalarGlobalParameter_0.getQuantity().setValueAndUnits(env_temp, units_0); ScalarGlobalParameter scalarGlobalParameter_1 = ((ScalarGlobalParameter) simulation_0 .get(GlobalParameterManager.class).getObject(\u0026#34;fan_speed\u0026#34;)); Units units_1 = ((Units) simulation_0.getUnitsManager().getObject(\u0026#34;rpm\u0026#34;)); scalarGlobalParameter_1.getQuantity().setValueAndUnits(fan_speed, units_1); } // 设置最大步数 private void modifyMaxStep() { Simulation simulation_0 = getActiveSimulation(); StepStoppingCriterion stepStoppingCriterion_0 = ((StepStoppingCriterion) simulation_0 .getSolverStoppingCriterionManager().getSolverStoppingCriterion(\u0026#34;Maximum Steps\u0026#34;)); IntegerValue integerValue_0 = stepStoppingCriterion_0.getMaximumNumberStepsObject(); integerValue_0.getQuantity().setValue(maxStep); } // 运行求解器 private void run() { Simulation simulation_0 = getActiveSimulation(); // ResidualPlot residualPlot_0 = ((ResidualPlot) // simulation_0.getPlotManager().getPlot(\u0026#34;Residuals\u0026#34;)); // residualPlot_0.openInteractive(); simulation_0.getSimulationIterator().run(); } // 输出绘图 private void exportPlot() { Simulation simulation_0 = getActiveSimulation(); MonitorPlot monitorPlot_0 = ((MonitorPlot) simulation_0.getPlotManager().getPlot(\u0026#34;mass_flow\u0026#34;)); // 这里可以用相对路径，也可以用绝对路径 monitorPlot_0.export(resolvePath(\u0026#34;mass_flow.csv\u0026#34;), \u0026#34;,\u0026#34;); } // 输出场景 private void exportScene() { Simulation simulation_0 = getActiveSimulation(); Scene scene_0 = simulation_0.getSceneManager().getScene(\u0026#34;Geometry\u0026#34;); scene_0.export3DSceneFileAndWait(resolvePath(\u0026#34;Geometry.sce\u0026#34;), \u0026#34;Geometry\u0026#34;, \u0026#34;\u0026#34;, false, SceneFileCompressionLevel.OFF); } // 保存结果 private void saveAs() { Simulation simulation_0 = getActiveSimulation(); simulation_0.saveState(resultsFileName); } } 3. 运行宏 编写完成后可以尝试运行以下，看看有没有报错。尝试运行求解的话可以把求解步数调小一点，看输出的文件是否正确。更改条件和参数，多测试几次，经过完整测试没有报错才可以用于生产环境。\n如果要提交超算运行，应在-batch开关后附加java宏文件路径1。\nstarccm+ [path-to-sim-file] -batch [path-to-java-file] -np [number-of-threads] ... 编制应用程序脚本 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-17-starccm-record-and-write-marco/","tags":[{"LinkTitle":"Java","RelPermalink":"/zh-cn/tags/java/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/zh-cn/tags/star-ccm+/"}],"title":"STAR-CCM+宏文件录制和编写"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"最近在用OpenFOAM和SU2提交计算时，反复出现\u0026quot;Authorization required, but no authorization protocol specified\u0026quot;报错信息。尽管不理会它最终还是能完成计算，但总是出现的报错信息让人心里不安。\n怀疑是OpenMPI的问题，通过以下命令验证：\nmpirun -np 2 hostname 果然出现报错信息： 翻阅网上信息，比较靠谱的解决方案是这个1：在Slurm脚本中增加以下环境变量：\nexport HWLOC_COMPONENTS=-gl 测试一下，报错问题解决了： github issue \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-16-solve-openmpi-warning/","tags":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Openfoam","RelPermalink":"/zh-cn/tags/openfoam/"},{"LinkTitle":"Su2","RelPermalink":"/zh-cn/tags/su2/"}],"title":"解决OpenMPI的\"Authorization required, but no authorization protocol specified\"错误提示"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"近期发现一个问题，Abaqus和CFX无法在Linux挂载的Windows共享文件夹上进行计算。Linux是虚拟机部署的，在虚拟机本地路径上启动计算程序，虽然不会出现报错，但会导致虚拟磁盘占用空间增大，并且对读写效率也有一定的影响。\n1. SMB版本问题 1.1 SMBv3 之前用cifs挂载共享文件夹的时候，没有仔细关注SMB协议的版本1。以下命令行没有指定协议版本：\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o uid=xxx,gid=xxx,username=xxx 用mount命令查看，默认连接协议是3.1版本： 在3.1版本协议的挂载目录下运行Abaqus程序，无法计算，报错如下：\nmpirun: Warning one or more remote shell commands exited with non-zero status, which may indicate a remote access problem. driverExceptions.AbaqusExecutionError: (\u0026#39;SIMULIA Job Layout Engine\u0026#39;, 255, \u0026#39;abaqus_test\u0026#39;) !!! !!! SIM wrap utility command error: !!! System Error open: 无效的参数 !!! driverExceptions.AbaqusExecutionError: (\u0026#39;SIM Wrap-up\u0026#39;, 1, \u0026#39;abaqus_test\u0026#39;) 在3.1版本协议的挂载目录下运行CFX求解器，无法计算，报错如下：\nAn error has occurred in cfx5solve: Error copying MMS input file mms for reading: Input/output error 1.2 SMBv2 在挂载命令行中增加vers=2.0关键字，显式指定协议版本为2.0：\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o vers=2.0,uid=xxx,gid=xxx,username=xxx 用mount命令查看确认协议版本是2.0： 在2.0版本协议的挂载目录下运行Abaqus程序，可以完成计算，但存在报错：\nmpirun: Warning one or more remote shell commands exited with non-zero status, which may indicate a remote access problem. driverExceptions.AbaqusExecutionError: (\u0026#39;SIMULIA Job Layout Engine\u0026#39;, 255, \u0026#39;abaqus_test\u0026#39;) 在2.0版本协议的挂载目录下运行CFX求解器，还是无法计算，报错信息和SMBv3版本一样。\n1.3 SMBv1 指定协议版本为1.0：\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o vers=1.0,uid=xxx,gid=xxx,username=xxx mount确认： 在1.0版本协议的挂载目录下运行Abaqus程序，和SMBv2版本一样，可以完成计算但存在报错信息。\n在1.0版本协议的挂载目录下运行CFX求解器，无法计算，报错信息如下：\nAn error has occurred in cfx5solve: Error copying MMS input file mms for reading: Operation not supported 2. SMB符号链接问题 2.1 CFX环境变量 查看Ansys官方说明2，CFX报错多半是符号链接的问题，在slurm计算脚本中增加以下环境变量：\nexport CFX5_DISABLE_SYMLINKS=1 在SMBv1~SMBv3版本协议中测试，增加该环境变量后，CFX均可以完成计算，问题算是解决了。\n2.2 SMBv3启用符号链接功能 在挂载命令中增加符号链接选项mfsymlinks3：\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o mfsymlinks,uid=xxx,gid=xxx,username=xxx 实测效果跟2.1 增加CFX环境变量一样，可以解决CFX无法计算的问题。但Abaqus依旧报错且无法完成计算，看来Abaqus的问题和符号链接无关。\n3. 总结 如果需要在Linux挂载的Windows共享文件夹上运行Abaqus和CFX求解器，建议使用以下命令显式指定SMBv2版本并启用符号链接功能：\nmount -t cifs //172.25.64.1/Share /home/dell/Share -o vers=2.0,mfsymlinks,uid=xxx,gid=xxx,username=xxx 以上可以保证CFX、Abaqus正常计算。\n在 Windows 中检测、启用和禁用 SMBv1、SMBv2 和 SMBv3 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA local Linux machine is connected to a Windows machine through Samba. \u0026hellip; \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n挂载SMB协议文件系统 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-15-linux-mount-windows-dir-cannot-run-abaqus-and-cfx/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/zh-cn/tags/abaqus/"},{"LinkTitle":"Cfx","RelPermalink":"/zh-cn/tags/cfx/"},{"LinkTitle":"Ubuntu","RelPermalink":"/zh-cn/tags/ubuntu/"}],"title":"Linux挂载Windows共享文件夹无法运行Abaqus和CFX求解器的问题"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"Ansa联合Abaqus分析经常遇到螺栓施加预紧力问题，除了设置复杂外还容易踩坑，一不小心就会遇到各种各样的问题。下面记录一下Ansa螺栓预紧力设置过程和一些坑，为后面相关分析作参考。\n1. Solid+Assistant 实体螺栓和法兰面之间要先建立连接，具体可以参考Abaqus接触设置 。 单个螺栓可以通过AUXILIARIES→PRTENS→Assistant向导进行设置。 在弹出向导中选择Surface - Solid Elements或者Surface - Solid Property，然后选择螺栓实体。 勾选下面的Detect and create all possible pretensions.，会尝试自动搜索类似实体并创建相同的预紧力。 下一步，在螺栓实体上选择第一个参考点。 下一步，选择第二个参考点定义螺栓预紧力方向，此时会在第一个参考点生成参考平面。注意预紧力方向要与螺栓轴线平行。 下一步，输入预紧力大小50kN，注意勾选后面的Fixed。这里会自动生成两个STEP。第一个STEP用于加载预紧力，第二个STEP用于固定预紧力状态。其他的载荷可以施加到第二个STEP里面。 最后，定义参考平面的法向，确认应用螺栓预紧力。 以上步骤完成后就可以提交计算了。\n容易踩坑的地方：\n涉及到接触问题导致的不收敛，可以加上接触控制Contact Control→Stabilize，提高求解的稳定性。 如果加上接触控制还是不收敛，建议将实体螺栓和法兰面之间的接触改用Coupling约束替代之。 如果计算结果出现螺栓变长的情况，多半是第6步参考平面的法向定义反了，更改参考面法线方向即可。 计算完成后通过Meta打开，可以查看实体螺栓内部的应力分布状态。 2. Beam+Assistant 当要研究的问题不涉及螺栓本体强度时，可采用梁单元替代螺栓实体，减少计算量提高计算效率的同时还能避免出现计算不收敛的问题。\n梁单元需要先新建梁的PID，先定义螺栓的截面和材料。截面形式选圆形，定义螺栓半径。 需要通过Beam新建梁单元（建议先通过螺栓孔新建两个Coupling约束，再通过Coupling约束的中心新建Beam单元），再指派到上面的PID里，此时Beam单元只有一段。 然后转到Mesh，通过Insert插入点的方式，将Beam单元分割为多段。 接下来启动向导，选择Beam Elements，选择中间的一段Beam单元。如果上一步没有分割Beam单元，这里可以选择下面的Split Beams，会自动将Beam单元分割为3段并选中中间的一段。 同样，输入预紧力50kN，选择固定预紧力状态。 确认创建预紧力。 以上步骤完成后可以提交计算。\n常见问题：\nMeta不支持显示Beam单元应力分布。如果要查看Beam单元应力分布状态，需要通过Abaqus Viewer来渲染结果文件。\n需要在View→ODB Display Options里打开Render beam profiles才能显示梁单元的状态。 梁单元的应力显示要选择BEAM_STRESS。 想在结果中查看梁的应力分布状态，建议在*OUTPUT关键字选择输出所有变量，这样用Abaqus Viewer渲染的时候才能查看BEAM_STRESS。 3. Beam+Connection Connection设置可以参考Ansa快速设置连接 。通过Connection也可以设置螺栓预紧力，尤其适合多个螺栓连接、需要经常替换模型或调整参数的场合。这里螺栓本体选择梁单元。\n建立PID的步骤同上一章节中的第2步。Body Type选择CBEAM，激活Create Pretension并输入预紧力大小。 多个螺栓应力显示： 4. 总结 Solid+Assistant ： 螺栓本体应力结果相对更准确，但计算量较大，容易出现不收敛情况。 Beam+Assistant ：计算量少，更容易收敛，但只能用在不考察螺栓本体强度的场合。 Beam+Connection ：计算量少、容易收敛，更改、替换操作更方便，同样不能直接考察螺栓本体强度。 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-03-ansa-abaqus-bolt-pretension/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/zh-cn/tags/abaqus/"},{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Ansa联合Abaqus分析螺栓预紧力设置"},{"categories":[{"LinkTitle":"Code","RelPermalink":"/zh-cn/categories/code/"}],"content":"经常写Markdown的小伙伴都会遇到一个问题，那就是图片存储问题。Markdown本身并不支持图片内置，传统用法一般通过外链引用本机或网络上的图片。但是当文章在网络上发表时，存储图片就成了个问题，虽然可以通过图床上传，但是操作麻烦。\n好在Markdown支持通过base641编码数据渲染图片，这里根据这个功能编写了一个小工具Image2Base64 ，可以方便地在图片文件和base64编码之间转换。\n1. 软件功能 下面就是这个软件的图形界面，这个截图就是通过base64渲染的。 软件界面左侧窗口是图片显示窗口，可以通过拖放打开图片，支持bmp、png、jpg等主流格式的图片(暂不支持gif动图显示)，支持右键菜单，支持图片放大缩小和重置等基本操作。\n左侧窗口下方的Paste按钮可以将剪贴板中的图像数据读取并显示出来，方便通过截图软件和剪贴板交互读取图像数据。\n右侧窗口是文本显示窗口，如果转换成功这里会图片对应的base64编码并显示出来。下方的Markdown复选框用于添加Markdown的图片语法标签，点击Copy按钮后可以将文本数据复制到剪贴板，之后就可以直接在MD文件中粘贴了。\n值得注意的是，Copy按钮左侧的组合框可以选择base64对应的图片格式。没错，base64也是有对应格式区分的，base64本质上是将二进制编码的文本化，因此不同图片原始格式的大小所导致的base64编码长度自然不一样，甚至图片的复杂程度和压缩比等因素对base64编码长度都有影响。\n右侧窗口下方的Save as按钮支持将图片文件以原始格式或base64编码(txt文件)存储。通过中间的两个方向按钮可以实现将图片转换成base64编码，或者将base64编码转换成图片。需要注意的是，将base64编码转换成图片时，请不要包含markdown的语法标签，否则会报错。\n该软件经过测试，可以使用png格式base64编码，通过markdown语法标签在marktext 和joplin 上渲染图片，其他格式不保证能成功，取决于markdown编辑器的渲染引擎。对于一般屏幕截图，推荐采用png格式。\n2. 代码解析 2.1 C++实现 该软件基于Qt6实现，图片格式编码、渲染以及base64转换都是通过Qt实现的。\n图片显示通过QLabel实现，重载了QLabel类2，并做了一些调整。\nphotolabel.h：\n#ifndef PHOTOLABEL_H #define PHOTOLABEL_H #include \u0026lt;QObject\u0026gt; #include \u0026lt;QLabel\u0026gt; #include \u0026lt;QMenu\u0026gt; #include \u0026lt;QDragEnterEvent\u0026gt; #include \u0026lt;QDropEvent\u0026gt; class PhotoLabel : public QLabel { Q_OBJECT public: explicit PhotoLabel(QWidget* parent = nullptr); void openFile(QString); //打开图片文件 void clearShow(); //清空显示 void setImage(QImage\u0026amp;); //设置图片 void openAction(); //调用打开文件对话框 void pasteAction(); //粘贴来自剪贴板的图片 const QImage\u0026amp; getImage(); //调用存储的图片数据 signals: // 图片加载成功信号 void imageLoadSuccess(); protected: void contextMenuEvent(QContextMenuEvent* event) override; //右键菜单 void paintEvent(QPaintEvent* event) override; //QPaint画图 void wheelEvent(QWheelEvent* event) override; //鼠标滚轮滚动 void mousePressEvent(QMouseEvent* event) override; //鼠标摁下 void mouseMoveEvent(QMouseEvent* event) override; //鼠标松开 void mouseReleaseEvent(QMouseEvent* event) override; //鼠标发射事件 //拖放文件 void dragEnterEvent(QDragEnterEvent* event) override; void dragMoveEvent(QDragMoveEvent* event) override; void dropEvent(QDropEvent* event) override; private slots: void initWidget(); //初始化 void onSelectImage(); //选择打开图片 void onPasteImage(); //选择粘贴图片 void onZoomInImage(); //图片放大 void onZoomOutImage(); //图片缩小 void onPresetImage(); //图片还原 private: QImage m_image; //显示的图片 qreal m_zoomValue = 1.0; //鼠标缩放值 int m_xPtInterval = 0; //平移X轴的值 int m_yPtInterval = 0; //平移Y轴的值 QPoint m_oldPos; //旧的鼠标位置 bool m_pressed = false; //鼠标是否被摁压 QString m_localFileName; //文件名称 QMenu* m_menu; //右键菜单 }; #endif // PHOTOLABEL_H photolabel.cpp：\n#include \u0026#34;photolabel.h\u0026#34; #include \u0026lt;QPainter\u0026gt; #include \u0026lt;QDebug\u0026gt; #include \u0026lt;QWheelEvent\u0026gt; #include \u0026lt;QFileDialog\u0026gt; #include \u0026lt;QClipboard\u0026gt; #include \u0026lt;QApplication\u0026gt; #include \u0026lt;QMimeData\u0026gt; #include \u0026lt;QFileInfo\u0026gt; #include \u0026lt;QMessageBox\u0026gt; PhotoLabel::PhotoLabel(QWidget* parent) :QLabel(parent) { initWidget(); } void PhotoLabel::initWidget() { //初始化右键菜单 m_menu = new QMenu(this); QAction* loadImage = new QAction; loadImage-\u0026gt;setText(tr(\u0026#34;\u0026amp;Open new...\u0026#34;)); connect(loadImage, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onSelectImage); m_menu-\u0026gt;addAction(loadImage); QAction* pasteImage = new QAction; pasteImage-\u0026gt;setText(tr(\u0026#34;\u0026amp;Paste image\u0026#34;)); connect(pasteImage, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onPasteImage); m_menu-\u0026gt;addAction(pasteImage); m_menu-\u0026gt;addSeparator(); QAction* zoomInAction = new QAction; zoomInAction-\u0026gt;setText(tr(\u0026#34;Zoom in \u0026amp;+\u0026#34;)); connect(zoomInAction, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onZoomInImage); m_menu-\u0026gt;addAction(zoomInAction); QAction* zoomOutAction = new QAction; zoomOutAction-\u0026gt;setText(tr(\u0026#34;Zoom out \u0026amp;-\u0026#34;)); connect(zoomOutAction, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onZoomOutImage); m_menu-\u0026gt;addAction(zoomOutAction); QAction* presetAction = new QAction; presetAction-\u0026gt;setText(tr(\u0026#34;\u0026amp;Reset\u0026#34;)); connect(presetAction, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::onPresetImage); m_menu-\u0026gt;addAction(presetAction); m_menu-\u0026gt;addSeparator(); /* QAction *clearAction = new QAction; clearAction-\u0026gt;setText(\u0026#34;Clear\u0026#34;); connect(clearAction, \u0026amp;QAction::triggered, this, \u0026amp;PhotoLabel::clearShow); m_menu-\u0026gt;addAction(clearAction); */ } void PhotoLabel::openFile(QString path) { if (path.isEmpty()) { return; } if (!m_image.load(path)) { QMessageBox::warning(this, tr(\u0026#34;Error\u0026#34;), tr(\u0026#34;Cannot load file!\u0026#34;)); return; } m_zoomValue = 1.0; m_xPtInterval = 0; m_yPtInterval = 0; m_localFileName = path; emit this-\u0026gt;imageLoadSuccess(); update(); } void PhotoLabel::clearShow() { m_localFileName = \u0026#34;\u0026#34;; m_image = QImage(); this-\u0026gt;clear(); } void PhotoLabel::setImage(QImage\u0026amp; img) { if (img.isNull()) { return; } m_zoomValue = 1.0; m_xPtInterval = 0; m_yPtInterval = 0; m_localFileName = \u0026#34;\u0026#34;; m_image = img.copy(0, 0, img.width(), img.height()); emit imageLoadSuccess(); update(); } void PhotoLabel::openAction() { this-\u0026gt;onSelectImage(); } void PhotoLabel::pasteAction() { this-\u0026gt;onPasteImage(); } const QImage\u0026amp; PhotoLabel::getImage() { return m_image; } void PhotoLabel::paintEvent(QPaintEvent* event) { if (m_image.isNull()) return QWidget::paintEvent(event); QPainter painter(this); // 根据窗口计算应该显示的图片的大小 int width = qMin(m_image.width(), this-\u0026gt;width()); int height = int(width * 1.0 / (m_image.width() * 1.0 / m_image.height())); height = qMin(height, this-\u0026gt;height()); width = int(height * 1.0 * (m_image.width() * 1.0 / m_image.height())); // 平移 painter.translate(this-\u0026gt;width() / 2 + m_xPtInterval, this-\u0026gt;height() / 2 + m_yPtInterval); // 缩放 painter.scale(m_zoomValue, m_zoomValue); // 绘制图像 QRect picRect(-width / 2, -height / 2, width, height); painter.drawImage(picRect, m_image); QWidget::paintEvent(event); } void PhotoLabel::wheelEvent(QWheelEvent* event) { int value = event-\u0026gt;angleDelta().y() / 15; if (value \u0026lt; 0) //放大 onZoomInImage(); else //缩小 onZoomOutImage(); update(); } void PhotoLabel::mousePressEvent(QMouseEvent* event) { m_oldPos = event-\u0026gt;pos(); m_pressed = true; this-\u0026gt;setCursor(Qt::ClosedHandCursor); //设置鼠标样式 } void PhotoLabel::mouseMoveEvent(QMouseEvent* event) { if (!m_pressed) return QWidget::mouseMoveEvent(event); QPoint pos = event-\u0026gt;pos(); int xPtInterval = pos.x() - m_oldPos.x(); int yPtInterval = pos.y() - m_oldPos.y(); m_xPtInterval += xPtInterval; m_yPtInterval += yPtInterval; m_oldPos = pos; update(); } void PhotoLabel::mouseReleaseEvent(QMouseEvent*/*event*/) { m_pressed = false; this-\u0026gt;setCursor(Qt::ArrowCursor); //设置鼠标样式 } void PhotoLabel::dragEnterEvent(QDragEnterEvent* event) { if (event-\u0026gt;mimeData()-\u0026gt;hasUrls()) { event-\u0026gt;acceptProposedAction(); } else { event-\u0026gt;ignore(); } } void PhotoLabel::dragMoveEvent(QDragMoveEvent* event) { } void PhotoLabel::dropEvent(QDropEvent* event) { QList\u0026lt;QUrl\u0026gt; urls = event-\u0026gt;mimeData()-\u0026gt;urls(); if (urls.empty()) return; QString fileName = urls.first().toLocalFile(); QFileInfo info(fileName); if (!info.isFile()) return; this-\u0026gt;openFile(fileName); } void PhotoLabel::contextMenuEvent(QContextMenuEvent* event) { QPoint pos = event-\u0026gt;pos(); pos = this-\u0026gt;mapToGlobal(pos); m_menu-\u0026gt;exec(pos); } void PhotoLabel::onSelectImage() { QString path = QFileDialog::getOpenFileName(this, tr(\u0026#34;Open a image file\u0026#34;), \u0026#34;./\u0026#34;, tr(\u0026#34;Images (*.bmp *.png *.jpg *.jpeg *.gif *.tiff)\\nAll files (*.*)\u0026#34;)); if (path.isEmpty()) return; openFile(path); } void PhotoLabel::onPasteImage() { QClipboard* clipboard = QApplication::clipboard(); QImage img = clipboard-\u0026gt;image(); if (img.isNull()) { return; } this-\u0026gt;setImage(img); } void PhotoLabel::onZoomInImage() { m_zoomValue += 0.05; update(); } void PhotoLabel::onZoomOutImage() { m_zoomValue -= 0.05; if (m_zoomValue \u0026lt;= 0) { m_zoomValue = 0.05; return; } update(); } void PhotoLabel::onPresetImage() { m_zoomValue = 1.0; m_xPtInterval = 0; m_yPtInterval = 0; update(); } base64和图片转换的代码放在窗口控件的槽函数中。\nwidget.cpp：\nvoid Widget::updateCode() //图片数据转换成base64编码 { QImage image = ui-\u0026gt;viewer-\u0026gt;getImage(); if (image.isNull()) { QMessageBox::warning(this, tr(\u0026#34;Error\u0026#34;), tr(\u0026#34;Please load an image file!\u0026#34;)); return; } QByteArray ba; QBuffer buf(\u0026amp;ba); image.save(\u0026amp;buf, format.toStdString().c_str()); QByteArray md5 = QCryptographicHash::hash(ba, QCryptographicHash::Md5); QString strMd5 = md5.toHex(); QString head_md = QString::fromUtf8(\u0026#34;![%1](%2)\u0026#34;); QString prefix = QString::fromUtf8(\u0026#34;data:image/%1;base64,\u0026#34;).arg(format); QString code = QString::fromUtf8(ba.toBase64()); if (ui-\u0026gt;checkBox-\u0026gt;isChecked()) { ui-\u0026gt;textEdit-\u0026gt;setText(head_md.arg(strMd5).arg(prefix + code)); } else { ui-\u0026gt;textEdit-\u0026gt;setText(prefix + code); } buf.close(); int num = ui-\u0026gt;textEdit-\u0026gt;toPlainText().length(); ui-\u0026gt;label-\u0026gt;setText(tr(\u0026#34;Length : \u0026#34;) + QString::number(num)); } void Widget::updateImage() // base64编码转换成图片数据 { QString p_b = ui-\u0026gt;textEdit-\u0026gt;toPlainText(); if (p_b.isEmpty()) { return; } if (p_b.contains(QRegularExpression(\u0026#34;data:image[/a-z]*;base64,\u0026#34;))) { // 清除base64编码的html标签 p_b = p_b.remove(QRegularExpression(\u0026#34;data:image[/a-z]*;base64,\u0026#34;)); } QImage image; if (!image.loadFromData(QByteArray::fromBase64(p_b.toLocal8Bit()))) { QMessageBox::warning(this, tr(\u0026#34;Error\u0026#34;), tr(\u0026#34;Fail to decrypt codes!\u0026#34;)); return; } ui-\u0026gt;viewer-\u0026gt;setImage(image); } 2.2 Python实现 同样的，该软件也提供了基于Python的实现，同样通过重载QLabel实现图片显示。\nphotolabel.py：\n# This Python file uses the following encoding: utf-8 import sys from PySide6.QtCore import (Qt, qDebug, QFileInfo, QMimeData, QRect, QPoint) from PySide6.QtGui import (QAction, QImage, QAction, QDragEnterEvent, QDragMoveEvent, QDropEvent, QContextMenuEvent, QPaintEvent, QMouseEvent, QWheelEvent, QPainter, QClipboard, QCursor) from PySide6.QtWidgets import (QApplication, QLabel, QMenu, QMessageBox, QFileDialog) class PhotoLabel(QLabel): def __init__(self, parent): super().__init__(parent) self.m_image = QImage() # 显示的图片 self.m_zoomValue = 1.0 # 鼠标缩放值 self.m_xPtInterval = 0 # 平移X轴的值 self.m_yPtInterval = 0 # 平移Y轴的值 self.m_oldPos = QPoint() # 旧的鼠标位置 self.m_pressed = False # 鼠标是否被摁压 self.m_localFileName = None # 文件名称 self.m_menu = None self.initial_widget() def initial_widget(self): self.m_menu = QMenu(self) load_image = QAction(u\u0026#34;\u0026amp;Open new...\u0026#34;, self) load_image.triggered.connect(self.on_select_image) self.m_menu.addAction(load_image) paste_image = QAction(u\u0026#34;\u0026amp;Paste image\u0026#34;, self) paste_image.triggered.connect(self.on_paste_image) self.m_menu.addAction(paste_image) self.m_menu.addSeparator() zoom_in_action = QAction(u\u0026#34;Zoom in \u0026amp;+\u0026#34;, self) zoom_in_action.triggered.connect(self.on_zoom_in_image) self.m_menu.addAction(zoom_in_action) zoom_out_action = QAction(u\u0026#34;Zoom out \u0026amp;-\u0026#34;, self) zoom_out_action.triggered.connect(self.on_zoom_out_image) self.m_menu.addAction(zoom_out_action) self.m_menu.addSeparator() preset_action = QAction(u\u0026#34;\u0026amp;Reset\u0026#34;, self) preset_action.triggered.connect(self.on_preset_image) self.m_menu.addAction(preset_action) self.m_menu.addSeparator() # clear_action = QAction(u\u0026#34;\u0026amp;Reset\u0026#34;, self) # clear_action.triggered.connect(self.clear_show) # self.m_menu.addAction(clear_action) def open_file(self, path: str): # 打开图片文件 if path is None: return if self.m_image.load(path) is False: QMessageBox.warning(self, \u0026#34;Error\u0026#34;, \u0026#34;Cannot load file!\u0026#34;) return self.m_zoomValue = 1.0 self.m_xPtInterval = 0 self.m_yPtInterval = 0 self.m_localFileName = path self.update() def clear_show(self): # 清空显示 self.m_localFileName = None self.m_image = QImage() self.clear() def set_image(self, image: QImage): # 设置图片 if image is None: return self.m_zoomValue = 1.0 self.m_xPtInterval = 0 self.m_yPtInterval = 0 self.m_localFileName = None self.m_image = image.copy(0, 0, image.width(), image.height()) self.update() def open_action(self): # 调用打开文件对话框 self.on_select_image() def paste_action(self): # 粘贴来自剪贴板的图片 self.on_paste_image() def get_image(self) -\u0026gt; QImage: # 调用存储的图片数据 return self.m_image def contextMenuEvent(self, event: QContextMenuEvent): # 右键菜单 pos = event.pos() pos = self.mapToGlobal(pos) self.m_menu.exec(pos) def paintEvent(self, event: QPaintEvent): # QPaint画图 if self.m_image.isNull(): # super().paintEvent(event) return painter = QPainter(self) # 根据窗口计算应该显示的图片的大小 width = min(self.m_image.width(), self.width()) height = int(width * 1.0 / (self.m_image.width() * 1.0 / self.m_image.height())) height = min(height, self.height()) width = int(height * 1.0 * (self.m_image.width() * 1.0 / self.m_image.height())) # 平移 painter.translate(self.width() / 2 + self.m_xPtInterval, self.height() / 2 + self.m_yPtInterval) # 缩放 painter.scale(self.m_zoomValue, self.m_zoomValue) # 绘制图像 pic_rect = QRect(int(-width / 2), int(-height / 2), width, height) painter.drawImage(pic_rect, self.m_image) # super().paintEvent(event) def wheelEvent(self, event: QWheelEvent): # 鼠标滚轮滚动 value = int(event.angleDelta().y() / 15) if value \u0026lt; 0: # 放大 self.on_zoom_in_image() else: # 缩小 self.on_zoom_out_image() self.update() def mousePressEvent(self, event: QMouseEvent): # 鼠标摁下 self.m_oldPos = event.pos() self.m_pressed = True self.setCursor(Qt.ClosedHandCursor) # 设置鼠标样式 def mouseMoveEvent(self, event: QMouseEvent): # 鼠标松开 if self.m_pressed is False: # super().mouseMoveEvent(event) return pos = event.pos() xp = pos.x() - self.m_oldPos.x() yp = pos.y() - self.m_oldPos.y() self.m_xPtInterval += xp self.m_yPtInterval += yp self.m_oldPos = pos self.update() def mouseReleaseEvent(self, event: QMouseEvent): # 鼠标发射事件 self.m_pressed = False self.setCursor(Qt.ArrowCursor) # 设置鼠标样式 # 拖放文件 def dragEnterEvent(self, event: QDragEnterEvent): if event.mimeData().hasUrls(): event.acceptProposedAction() else: event.ignore() def dragMoveEvent(self, event: QDragMoveEvent): pass def dropEvent(self, event: QDropEvent): urls = event.mimeData().urls() if urls is None: return file_name = urls[0].toLocalFile() info = QFileInfo(file_name) if info.isFile() is False: return self.open_file(file_name) def on_select_image(self): # 选择打开图片 path, _ = QFileDialog.getOpenFileName(self, \u0026#34;Open a image file\u0026#34;, \u0026#34;./\u0026#34;, \u0026#34;Images (*.bmp *.png *.jpg *.jpeg *.gif *.tiff)\\nAll files (*.*)\u0026#34;) if path is None: return info = QFileInfo(path) if info.isFile() is False: return self.open_file(path) def on_paste_image(self): # 选择粘贴图片 clipboard = QApplication.clipboard() img = clipboard.image() if img.isNull(): return self.set_image(img) def on_zoom_in_image(self): # 图片放大 self.m_zoomValue += 0.05 self.update() def on_zoom_out_image(self): # 图片缩小 self.m_zoomValue -= 0.05 if self.m_zoomValue \u0026lt;= 0: self.m_zoomValue = 0.05 return self.update() def on_preset_image(self): # 图片还原 self.m_zoomValue = 1.0 self.m_xPtInterval = 0 self.m_yPtInterval = 0 self.update() if __name__ == \u0026#34;__main__\u0026#34;: pass 值得注意的是，python实现代码中，二进制数据到base64的转换是通过ptyhon的str函数完成的，因此输出字符串会包含b'...'的标签，需要通过字符串切片去除该标签。widget.py：\ndef update_code(self): #图片→base64 image = self.ui.viewLabel.get_image() if image.isNull(): QMessageBox.warning(self, \u0026#34;Error\u0026#34;, \u0026#34;Please load an image file!\u0026#34;) return ba = QByteArray() buf = QBuffer(ba) image.save(buf, self.m_format) md5 = QCryptographicHash.hash(ba, QCryptographicHash.Md5) strMd5 = str(md5.toHex())[2:-1] prefix = \u0026#34;data:image/{};base64,\u0026#34;.format(self.m_format) code = str(ba.toBase64())[2:-1] if self.ui.checkBox.isChecked(): self.ui.textEdit.setText(\u0026#34;![{0}]({1})\u0026#34;.format(strMd5, prefix + code)) else: self.ui.textEdit.setText(prefix + code) buf.close() num = len(self.ui.textEdit.toPlainText()) self.ui.lengthLabel.setText(\u0026#34;Length : \u0026#34; + str(num)) def update_image(self): #base64→图片 p_b = self.ui.textEdit.toPlainText() if len(p_b) == 0: return if re.match(\u0026#34;data:image[/a-z]*;base64,\u0026#34;, p_b): p_b = re.sub(\u0026#34;data:image[/a-z]*;base64,\u0026#34;, \u0026#34;\u0026#34;, p_b, count=1) image = QImage() if image.loadFromData(QByteArray.fromBase64(p_b.encode())) is False: QMessageBox.warning(self, \u0026#34;Error\u0026#34;, \u0026#34;Fail to decrypt codes!\u0026#34;) return self.ui.viewLabel.set_image(image) UI均通过QtDesigner实现，无论C++实现还是Python实现，软件界面效果均一致。\nBase64 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n15、Qt显示图片并支持缩放、移动等操作 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-04-02-image2base64/","tags":[{"LinkTitle":"C++","RelPermalink":"/zh-cn/tags/c++/"},{"LinkTitle":"Python","RelPermalink":"/zh-cn/tags/python/"},{"LinkTitle":"Qt","RelPermalink":"/zh-cn/tags/qt/"}],"title":"一款基于Qt6的图片base64转换工具"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/categories/cfd/"}],"content":"1. 编译cfmesh 前期编译安装的是com版的OpenFOAM，版本号是v2412，没有附带cfmesh的源码。根据官方文档的提示，需要手动下载cfmesh的源码文件：\ncd $WM_PROJECT_DIR git submodule update --init --recursive plugins/cfmesh 出现以下错误：\nfatal: 不是 git 仓库（或者任何父目录）：.git 无语了，直接通过git下载源码到指定文件夹，文件夹和URL路径可以查看.gitmodules文件：\ncd $WM_PROJECT_DIR git clone https://develop.openfoam.com/Community/integration-cfmesh.git plugins/cfmesh 进入cfmesh文件夹，开始编译：\ncd plugins/cfmesh ./Allwmake -jN # N替换成CPU核心数 运行pMesh，验证是否安装成功：\npMesh -help 2. 编译ccm工具 OpenFOAM的ccm工具包含ccmToFoam和foamToCcm，前者用于将STAR-CCM+输出的ccm格式网格转换成OpenFOAM的网格，后者相反。需要先编译安装第三方库libccmio：\ncd $WM_THIRD_PARTY_DIR # 下载libccmio wget http://portal.nersc.gov/project/visit/third_party/libccmio-2.6.1.tar.gz # 备选链接 # wget https://sourceforge.net/projects/foam-extend/files/ThirdParty/libccmio-2.6.1.tar.gz tar xvzf libccmio-2.6.1.tar.gz\t# 解压下载的压缩包 ./makeCCMIO\t# 运行libccmio的编译程序 接下来编译ccmToFoam：\n# 先编译libccm cd $WM_PROJECT_DIR/src/conversion/ccm ./Allwmake # 然后编译ccmToFoam和foamToCcm cd $WM_PROJECT_DIR/applications/utilities/mesh/conversion/ccm ./Allwmake 运行ccmToFoam，验证是否安装成功：\nccmToFoam -help ","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-27-openfoam-compile-install-cfmesh-and-ccmtool/","tags":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Openfoam","RelPermalink":"/zh-cn/tags/openfoam/"}],"title":"OpenFOAM编译安装cfmesh和ccm工具"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"SU2 是一款由斯坦福大学航空航天学院开发的开源CFD求解器，基于C++和Python开发，定位类似于OpenFOAM，但不支持多面体网格。相比OpenFOAM，SU2在高速可压缩流方面的求解更有优势。\n下载SU2源代码：\nmkdir $HOME/su2code \u0026amp;\u0026amp; cd $HOME/su2code # 只clone最近commit版本，加快下载速度 git clone https://github.com/su2code/SU2.git --depth=1 定义环境变量，新建配置文件su2.env：\ntouch $HOME/su2code/su2.env chmod +x $HOME/su2code/su2.env vim $HOME/su2code/su2.env 在su2.env文件中加入以下内容，保存退出：\n#!/bin/sh # 定义SU2环境变量 export SU2_RUN=$HOME/su2code/bin\t# 编译完成后su2的安装路径 export SU2_HOME=$HOME/su2code/SU2\t# 下载su2的源码的文件夹路径 export PATH=$PATH:$SU2_RUN export PYTHONPATH=$SU2_RUN:$PYTHONPATH 编译程序的配置文件meson_options.txt位于SU2源代码文件夹下，根据自己的需求调整其中的编译选项：\nvim $HOME/su2code/SU2/meson_options.txt 这里打开mpi和blas支持，修改以下两行的value：\noption(\u0026#39;with-mpi\u0026#39;, type : \u0026#39;feature\u0026#39;, value : \u0026#39;enabled\u0026#39;, description: \u0026#39;enable MPI support\u0026#39;) option(\u0026#39;enable-openblas\u0026#39;, type : \u0026#39;boolean\u0026#39;, value : true, description: \u0026#39;enable BLAS and LAPACK support via OpenBLAS\u0026#39;) 如果是Intel的机器，建议打开mkl支持。\n默认支持的blas库是openblas，要先下载openblas库：\nsudo apt install libopenblas-dev -y 进入下载的源码目录，运行编译程序\n# 载入环境变量 source $HOME/su2code/su2.env # 进入源码文件夹 cd $SU2_HOME # 配置编译器，生成ninja构建文件 # 配置过程中会自动从git上下载外部依赖 # 非常花时间…… ./meson.py build --prefix=$SU2_RUN/.. # 开始编译并安装 ./ninja -C build install 验证是否安装成功：\nSU2_CFD --help 安装成功会输出软件版本号和帮助信息。\n编写SU2的slurm计算脚本：\n#!/bin/bash #SBATCH --job-name=su2_test #SBATCH --partition=debug #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 1 #SBATCH --ntasks-per-node=32 cd $SLURM_SUBMIT_DIR source ${HOME}/su2code/su2.env # 应填写绝对路径 export CFG_FILE=`find . -name \u0026#34;*.cfg\u0026#34;` export MACHINEFILE=$SLURM_JOBID.nodes scontrol show hostnames $SLURM_JOB_NODELIST \u0026gt; $MACHINEFILE mpiexec -np $SLURM_NPROCS --machinefile $MACHINEFILE SU2_CFD $CFG_FILE 将脚本文件和SU2的cfg文件及网格放到同一文件夹，通过sbatch命令提交计算任务。\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-27-ubuntu-compile-su2/","tags":[{"LinkTitle":"Slurm","RelPermalink":"/zh-cn/tags/slurm/"},{"LinkTitle":"Su2","RelPermalink":"/zh-cn/tags/su2/"},{"LinkTitle":"Ubuntu","RelPermalink":"/zh-cn/tags/ubuntu/"}],"title":"Ubuntu编译安装SU2"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/categories/cfd/"}],"content":"1. Fluent 首先要编写Fluent的jou脚本：\n/file/read-case \u0026#34;small_fan.cas.h5\u0026#34; /solve/initialize/hyb-initialization /solve/iterate 100 /file/write-case \u0026#34;small_fan_results.cas.h5\u0026#34; ok /file/write-data \u0026#34;small_fan_results.dat.h5\u0026#34; ok /exit ok 这个jou文件很简单，就是告诉Fluent读取哪个文件、怎么初始化、迭代多少步、如何保存直至最后退出。如果计算比较复杂的，比如涉及到UDF加载或特殊条件初始化设置的，还需要增加相应的命令行。不熟悉怎么编写TUI命令的话，可以通过Fluent图形界面下面的命令行窗口录制脚本。\n编写Slurm脚本：\n#!/bin/bash #SBATCH --job-name=fluent_test\t# 任务名称 #SBATCH --partition=debug #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 1\t# 计算节点数 #SBATCH --ntasks-per-node=32\t# 每节点计算进程数 cd $SLURM_SUBMIT_DIR source ${HOME}/opt/ansys2025r1.env\t# 载入许可证设置环境变量，这里应该使用绝对路径 export FLUENT=/ansys_inc/v251/fluent/bin/fluent\t# fluent可执行文件路径 export MPI_TYPE=intel # intel or openmpi export JOU_FILE=`find . -name \u0026#34;*.jou\u0026#34;` export MACHINEFILE=$SLURM_JOBID.node scontrol show hostnames $SLURM_JOB_NODELIST \u0026gt; $MACHINEFILE #注意fluent根据2维3维单双精度的不同有4钟计算模式：2d、3d、2ddp、3ddp，根据自己的需求选择对应的计算模式 $FLUENT 3ddp -g -t$SLURM_NPROCS -cnf=$MACHINEFILE -mpi=$MPI_TYPE -ssh -i $JOU_FILE 保存以上脚本，将待提交的cas文件和jou文件放到脚本所在文件夹，通过sbatch命令提交脚本即可。计算完成后将输出的结果文件下载到本地机器上处理。\n2. CFX 相比fluent，cfx计算脚本简单很多：\n#!/bin/bash #SBATCH --job-name=cfx_test\t# 任务名称 #SBATCH --partition=debug #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 1\t# 计算节点数 #SBATCH --ntasks-per-node=32\t# 每节点计算进程数 cd $SLURM_SUBMIT_DIR source ${HOME}/opt/ansys2025r1.env\t# 载入许可证设置环境变量，这里应该使用绝对路径 export CFX=/ansys_inc/v251/CFX/bin/cfx5solve\t# cfx求解器可执行文件路径 export DEF_FILE=`find . -name \u0026#34;*.def\u0026#34;` hostnames=`scontrol show hostnames $SLURM_JOB_NODELIST` hostnames=`echo $hostnames | sed -e \u0026#39;s/ /,/g\u0026#39;` $CFX -def $DEF_FILE -double -part $SLURM_NPROCS -par-dist $hostnames -start-method \u0026#39;Intel MPI Distributed Parallel\u0026#39; -name $SLURM_JOB_NAME 将脚本文件和def文件放到同一文件夹并提交即可。\n参考资料：\nJournal 脚本编写指南 Fluent 极客 —— 强大的自定义功能（UDF，jou，参数化，expression，ACT ） ANSYS - CFX, Fluent, Mechanical CFX本地多核批处理文件编写方法 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-26-slurm-submit-fluent-and-cfx-script/","tags":[{"LinkTitle":"Cfx","RelPermalink":"/zh-cn/tags/cfx/"},{"LinkTitle":"Fluent","RelPermalink":"/zh-cn/tags/fluent/"},{"LinkTitle":"Slurm","RelPermalink":"/zh-cn/tags/slurm/"}],"title":"Slurm提交Fluent和CFX的计算脚本"},{"categories":[{"LinkTitle":"Tech","RelPermalink":"/zh-cn/categories/tech/"}],"content":"Tabby 是一款颜值很高的终端工具，最开始用它是为了替代本机终端，用着用着后来发现越来越多的优点。首先它内置支持SSH连接方式，同时支持SFTP传输文件，设置操作简单，避免了在Windows终端中的繁琐设置。其次，可以用来替代MSYS2、Cygwin原有的Mintty界面，实现不同终端之间无缝切换。关于Tabby终端中如何调用MSYS2，下面记录一下配置方法：\n首先在Tabby设置中克隆一个CMD配置： 在名称中填写要调用的MSYS2工具链： 注意，图标这里不能直接指派ico文件，Tabby无法识别，必须将ico图标转换成svg格式。这种ico2svg在线转换资源很多，网上一搜一大把：\nACCONVERT - ICO转SVG CDKM - ICO转SVG FreeConvert - ICO到SVG转换器 转换完成后下载svg文件，用文本工具打开将svg源码复制粘贴到上面的图标栏里，Tabby就可以正常显示图标了。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE svg PUBLIC \u0026#34;-//W3C//DTD SVG 1.1//EN\u0026#34; \u0026#34;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\u0026#34;\u0026gt; \u0026lt;svg version=\u0026#34;1.1\u0026#34; id=\u0026#34;Layer_1\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; xmlns:xlink=\u0026#34;http://www.w3.org/1999/xlink\u0026#34; x=\u0026#34;0px\u0026#34; y=\u0026#34;0px\u0026#34; width=\u0026#34;16px\u0026#34; height=\u0026#34;16px\u0026#34; viewBox=\u0026#34;0 0 16 16\u0026#34; enable-background=\u0026#34;new 0 0 16 16\u0026#34; xml:space=\u0026#34;preserve\u0026#34;\u0026gt; \u0026lt;image id=\u0026#34;image0\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; x=\u0026#34;0\u0026#34; y=\u0026#34;0\u0026#34; xlink:href=\u0026#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAAAIGNIUk0AAHomAACAhAAA+gAAAIDo AAB1MAAA6mAAADqYAAAXcJy6UTwAAADPUExURb5kPsBpRdWbgtSZgL9lQMBoQ8+McMyEZsJuS/36 +P///9uplL5lP+3UyevOws2HafDc1NCPdOjHuejIur9mQO3TyeXCs8NxTv37+vv18sh7W8h8XP/+ /uS+rt2umtOXfuXBsvfs6Pjv6+bDtMFqRvny7vHf1/36+dyrlvXn4e/a0dadhMBpRP79/f7+/fz4 9sJuSvHe1vnw7PPi2+O9rNqmkN2tmeC2pP78+8Z3Vfft6L9mQfz59+fFt8JtSdKVe79nQsuDZcV0 UtGSd8NvTFLhcR8AAAABYktHRApo0PRWAAAAB3RJTUUH6QMaAysVnHCKGgAAAJpJREFUGNONj1cS ggAUA2NBIdi7WAErYEFFBRXr/c8k7QC8n8zuRyYPSHWZbC4fhFAoJkKUKAOlMiuJqJI1oE42EtEk pRbaZCfmbq9PKoMhOYrFmDI5maoa9Yhn84VC6svVmkYkZJrWhtvd3qYkBmwdNAFH8hR2O4FweAYu vLowSc/CzbMtwL0/gqUGVR/+8xUWvT/h9u8vxaN/UKUNLao7WagAAAAldEVYdGRhdGU6Y3JlYXRl ADIwMjUtMDMtMjZUMDM6NDM6MjArMDA6MDDH702KAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDI1LTAz LTI2VDAzOjQzOjIwKzAwOjAwtrL1NgAAACh0RVh0ZGF0ZTp0aW1lc3RhbXAAMjAyNS0wMy0yNlQw Mzo0MzoyMSswMDowMEfQ310AAAAASUVORK5CYII=\u0026#34; /\u0026gt; \u0026lt;/svg\u0026gt; 禁用动态标签页标题：根据自己需要设置，建议选上。分组：需要先新建配置文件组，然后再在这里选。\n程序、参数和环境变量如图所示。 工作目录和HOME目录填写MSYS2的HOME目录，MSYSTEM填写要调用的工具链。\n上面第3步保存之后，就可以通过Tabby标题栏的图标快速调用MSYS2的Clang64终端了，其他工具链按同样的方法配置。 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-26-tabby-setup-msys2/","tags":[],"title":"Tabby配置MSYS2"},{"categories":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/categories/cfd/"}],"content":"以前用STAR-CCM+在Windows工作站做计算的时候（没钱，公司舍不得上超算……），有时候一晚上要提交十几二十个计算任务（瞎卷ㄟ( ▔, ▔ )ㄏ），当然不可能十几个任务全都一起跑（机器遭不住），也不可能全程盯着它跑一个个手动提交（人遭不住）。几年前用PowerShell编写了这个简易的排队计算的模板，在这里分享给大家。\n$title = \u0026#34;STAR-CCM+ 19.06.009-r8\u0026#34;\t# 窗口标题，怎么填随你喜欢 $host.ui.RawUI.WindowTitle = $title $STARCCM_PATH = \u0026#34;D:\\XXX\\Siemens\\19.06.009-R8\\STAR-CCM+19.06.009-R8\\star\\lib\\win64\\clang17.0vc14.2-r8\\lib\u0026#34;\t# 填写本机STAR-CCM+的安装绝对路径 $env:path += \u0026#34;;$STARCCM_PATH\u0026#34; $run_dir = $pwd $thread_number = 32\t# 填写本机的CPU核心数 $Array = Get-ChildItem -Path $run_dir -Name \u0026#34;*.sim\u0026#34; $n = 0 foreach($item in $Array) { $n += 1 $sub_dir = $n.ToString() + \u0026#34;_\u0026#34; + $item.Substring(0,$item.Length-4) mkdir $sub_dir mv $item $sub_dir cd $sub_dir $host.ui.RawUI.WindowTitle = $title + \u0026#34; - \u0026#34; + $item + \u0026#34; - \u0026#34; + $n + \u0026#34;/\u0026#34; + $Array.Count $log = $item + \u0026#34;.log\u0026#34; starccm+ $item -batch run -np $thread_number -mpi ms | tee $log cd $run_dir } 把以上命令行以文本格式保存为.ps1脚本文件，和要提交计算的.sim文件放到同一个文件夹，然后通过终端运行这个脚本。会自动统计排队任务数，将计算的.sim文件转移至新建子文件夹，同时生成.log日志文件，也可以在输出窗口中监控运行情况。计算完成后关闭终端即可。\n美中不足的是，不支持宏文件，也不支持临时增加或插入算例。\n各位可以针对自己的情况，做一些针对性的调整。\n如果碰到输出窗口和日志文件中有乱码的情况，多半是你的PowerShell不支持UTF-8所导致。参考以下方法1，在PowerShell窗口中输入：\n# 配置文件一般位于：C:\\Users\\用户名\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 # 如果没有就新建一个 notepad $PROFILE # 编辑配置文件 在配置文件中增加以下内容，保存退出：\n$OutputEncoding = [console]::InputEncoding = [console]::OutputEncoding = [Text.UTF8Encoding]::UTF8 WindowsPowerShell中文乱码问题解决 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-25-posershell-starccm-script/","tags":[{"LinkTitle":"Powershell","RelPermalink":"/zh-cn/tags/powershell/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/zh-cn/tags/star-ccm+/"}],"title":"PowerShell编写STAR-CCM+自动排队计算脚本"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"1. 准备工作 全部安装需要161GB磁盘空间，请确保磁盘剩余可用空间满足需求，可以根据自己需求选择安装内容。 安装过程中至少需要8GB内存，推荐内存16~32GB。\n安装过程中的依赖工具：\nsudo apt install libnsl2 libpcre3 lsb-* ldap-utils libunistring5 xfonts-100dpi xfonts-75dpi 其他运行时的依赖工具安装，请查询官方文档。\nAnsys2025R1安装文件中包含9个.iso安装文件，用以下命令挂载.iso文件到指定路径：\nmkdir ${HOME}/ISO/1 sudo mount ${HOME}/Share/Ansys/ANSYS2025R1_LINX64_DISK1.iso ${HOME}/ISO/1 -o loop 同样的办法，将剩下的安装包分别挂载至其他文件夹。\n2. 安装许可证服务器 2.1 安装Ansys License Manager 由于Ansys安装需要用到图形界面，因此需要先通过远程连接进入Ubuntu桌面系统。\n进入第一个安装包的挂载点，运行安装程序：\ncd ${HOME}/ISO/1 sudo ./INSTALL # 由于需要以服务形式运行，建议以管理员用户权限运行安装程序 许可证安装路径选择：/opt/ansys_inc： 安装完成后退出安装程序。\n确认Ansys许可证服务器的守护进程是否在运行：\nps -eaf | grep ansyslmd 如果ansyslmd进程在运行，kill掉它：\nkill -s 9 [PID] # [PID]填写ansyslmd进程的PID 2.2 配置许可证服务器 我们需要将ansys.2025r1.linx64-ssq.tar.gz压缩包中的/shared_files文件夹下的文件解压到安装目录下并覆盖：\ntar xvzf ansys.2025r1.linx64-ssq.tar.gz sudo cp -r shared_files /opt/ansys_inc/ 接下来配置许可证服务，以下命令运行许可证配置程序：\nsudo /opt/ansys_inc/shared_files/licensing/init_ansyslm_tomcat start 然后用浏览器打开该地址：http://localhost:1084/ANSYSLMCenter.html，定位到Get System Hostid information栏目，找到HOSTID，把它复制下来后面会用到： 打开license_2024.12.15.txt文件，在开头的SERVER localhost XXXXXXXXXXXX 1055这一行中将XXXXXXXXXXXX替换为上面复制下来的HOSTID，保存文件。 定位到Add a License File栏目，载入刚刚编辑过的license_2024.12.15.txt文件。 这里出现了一个问题，提示lmgrd找不到，无法启动服务器守护进程，运行以下命令：\nsudo cp /opt/ansys_inc/shared_files/licensing/linx64/update/lmgrd /opt/ansys_inc/shared_files/licensing/linx64/lmgrd 再次载入文件即可： 定位到View Status/Start/Stop License Manager栏目，确认服务器运行状态： 退出浏览器并关闭许可证配置程序，确认守护进程是否在运行：\nsudo /opt/ansys_inc/shared_files/licensing/init_ansyslm_tomcat stop ps -eaf | grep ansyslmd 如果许可证服务器和主程序分别安装在不同机器上，需要在防火墙中开启许可证服务器的1055端口：\nsudo ufw allow from any to any port 1055 proto tcp 2.3 添加自定义服务 完成以上设置，当重启服务器之后，系统不会自动启动守护进程，需要通过以下命令启动ansyslmd：\nsudo /opt/ansys_inc/shared_files/licensing/start_ansyslmd 为了防止服务器断电重启之后因许可证问题导致无法计算，下面通过systemd添加自定义服务，实现开机自动启动守护进程。以下命令新建ansyslmd服务：\nsudo touch /usr/lib/systemd/system/ansyslmd.service sudo vim /usr/lib/systemd/system/ansyslmd.service sudo chmod 754 /usr/lib/systemd/system/ansyslmd.service 添加如下内容并保存退出：\n[Unit] Description=Ansys License Deamon After=ansyslmd.service [Service] Type=forking User=root Group=root ExecStart=/opt/ansys_inc/shared_files/licensing/start_ansyslmd ExecReload= ExecStop=/opt/ansys_inc/shared_files/licensing/stop_ansyslmd [Install] WantedBy=multi-user.target 通过以下命令启动服务：\nsudo systemctl enable ansyslmd sudo systemctl start ansyslmd 查看服务状态：\nsudo systemctl status ansyslmd 服务启动成功，安装完毕后可以重启验证下。\n3. 安装主程序 安装主程序同样需要进入Ubuntu桌面系统。\n进入第一个安装包的挂载点，运行安装程序：\ncd ${HOME}/ISO/1 sudo ./INSTALL # 用root权限安装 求解同样选择安装在：/opt/ansys_inc 根据自己需要选择安装哪些内容： 配置CAD几何图形接口，本机没安装CAD软件，选择跳过： 根据前面选择安装内容的不同，后面会询问一堆配置文件路径，根据自己需要设置吧，没有就无脑Next。 选择是否创建桌面快捷方式，选上吧，兴许有用呢。 确认安装内容，点击Next开始复制文件： 可以查看安装进度，安装过程中会询问载入其他.iso文件挂载路径。 安装完成，提示有一些报错，不管它，退出。 接下来将ansys.2025r1.linx64-ssq.tar.gz压缩包中的v251文件夹下的文件解压到安装目录下并覆盖：\ntar xvzf ansys.2025r1.linx64-ssq.tar.gz sudo cp -r v251 /opt/ansys_inc/ 主程序安装完成，卸载.iso文件的挂载点：\nsudo umount ${HOME}/ISO/1 剩余挂载点按相同方式卸载。\n4. 配置主程序许可证 安装完主程序之后，需要给主程序配置许可证，有两种方法。\n4.1 方法1→配置环境变量 在${HOME}/opt/中创建并编辑环境变量文件：\ntouch ${HOME}/opt/ansys2025r1.env chmod +x ${HOME}/opt/ansys2025r1.env vim ${HOME}/opt/ansys2025r1.env 加入以下内容，保存退出：\nexport ANSYSLMD_LICENSE_FILE=1055@localhost 后续可以通过以下命令载入环境变量。\nsource ${HOME}/opt/ansys2025r1.env 可以把上面的命令行加入到${HOME}/.bashrc文件当中，这样每次登录都会自动载入环境变量。\n4.2 方法2→Ansys Licensing Settings 运行以下命令启动Licensing Settings程序：\n/ansys_inc/v251/licensingclient/linx64/LicensingSettings 按步骤进行以下设置：\n将配置文件等级从Installion改为User 将状态设置为Enable 端口号填写1055 Server1地址填写localhost 点击Test按钮测试 点击Save保存配置 保存退出，完成许可证配置。\n5. 启动主程序 配置完许可证之后，通过以下命令启动Workbench：\n/ansys_inc/v251/Framework/bin/Linux64/runwb2 类似的，一些关键主程序的入口参考如下：\n应用 启动命令 ACP /ansys_inc/v251/ACP/ACP.sh CFD-Post /ansys_inc/v251/cfdpost CFX /ansys_inc/v251/CFX/bin/cfx5(注) FLUENT /ansys_inc/v251/fluent/bin/fluent ICEM CFD /ansys_inc/v251/icemcfd/icemcfd ICEPAK /ansys_inc/v251/icepak/icepak Motion /ansys_inc/v251/Motion/solver/rundfs.sh Polyflow Classic /ansys_inc/v251/polyflow/bin/polyflow TurboGrid /ansys_inc/v251/TurboGrid/bin/cfxtg Sherlock /ansys_inc/v251/sherlock/runSherlock Workbench /ansys_inc/v251/Framework/bin/Linux64/runwb2 Electronics /ansys_inc/v251/AnsysEM/ansysedt (注): cfx5可以替换成 cfx5launch、cfx5pre、cfx5solve或cfx5post.，分别对应不同的功能模块。\n至此，Ansys安装完毕。\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-25-ubuntu24.04-install-ansys2025r1/","tags":[{"LinkTitle":"Ansys","RelPermalink":"/zh-cn/tags/ansys/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"},{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Ubuntu","RelPermalink":"/zh-cn/tags/ubuntu/"}],"title":"Ubuntu24.04安装Ansys2025R1"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"由于Ubuntu运行在虚拟机中，通过Ubuntu挂载Windows共享文件夹，在挂载点中运行某些CAE软件会出现计算报错。考虑将Ubuntu文件夹共享给Windows，因此需要在Ubuntu系统上搭建Samba服务器。\n首先在Ubuntu上安装samba软件包：\nsudo apt install samba -y 创建共享文件夹：\nmkdir ${HOME}/LinuxShare 编辑Samba配置文件/etc/samba/smb.conf：\nsudo vim /etc/samba/smb.conf 在文件末尾增加以下内容，保存退出：\n[Ubuntu_Share] # 在客户端上显示的共享文件夹的名称 comment = Samba\t# 注释，展示给用户看的 path = /home/***/LinuxShare\t# 共享文件夹的本地路径，填写绝对路径 public = yes\t# 是否允许匿名用户访问 writable = yes\t# 是否允许用户编辑 available = yes\t# 是否可用 browseable = yes\t# 是否可以在网络上浏览 valid users = user\t# 填写Ubuntu登录用户名\t给Samba用户设置密码：\nsudo smbpasswd -a user 启动Samba服务的守护进程：\nsudo systemctl enable smbd sudo systemctl start smbd 查询Samba服务状态：\nsudo systemctl status smbd 服务状态：Active: active (running)，正常运行。\n更新Samba配置文件/etc/samba/smb.conf之后通过以下命令刷新：\nsudo service smbd restart 添加防火墙规则：\nsudo ufw allow samba 配置完成，可以在Windows端访问该共享文件夹，这部分就省略了。\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-24-ubuntu24.04-samba-serve/","tags":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Ubuntu","RelPermalink":"/zh-cn/tags/ubuntu/"}],"title":"Ubuntu24.04搭建Samba服务器"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"前面讲过用NAT+端口映射的方式访问虚拟机，但使用过程中发现一个问题：每次宿主机重启之后，默认的Default SwitchIP地址都会变更，造成虚拟机无法访问网关进而无法联网。\n为了保证虚拟机联网，每次重启开机都要手动设置Default Switch对应适配器的IP地址，非常不便。下面是参考微软官方文档 给出的解决方案。\n这里前面的步骤可以通过图形用户界面去实现，首先新建一个内部的虚拟交换机： 命名虚拟交换机，记住它，后面会用到。确定新建： 在Windows网络适配器中找到新建的虚拟机适配器，将IP地址改成虚拟机设置的网关地址，子网掩码也改成和虚拟机设置的一致： 下面用管理员权限打开PowerShell，通过以下命令行查看是否已有NAT网络：\nGet-NetNat 因为一个机器上只能有一个NAT网络，因此要先移除现有的NAT网络：\nGet-NetNat | Remove-NetNat 设置NAT网络：\n# InLan为NAT网络名称，172.25.64.0为子网网关IP地址，/24为子网前缀长度 New-NetNat -Name InLan -InternalIPInterfaceAddressPrefix 172.25.64.0/24 关于子网网关，和上面设置的虚拟机适配器的IP地址即虚拟机的网关地址处在同一个网段即可，末尾设置为0；本例上面虚拟机的网关地址为172.25.64.1，那子网网关地址一般设置为172.25.64.0。子网前缀即子网掩码，关于子网前缀长度，通俗来讲，像2xx.2xx.2xx.0这种都是24位，2xx.2xx.2xx.2xx这种为32位。\n设置完成后通过Get-NetNat查看是否有NAT网络信息：\nName : InLan ExternalIPInterfaceAddressPrefix : InternalIPInterfaceAddressPrefix : 172.25.64.0/24 IcmpQueryTimeout : 30 TcpEstablishedConnectionTimeout : 1800 TcpTransientConnectionTimeout : 120 TcpFilteringBehavior : AddressDependentFiltering UdpFilteringBehavior : AddressDependentFiltering UdpIdleSessionTimeout : 120 UdpInboundRefresh : False Store : Local Active : True 这样新的NAT网络就配置好了，在虚拟机设置中更换成新设置的虚拟交换机。 再登录虚拟机，手动设置DNS即可（不理解为什么不能通过网关自动获取DNS，也许是BUG……），否则虚拟机无法访问外网。 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-21-hyperv-switch-static-ip-address/","tags":[{"LinkTitle":"Virtual-Machine","RelPermalink":"/zh-cn/tags/virtual-machine/"}],"title":"Hyper-V虚拟交换机NAT设置静态IP"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"为什么选择SIMULIA？首先是Abaqus功能强大，绝大多数结构问题都能求解。其次流固耦合方便，STAR-CCM+自带案例就手把手你怎么和Abaqus双向耦合。再次是过往经历的惯性，毕竟主机厂Abaqus用得可不少，不缺案例和资源。\n1. 准备工作 首先安装开发环境和一些必要的软件：\nsudo apt update # 更新软件源 sudo apt upgrade # 更新本地安装的软件 sudo apt install build-essential # 安装开发环境 sudo apt install csh tcsh ksh gcc g++ gfortran libstdc++5 build-essential make libjpeg62 libmotif-dev 解压安装包：\nmkdir iso # 新建一个iso文件夹，用于存放解压出来的文件 tar xvzf DS.SIMULIA.SUITE.2024.LINX64.tar.gz -C ./iso # 解压到iso文件夹中 2. 安装过程 2.1 启动安装程序 首先定义环境变量，不然无法启动安装程序：\n# 可以把下面的内容加到${HOME}/.bashrc里，然后重新启动终端 export DSYAuthOS_`lsb_release -si`=1 export DSY_Force_OS=linux_a64 export NOLICENSECHECK=true 进入/iso/1文件夹，运行安装程序。启动图形化界面在终端运行./StartGUI.sh，类似Windows下的安装方式，跟着向导一步步安装即可。\n这里选择文本界面：\n./StartTUI.sh # 启动文本界面安装向导 Enter下一步。\n记得一定要选4，也就是FLEXnet License Server。输入4，然后Enter继续。\n确认安装内容，Enter开始启动相关安装程序。\n2.2 安装许可证服务器 选择P，然后输入/iso/4文件夹的绝对路径。\n选择许可证服务器的安装路径，这里选择安装在${HOME}/opt/SIMULIA/License/2024路径下，要输入绝对路径。\n这里选择默认，安装后启动许可证服务器。\n选择许可证文件路径，输入绝对路径。\n输入许可证文件后提示检索主机ID失败，忽略它，输入2继续。\n继续选2。\n确认安装信息。\n安装完成，Enter继续下一个程序安装。\n2.3 安装求解器 选择P，然后输入/iso/5文件夹的绝对路径。\n选择安装路径，这里选择安装在${HOME}/opt/SIMULIA/EstProducts/2024路径下。\n选择要安装的内容，全选输入-1。\n选择许可证类型，默认选第1个。\n输入许可证服务器的访问地址和端口，输入29100@localhost定义License Server 1，其余Enter跳过。\n选择命令行程序的路径，这里选择安装到${HOME}/opt/SIMULIA/var/DassaultSystemes/SIMULIA/Commands。\n选择外部插件的路径，这里选择安装到${HOME}/opt/SIMULIA/var/DassaultSystemes/SIMULIA/CAE/plugins/2024。\n选择Tosca的接口，根据实际需要来。\n选择Tosca Fluid工作目录，这里指派到${HOME}/temp下。\n选择STAR-CCM+安装路径，根据实际情况设置，默认留空。\n选择STAR-CCM+许可证，根据实际情况设置，默认留空。\n选择Fluent安装路径，根据实际情况设置，默认留空。\n确认安装信息，Enter开始复制文件。\n安装过程中会启动验算程序，可以查看验算结果。Enter继续。\n求解器安装完成，Enter退出，进入下一个安装程序。\n2.4 安装CAA API 选择P，然后输入/iso/6文件夹的绝对路径。\n选择安装路径，这里选择安装到${HOME}/opt/SIMULIA/EstProducts/2024。\n确认要安装的内容，Enter继续。\n完成安装，Enter退出。\n2.5 安装Isight 选择安装路径，这里选择安装到${HOME}/opt/SIMULIA/Isight/2024。\n选择要安装的内容，-1全选。\n是否启动TomEE配置工具，默认跳过。\n这一步也默认跳过吧。\n没有安装文档，选择跳过吧。\n确认安装内容，Enter开始复制文件。\n安装完成，Enter退出。\n2.6 安装完成 确认安装结果，Enter退出SIMULIA安装程序。\n3. 安装后配置 3.1 启动配置 启动之前要修改配置文件：\nvim ${HOME}/opt/SIMULIA/EstProducts/2024/linux_a64/SMA/site/custom_v6.env 在最末尾增加两行内容，保存退出：\nlicense_server_type=FLEXNET abaquslm_license_file=\u0026#34;29100@localhost\u0026#34; 新建环境变量配置文件：\ntouch ${HOME}/opt/SIMULIA/simulia24.env chmod +x ${HOME}/opt/SIMULIA/simulia24.env vim ${HOME}/opt/SIMULIA/simulia24.env 编辑内容如下，最好都用绝对路径，保存退出：\nexport LICENSE_PREFIX_DIR=${HOME}/opt/SIMULIA/License/2024/linux_a64/code/bin export SIMULIA_COMMAND_DIR=${HOME}/opt/SIMULIA/var/DassaultSystemes/SIMULIA/Commands export PATH=$SIMULIA_COMMAND_DIR:$LICENSE_PREFIX_DIR:$PATH export LM_LICENSE_FILE=29100@localhost 运行以下命令载入环境变量：\nsource ${HOME}/opt/SIMULIA/simulia24.env 通过以下命令启动Abaqus图形界面1：\nabaqus cae -mesa abaqus view -mesa 3.2 许可证安装问题 如果3.1 中Abaqus能正常启动，那么这一步骤可以跳过了。\n验证许可证服务器是否在运行：\nps -eaf | grep ABAQUSLM 发现许可证服务器没有运行，通过以下命令启动许可证服务器：\n${HOME}/opt/SIMULIA/License/2024/linux_a64/code/bin/licenseStartup.sh 启动未成功，报错信息如下：\n/home/***/opt/SIMULIA/License/2024/linux_a64/code/bin/licenseStartup.sh: 2: /home/***/opt/SIMULIA/License/2024/linux_a64/code/bin/lmgrd: not found 无解了，这要么是安装包有问题，要么是这个发行版缺少了某些运行库，等大佬出手吧。\n不过好在Windows版许可证可以正常安装，只需要将许可证路径指派到Windows机器上即可。首先在Windows防火墙中打开29100端口，新建防火墙入站规则： 然后将3.1 中custom_v6.env文件的许可证地址修改如下：\nlicense_server_type=FLEXNET # abaquslm_license_file=\u0026#34;29100@localhost\u0026#34; abaquslm_license_file=\u0026#34;29100@172.25.64.1\u0026#34; # 172.25.64.1为Windows主机IP地址 修改环境变量文件${HOME}/opt/SIMULIA/simulia24.env：\n# export LM_LICENSE_FILE=29100@localhost export LM_LICENSE_FILE=29100@172.25.64.1 载入环境变量后，通过abaqus cae命令可以正常启动，不会再提示许可证问题。 不得不承认，Linux的图形界面确实不好用，但是谁关心呢？反正又不会在Linux下画图、处理网格，能提交计算就行了。\n3.3 提交集群计算 Abaqus的Slurm脚本2参考如下：\n#!/bin/bash #SBATCH --job-name=abaqus_test #SBATCH --partition=debug #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 1 #SBATCH --ntasks-per-node=32 cd $SLURM_SUBMIT_DIR source /home/***/opt/SIMULIA/simulia24.env # 填写绝对路径 export INPFILE=`find . -name \u0026#34;*.inp\u0026#34;` export ENVFILE=/home/***/opt/SIMULIA/EstProducts/2024/linux_a64/SMA/site/abaqus_v6.env # 填写绝对路径 # 生成abaqus_6.env文件，指定hosts rm -rf $PWD/abaqus_v6.env cp $ENVFILE $PWD/abaqus_v6.env node_list=$(scontrol show hostname ${SLURM_NODELIST} | sort -u) mp_host_list=\u0026#34;[\u0026#34; for host in ${node_list}; do mp_host_list=\u0026#34;${mp_host_list}[\u0026#39;$host\u0026#39;, ${SLURM_NTASKS_PER_NODE}],\u0026#34; done mp_host_list=$(echo ${mp_host_list} | sed -e \u0026#34;s/,$/]/\u0026#34;) echo \u0026#34;mp_host_list=${mp_host_list}\u0026#34; \u0026gt;\u0026gt; $PWD/abaqus_v6.env # 创建Scratch目录 mkdir scratch.$SLURM_JOB_ID abaqus job=$SLURM_JOB_NAME input=$INPFILE cpus=$SLURM_NPROCS scratch=scratch.$SLURM_JOB_ID mp_mode=mpi double=both output_precision=full resultsformat=odb int ask=off \u0026gt; $SLURM_JOB_NAME.log rm -rf $PWD/abaqus_v6.env scratch.$SLURM_JOB_ID 将inp文件和脚本放到同一文件夹，使用sbatch命令提交脚本。计算完成后下载结果到本机，用Abaqus Viewer或Meta查看结果。\nfranaudo/abaqus-ubuntu \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nabhpc/ABHPC-Guide \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-21-ubuntu-install-simulia2024/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/zh-cn/tags/abaqus/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"},{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Slurm","RelPermalink":"/zh-cn/tags/slurm/"},{"LinkTitle":"Ubuntu","RelPermalink":"/zh-cn/tags/ubuntu/"}],"title":"Ubuntu安装SIMULIA2024"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"Slurm，和PBS、LSF一样，超算上常用的任务管理系统。Slurm优点是开源免费、活跃度很高，近几年国内新兴的超算平台几乎都提供了Slurm作为主要的任务管理系统。PBS开源后活跃度低得可怜，更新到最新系统后安装一直出问题，提了issue也不见答复。LSF有版权风险，国内应用也不多，属于很少见的类型。至于命令和脚本，这三家都大差不差，学会了其中一家另外的也是手到擒来。\nUbuntu安装Slurm还是十分简单的，重要的工具基本都编译好了，直接apt安装即可，其他依赖项会自动安装：\nsudo apt install slurmd\t# 安装计算节点守护进程 sudo apt install slurmctld # 安装管理节点守护进程 Slurm需要有一个专门的用户用于通信等操作，这个用户的默认用户名是slurm，上面的命令其实已经自动在Ubuntu中生成了slurm用户，可以通过下面的命令验证：\nlastlog | grep slurm 如果Ubuntu没有生成slurm用户，可以用以下命令生成：\nsudo useradd slurm Slurm配置文件主要在 /etc/slurm/ 目录下，主配置文件：/etc/slurm/slurm.conf，我们需要生成配置文件。官方提供了辅助生成配置文件的工具：Slurm Configuration Tool 。\n根据网页内容提示，填写配置文件其中一些关键部分：\nClusterName=Cluster # 集群命名，任意英文和数字组合 SlurmctldHost=dell-vm # 管理节点，这里填本机名称 NodeName=dell-vm # 计算节点，同样填本机名称 PartitionName=debug # 计算节点所在分区，默认为debug CPUs=32 # 计算节点CPU核心数，根据实际情况填写 Sockets=1 # CPU插槽数，根据实际情况填写 CoresPerSocket＝32 # 每插槽核心数，根据实际情况填写 ThreadsPerCore=1 # 每核心线程数，建议为1，不建议打开超线程 SlurmUser=slurm # 默认为slurm用户，不建议改成root用户 StateSaveLocation=/var/spool/slurmctld # 管理节点守护进程的存储文件夹，默认即可 SlurmdSpoolDir=/var/spool/slurmd # 计算点守护进程的存储文件夹，默认即可 更多解释可以参考中科大网站上的信息。(1)\n网页内容填写完成后点击最下面的Submit，把显示的配置文件模板拷贝下来，存到/etc/slurm/slurm.conf文件中：\nsudo vim /etc/slurm/slurm.conf\t# 复制粘贴到这个文件里 生成守护进程的读写文件夹：\nsudo mkdir /var/spool/slurmd # Ubuntu下提示文件夹已存在，无视它 sudo mkdir /var/spool/slurmctld 启动Slurm的服务：\nsudo systemctl enable slurmd sudo systemctl enable slurmctld sudo systemctl start slurmd sudo systemctl start slurmctld 查看Slurm守护进程的启动状态：\nsudo systemctl status slurmd sudo systemctl status slurmctld slurmd守护进程启动成功，但slurmctld守护进程启动报错，查看报错信息如下：\n(null): _log_init: Unable to open logfile `/var/log/slurmctld.log\u0026#39;: Permission denied slurmctld: fatal: Incorrect permissions on state save loc: /var/spool/slurmctld 为解决这个问题，最简单办法是将SlurmUser改为root。这里采用另外一种方法：\nsudo touch /var/log/slurmctld.log # 创建slurmctld守护进程的日志文件 sudo chown slurm /var/log/slurmctld.log # 将日志文件所有者改为slurm用户 sudo chown -R slurm /var/spool/slurmctld # 将slurmctld守护进程读写文件夹的所有者改为slurm用户 重新启动slurmctld服务即可：\nsudo systemctl restart slurmctld Slurm脚本和命令行，国内的用户可以参考交大编写的用户手册，比较全面，这里就不一一列举了。(2)\n以下是一些常用Slurm命令：\n当前节点的配置可以通过以下命令获取：\nslurmd -C 查看当前集群的节点状态：\nsinfo -N 查看指定节点信息：\nscontrol show node dell-vm # dell-vm是计算节点的名称 查看当前用户提交的任务信息，通常只显示正在排队和运行中的任务：\nsqueue 提交计算任务：\nsbatch jobscript.slurm # jobscript.slurm为用户编写的计算脚本，可不带后缀名 查看和修改任务状态：\nscontrol show job ${JOB_ID} # 查看指定任务的信息，${JOB_ID}对应squeue显示的第一列的任务编号 scontrol hold ${JOB_ID} # 暂停${JOB_ID} scontrol release ${JOB_ID} # 恢复${JOB_ID} 取消计算任务：\nscancel ${JOB_ID} # 取消${JOB_ID} (1) Slurm资源管理与作业调度系统安装配置 (2) Slurm 作业调度系统 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-20-ubuntu-install-slurm/","tags":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Slurm","RelPermalink":"/zh-cn/tags/slurm/"},{"LinkTitle":"Ubuntu","RelPermalink":"/zh-cn/tags/ubuntu/"}],"title":"Ubuntu安装Slurm"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"在虚拟机中运行计算文件，会导致虚拟磁盘膨胀，占用太多磁盘空间。这个时候可以通过挂载宿主机文件夹的形式，把计算文件转移到宿主机磁盘上，避免了虚拟磁盘膨胀的问题。在Windows中建立共享文件夹，这里省略了，只需要确保虚拟机能通过IP地址访问宿主机即可。\n1. 查看资源路径 以下命令查看服务器共享出来的资源路径，确认挂载点：\nsmbclient -L 172.25.64.1 -U ${username} 挂载点访问路径：//172.25.64.1/Share\n2. 挂载方法 想要在Ubuntu中访问Windows共享文件夹，首先得安装cifs工具：\nsudo apt install cifs-utils 然后通过mount命令挂载共享文件夹：\nsudo mount -t cifs //172.25.64.1/Share /mnt -o username=${username},password=${password} 这里的IP地址172.25.64.1是虚拟机中访问的宿主机的网关地址，Share是宿主机共享的文件夹，/mnt是要挂载到的虚拟机本地访问路径，把命令后面的${username}和${password}替换成访问用户名和密码即可。需要注意的是，Windows本地用户的用户名需要写成${计算机名}\\${用户名}的形式，用反斜杠连接，例如：xxx-desktop\\administrator。如果是在线账户的话就需要填写完整的邮件账户名称。如果密码中包含逗号等特殊转义字符的话，命令行就不要包含,password=及后面的内容，后续根据提示输入密码登录。\n如果出现无读写权限的问题，挂载命令中增加dir_mode=0777,file_mode=0777：\nsudo mount -t cifs //172.25.64.1/Share /mnt -o dir_mode=0777,file_mode=0777,username=${username},password=${password} 如果想只添加某些特定用户的读写权限，通过uid和gid指定用户和组：\nsudo mount -t cifs //172.25.64.1/Share /mnt -o uid=user,gid=group,username=${username},password=${password} 通过mount命令可以查看挂载情况：\nmount | grep cifs 取消挂载通过umount命令：\nsudo umount /mnt 如果想开机自动挂载的话，就需要编辑/etc/fstab，内容如下:\n//172.25.64.1/Share /mnt cifs auto,dir_mode=0777,file_mode=0777,username=${username},password=${password} 0 0 3. 特殊字符密码 对于密码含有特殊转义字符的情况，要在Linux开机时实现自动挂载Windows共享文件夹，可以采取以下方法1：\n创建凭证文件：为保持密码的安全性，最好将Windows共享的用户名和密码保存在一个只有root权限能访问的文件中，例如：/etc/cifs-credentials，并确保它的权限设置为仅root可读。\nsudo touch /etc/cifs-credentials sudo chmod 600 /etc/cifs-credentials 使用文本编辑器编辑该文件， 如果密码中包含特殊字符，直接在文件中输入即可(无需转义)，写入用户名（xxx-desktop\\administrator）和密码（123456,abcde）:\nusername=xxx-desktop\\administrator password=123456,abcde 编辑 /etc/fstab 文件：打开 /etc/fstab 文件，在文件末尾添加如下内容，以包含挂载信息：\n//172.25.64.1/Share /mnt cifs credentials=/etc/cifs-credentials,iocharset=utf8,file_mode=0777,dir_mode=0777 0 0 注意/etc/cifs-credentials文件的编码需要是UTF-8。\n以上便完成了开机自动挂载设置，重启后可以通过df -h验证。 4. 权限问题 使用cifs挂载Windows的共享文件夹，chmod和chown等命令失效，无法调整被挂载的文件和文件夹权限。这里采用NFS(网络文件系统)挂载共享文件夹以解决此问题。\n默认情况下，NFS并不提供任何验证机制，因此不需要验证用户名密码，存在一定的安全风险。NFSv3根据客户端IP地址完成验证2，可以通过指定客户端IP地址的方式提高安全性。\nWindows10可以通过第三方工具haneWIN3创建NFS共享文件夹，下载安装并通过图形界面配置即可，这里不详细介绍，可以参考其他相关文章4。注意配置完成后要在防火墙中开放相关端口。 在Ubuntu上安装NFS相关工具，开启相关服务：\nsudo apt install nfs-common rpcbind sudo systemctl start rpcbind sudo systemctl enable rpcbind 使用mount命令挂载共享目录。注意后面的-t nfs：\nsudo mount -t nfs 172.25.64.1:/Share /mnt 取消挂载命令和cifs是一样的，当网络状态突然中断时可以增加-lf开关：\nsudo umount -lf /mnt 编辑/etc/fstab，添加开机自动挂载：\n172.25.64.1:/Share\t/mnt\tnfs\tdefaults,_netdev 0 0 Linux开机自动挂载window密码有转义字符的共享文件夹 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNFS身份验证 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhaneWIN \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWindows服务器使用haneWIN NFS Server快速搭建NFS服务并挂载到Linux服务器 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-20-ubuntu-mount-windows-share-dir/","tags":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Ubuntu","RelPermalink":"/zh-cn/tags/ubuntu/"}],"title":"Ubuntu挂载Windows共享文件夹(cifs+nfs)"},{"categories":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/categories/linux/"}],"content":"1. 需求 考虑在新电脑上安装Linux，不是双系统，因为还要满足日常办公。不喜欢折腾的可以用WSL，这里用Hyper-V实现，同时通过端口映射实现外网访问虚拟机。\n2. 准备工作 2.1 下载Ubuntu 笔者是做CFD的，自然离不开Fluent，这玩意儿挑发行版的。从官网资料确认支持哪个发行版，支持哪个就装哪个，免得后面倒腾回来重装系统。\nAnsys Computing Platform Support 2024R1 这里选择Ubuntu，前往官网下载最新的发行版。\n2.2 开启Hyper-V支持 在开始菜单搜索“启用或关闭Windows功能”，开启虚拟化支持。\n把Hyper-V勾上，安装并重启就行了。\n3. 安装虚拟机系统 Hyper-V启动界面，跟着向导一步步新建虚拟机即可。\n需要注意几点：\n虚拟机和虚拟磁盘最好指定在其他空余空间比较多的分区上，后面运行频繁读写会使虚拟磁盘文件膨胀的很大。 第一次启动前最大内存分配小一点，不然启动很花时间，建议安装并配置完虚拟机系统后再调整到想要的内存大小。 虚拟机设置可以打开TPM，但是要关掉安全启动（除非启用Microsoft UEFI证书颁发机构），否则无法载入安装盘。 Ubuntu安装过程不仔细说明了，跟着向导界面一步步安装即可。\n4. 配置虚拟机网格 4.1 虚拟机分配IP地址 我们需要通过端口映射访问虚拟机，因此需要给虚拟机指派一个固定的IP地址。由于虚拟机用的是默认的Default Switch桥接设置，需要查看宿主机给改适配器指派的IP地址，在网络连接选项中可以查看： 上图显示的地址就是虚拟机访问宿主机的IP地址。接着在虚拟机中指派一个固定地址，网关和DNS填上面宿主机的IP地址，子网掩码保持一致，IP地址建议就用现在指派的IP地址。 设置完成后ping一下google的DNS看看是否能正常联网：\nping 8.8.8.8 4.2 虚拟机开启远程桌面 Ubuntu提供了图形界面配置远程桌面，这里不作介绍了。下面是通过命令行配置RPD远程桌面的内容。安装第三方软件：\nsudo apt install xrdp 启用 XRDP 服务：\nsudo systemctl enable xrdp sudo systemctl start xrdp 检查服务状态： 启用 XRDP 服务：\nsudo systemctl sattus xrdp 4.3 检查虚拟机防火墙状态 Ubuntu 通常是 ufw(Uncomplicated Firewall)，以下命令检查系统上的防火墙是否已启用并显示其当前配置。\nsudo ufw status 如果防火墙尚未启用，以下命令启用防火墙：\nsudo ufw enable 以下命令在防火墙中打开3389端口：\nsudo ufw allow from any to any port 3389 proto tcp 这时候我们就可以通过远程桌面，而不是虚拟机的小窗口连接Ubuntu虚拟机了。提示：Ubuntu需要先在虚拟机窗口中注销用户，才能使用远程连接，否则会出现黑屏、闪退的问题。不知道这个bug何时能修复。\n5. 开启虚拟机相关服务 5.1 虚拟机开启SSH服务 安装 OpenSSH ：\nsudo apt install openssh-server 检查 SSH 服务器状态：\nsudo systemctl status ssh 如果输出显示Active: active (running)，表示 SSH 服务器正在运行。\n如果ssh服务显示Active: inactive (dead)，通过以下命令开启ssh服务：\nsudo systemctl enable ssh sudo systemctl start ssh OpenSSH服务器的配置文件默认位于/etc/ssh/sshd_config。用户可以根据需要修此配置文件来更改相关配置，例如监听端口、允许或禁止密码登录、限制登录用户等。\n5.2 虚拟机添加防火墙规则 以下命令在防火墙中添加 SSH 规则：\nsudo ufw allow OpenSSH 以下状态显示防火墙配置成功： 5.3 测试SSH连接 Ubuntu虚拟机的IP地址可以在Hyper-V管理器的窗口中查看，也可以在虚拟机中通过以下命令获取：\nip addr show | grep inet ip a | grep inet\t#这两条命令效果一样 使用bash或powershell客户端，通过以下命令连接到服务器，将username和ip_address分别替换为虚拟机的用户名和IP地址即可，会提示输入密码：\nssh username@ip_address 5.4 测试SFTP连接 启用ssh之后会默认开通sftp，端口号和ssh一样都是22，通过以下命令连接sftp，根据提示输入密码即可登录：\nsftp username@ip_address 上传文件put： 把本地服务器的D:\\temp\\test目录下面的test.txt文件上传到远程服务器的/home/username/test目录下。\nsftp\u0026gt; lcd D:/temp/test sftp\u0026gt; cd /home/username/test sftp\u0026gt; put test.txt 上传文件夹put：把本地服务器的D:\\temp\\test目录下面的logs文件夹上传到远程服务器的/home/username/test目录下。\nsftp\u0026gt; lcd D:/temp/test sftp\u0026gt; cd /home/username/test sftp\u0026gt; put -r logs 下载命令：get，用法与put类似。 sftp常用命令可以通过help查看。建议通过第三方工具，比如FileZilla来登录操作。\n6. 端口映射 6.1 宿主机端口映射 首先要查看宿主机的端口占用情况。在PowerShell或CMD中通过以下命令查看：\nnetstat -ano #查看所有端口 netstat -ano | findstr 8022 #8022为查询的端口号 tasklist | findstr 5748 #5748指的是8022端口对应的pid，查看占用该端口的程序 宿主机中用管理员权限打开PowerShell或CMD窗口，通过以下命令查询、添加、删除端口映射。\n# 查询端口映射 netsh interface portproxy show v4tov4 # 查询指定IP端口映射 netsh interface portproxy show v4tov4|findstr \u0026#34;192.168.100.135\u0026#34; \u0026lt;# 增加一个端口映射 netsh interface portproxy add v4tov4 listenport=宿主机端口 listenaddress=宿主机IP connectaddress=虚拟机IP connectport=虚拟机端口 #\u0026gt; # 通过宿主机8022端口映射虚拟机22端口，访问SSH netsh interface portproxy add v4tov4 listenport=8022 listenaddress=192.168.100.135 connectaddress=172.25.68.88 connectport=22 # 通过宿主机63389、63390端口映射虚拟机3389、3390端口，访问远程桌面 # 端口号范围：1-65535，不能超出该范围 netsh interface portproxy add v4tov4 listenport=63389 listenaddress=192.168.100.135 connectaddress=172.25.68.88 connectport=3389 netsh interface portproxy add v4tov4 listenport=63390 listenaddress=192.168.100.135 connectaddress=172.25.68.88 connectport=3390 \u0026lt;# 删除一个端口映射 netsh interface portproxy delete v4tov4 listenaddress=宿主机IP listenport=宿主机端口 #\u0026gt; # 删除上面定义的端口映射 netsh interface portproxy delete v4tov4 listenaddress=192.168.100.135 listenport=8022 netsh interface portproxy delete v4tov4 listenaddress=192.168.100.135 listenport=63389 netsh interface portproxy delete v4tov4 listenaddress=192.168.100.135 listenport=63390 配置好端口映射后，就可以在远程桌面中通过访问IP地址:端口号的形式连接到虚拟机桌面了。\n6.2 宿主机防火墙设置 宿主机防火墙要开放端口，这样才能通过外网访问。首先打开Windows Defender防火墙，在高级设置里面新建入站规则。 规则类型选择端口，端口号输入上面映射的端口(用英文半角逗号隔开)，后面全部确认即可。\n到此为止，终于实现外网用户通过宿主机的IP地址+端口号的形式访问该虚拟机了。\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-19-hyperv-install-ubuntu24/","tags":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Ubuntu","RelPermalink":"/zh-cn/tags/ubuntu/"},{"LinkTitle":"Virtual-Machine","RelPermalink":"/zh-cn/tags/virtual-machine/"}],"title":"Hyper-V安装Ubuntu24.04"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"由于近期经常做一些冷板结构分析，涉及到薄板建模的。虽然尝试过用Shell单元进行分析，但相比于Solid单元，Shell单元做出来的结果应力、位移均有所偏大，而且涉及到接触问题更容易出现计算不收敛的情况。所幸Ansa提供了通过Shell单元拉伸生成体网格的方式，可以快速通过Shell单元生成Solid网格，极大地节省了建模时间。\n建模步骤如下：\n薄板实体简化为Shell并划分网格，可以参考Ansa抽中面功能 。 Volume Mesh→Extrude调出Extrude窗口，选中要拉伸的Shell单元。 目标面选择None。 拉伸方式选择Offse，方向选择Both sides (middle)。 Steps选择层数，Distance输入薄板厚度，Biasing建议选择Smooth，Finish完成。 检查生成的网格质量，修复差网格。 总结：这种方式比较适合厚度均匀的薄板生成体网格。但是当薄板厚度不均匀，或者存在T型连接（例如注塑件筋条等，如下图）的时候，就不适用了。 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-18-ansa-thin-mesh/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Ansa薄板网格建模"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"1. 相关命令 Ansa中，给Abaqus设置接触一般常用以下命令： 2. 接触对 最常用的接触对，一般通过AUXILIARIES→ CONTACT→CONTACT调出创建对话框。 常用设置一般为以下几个：\n定义接触名称，方便检索 定义接触类型为接触对(CONTACT PAIR)或者通用接触(CONTACT INCLUSIONS或CONTACT EXCLUSIONS) 接触从面，需要预先在SET里设置好单元集合 接触主面，需要预先在SET里设置好单元集合 接触行为，定义摩擦系数等 接触面调整行为，选择YES可以在后面的POS_TOLER定义容差数值 接触面是否产生小滑移，默认NO 接触类型，默认surface-to-surface，或者node-to-surface 接触间隙，定义接触的过盈或间隙值 3. 通用接触 跟创建接触对共用同一个命令和对话框，只需要把接触类型改为接触包含面(CONTACT INCLUSIONS)或接触排除面(CONTACT EXCLUSIONS)即可。 定义接触类型 接触从面，选择SET里设置好的单元集合 接触主面，选择SET里设置好的单元集合（一般不需要，可以留空） 接触行为 4. 绑定接触 通过AUXILIARIES→ CONTACT→TIE调出对话框。 接触从面，选择SET里设置好的单元集合 接触主面，选择SET里设置好的单元集合 当两个接触面存在不重合时，可以在这里设置接触容差 5.接触向导 Ansa内置的功能，可以通过引导建立接触。通过AUXILIARIES→ CONTACT→Assistant命令调出。 6. 自动创建接触对 通过AUXILIARIES→ CONTACT→Flanges调出。 选择要搜索的对象 定义搜索距离容差 在弹出对话框里面检查、定义接触参数。 7. 与接触有关的输出设置 一般在STEP的OUTPUT选项卡里面定义，没有特殊要求的话输出参数选择PRESELECT就行了。 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-10-abaqus-contact/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/zh-cn/tags/abaqus/"},{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Abaqus接触设置"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"用Ansa输出inp文件之后，可以通过Abaqus CAE窗口界面提交inp文件进行计算，也可以通过命令行方式提交计算。使用命令行提交需要添加Path，Windows下通常是[Abaqus安装目录]\\Commands路径。\n其实Windows下装完Abaqus后，开始菜单会出现Abaqus Command快捷方式，点击该快捷方式可以通过命令行提交abaqus计算文件，不需要添加Path。 使用以下命令行提交inp计算文件。\nabaqus job=[文件名] cpus=[核心数] double=both output_precision=full int ask=off 通常来说通过job指定文件名就可以开始计算了，其他一些命令开关不是必要选项，它们的含义如下：\nhelp →查看帮助 double={ explicit | both | off | constraint } → 计算精度 output_precision={ single | full } → 输出结果精度 int / interactive → 打印求解过程 ask=off → 不询问直接覆盖 memory=5gb → 指定最大运行内存5GB scratch=D:\\Temp → 指定计算临时文件存储地 参考资料：https://abaqus-docs.mit.edu/2017/English/SIMACAEEXCRefMap/simaexc-c-analysisproc.htm\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-06-abaqus-command/","tags":[{"LinkTitle":"Abaqus","RelPermalink":"/zh-cn/tags/abaqus/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Abaqus提交命令"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"Ansa导入step模型默认长度单位是mm，重量是ton。但我们常用的SI单位制长度是m，重量是kg。关于两种单位的转换如下表所示。\n长度 质量 时间 力 应力 能量 密度 杨氏模量 速度 (56.3KPH) 重力 m kg s N Pa Joule 7.83E+03 2.07E+11 15.64 9.81 mm ton s N MPa N-mm 7.83E-09 2.07E+05 1.564E+04 9.81E+03 在定义模型参数时，应注意根据长度选择对应的单位制。\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-05-ansa-unit/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Ansa单位制"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"Ansa可以通过Connection功能快速设置连接。\n这里以螺接为例，首先将所有螺栓单独显示出来。使用TOPO→Curves→Tubes2Curve将螺栓几何转换成曲线。 保留曲线，将螺栓几何曲面全删掉。 再使用Topo→Points→Edges将曲线转换成3D点，删除掉曲线。 根据3D点生成螺栓连接点，框选上一步生成的3D点。除了根据曲线定义连接点意外，还可以根据点和面生成连接点。但不同的几何形式对应不同的连接类型，应注意根据需要的连接类型选择对应的几何。这里列出一些常用的连接类型和几何对应关系：\n3D点：螺接、铆接、焊点 线段：粘胶、焊缝、包边、螺接(直线段) 面：粘胶 螺栓连接点可以根据3D点或直线段生成，这里选择用3D点生成。 在模型树中可以看到生成的连接类型为螺栓。 将所有的螺栓连接点都扔到一个Part里，Part显示为连接类型。 选择连接管理器Connection Manager，框选要定义的连接点然后鼠标中键，进入连接定义对话框 连接方式选择BOLT，搜索距离根据几何尺寸填写。框选部分选择需要连接的PID，双击进入编辑然后输入英文半角符号?可以打开选择对话框，最多支持4个PID连接。这里ANSA默认连接方式是按Part搜索，需要手动改成PID。补充螺栓半径和其他连接信息，按Realize生成连接。 BEAM需要设置材料和截面尺寸，在弹出的PID对话框中设置。 连接效果如下。 PS：如果是SHELL网格，可以通过上面的3D点生成的螺栓连接点的方式生成螺接网格（如上图）。但如果是实体网格的话，就必须要通过直线段生成的螺栓连接点连接螺栓孔，生成带方向的连接点，连接类型还必须选择BOLT ON SOLID。 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-03-05-ansa-connector/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Ansa快速设置连接"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"使用Ansa建面网格的时候我们都希望螺钉孔能处理成washer网格的形式，方便后面加约束。 但默认设置生成的网格螺钉孔貌似都没有按washer处理。 有几种方法，其一是通过几何处理手动添加Zone Cut的方式，这种方法手工处理工作量太大，不推荐\n另一种方法需要手动Reconstruct网格，首先在网格参数设置里设置孔的特征参数。 返回Mesh里，Reconstrust就有了。 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-02-28-ansa-washer-mesh/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Ansa螺钉孔washer网格"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"ANSA针对钣金件抽中面有几种不同处理方式。\n1. Skin方法 TOPO→Mid.Surfa.→Skin，选中整个几何体然后无脑按鼠标中键就可以自动抽中面。 适合一些厚度比较均匀的钣金结构件，没有筋条、凸台、凹坑等特征。 抽出来的中面是几何不带网格，需要手动画网格，会自动删除原来的几何并添加厚度。 对一些挤压件像挤压铝型材也支持，但是有圆角的话要先处理掉，不然抽出来的中面圆角会保留并放大。 2. Casting/Extrusion方法 还有一种针对铸造和挤压件的抽中面方法，TOPO→Mid.Surfa.→Casting/Extrusion 首先要确保几何是封闭的，没有错误面和边。 在弹出选项卡选好要抽中面的零部件是铸造件还是挤压件，定义好最小厚度和面网格尺寸。抽中面之前最好先在网格参数中定义好特征孔参数，用于生成washer网格，因为抽完面之后就直接是面网格。 最后生成的网格，厚度根据几何形状自动计算，不需要在PID里再设置厚度。 3. Middle Multi.手工处理 很麻烦，不推荐。但是对于一些复杂结构件，比如注塑件等，只能用这种办法。 首先要把一些和筋条等特征共面的曲面按拓扑分割，方便后面处理。 用红色和绿色标注中面的正反面，蓝色是侧面。 抽完的中面会自动隐藏，方便处理剩下的模型。 4. 总结 Skin方法：简单，适合钣金件，生成几何数据，精度好 Casting/Extrusion方法：简单，适合铸造和挤压件，生成网格数据，精度稍差 Middle Multi.手工处理：繁琐，但适合复杂结构件如注塑件等，生成几何数据，精度因人而异 ","permalink":"https://andrewmoa.site/zh-cn/post/2025-02-27-ansa-midsurf/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Ansa抽中面功能"},{"categories":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/categories/ansa/"}],"content":"快捷键F11，或者下面这个按钮，定义Ansa网格质量 面网格质量（Abaqus） 体网格质量（Abaqus） 最好把配置文件另存为到本地，下次重新打开配置文件覆盖即可。\n","permalink":"https://andrewmoa.site/zh-cn/post/2025-02-27-ansa-mesh-quelity/","tags":[{"LinkTitle":"Ansa","RelPermalink":"/zh-cn/tags/ansa/"},{"LinkTitle":"Cae","RelPermalink":"/zh-cn/tags/cae/"}],"title":"Ansa网格质量设置"},{"categories":[{"LinkTitle":"Zhihu","RelPermalink":"/zh-cn/categories/zhihu/"}],"content":"最近提供超算试用的平台挺多的，很多平台都有免费试用的申请。因工作需要申请了某超算平台的账号并进行了相关的试用，就超算平台部署STAR-CCM+软件及应用的过程做一个简单的记录，也为后续相关应用提供参考。\n1. 超算平台信息 远程登陆超算可以通过SSH连接，某些平台还提供的webSSH、webVNC连接，支持通过浏览器连接命令行或图形界面。具体登陆方式请参考平台提供的相关文档。\n首次登陆安装部署软件之前应当先了解超算平台的配置，确定平台是否支持需要安装的软件。通过以下命令了解超算平台的发行版信息。\nlsb_release -a 可以了解到该平台发行版为CentOS，版本7.9.2009。\n该超算平台所用并行作业调度系统为开源的Slurm，可以通过以下命令查看可供调用的计算资源。\nsinfo -a 输出比较长，这里只截了一部分。下图中amd_256表示计算节点所在分区，记住它，后面编写脚本会用到。\n2. 软件安装 软件上传及存储请参考平台提供的相关文档。\n本文安装的是16.06.010双精度Linux版本。通过以下命令解压tar.gz安装包。\ntar xvzf [file-name].tar.gz 安装文件被解压到starccm+_16.06.010目录中，进入该目录运行.sh文件开始安装。注意，此处不需要root用户权限（多数情况下平台是不会提供root账号的，但不影响软件安装）。\n./STAR-CCM+16.06.010_01_linux-x86_64-2.17_gnu9.2-r8.sh 用VNC连接的可以通过图形界面安装，不想通过图形界面安装可以用以下命令强制通过控制台安装。\n./STAR-CCM+16.06.010_01_linux-x86_64-2.17_gnu9.2-r8.sh -i console 本文采用控制台方式进行安装。首先提示LICENSE，如下图所示，按ENTER继续。\n是否接受用户协议，输入Y，ENTER确认继续。\n用户体验计划，根据自己需要选择是否接受(Y/N)，不影响后续使用。\n安装位置，本文选择安装在${HOME}/opt/Siemens目录下，按提示输入绝对路径，Y确认。\n安装信息，ENTER确认，开始复制文件。\n安装完成，ENTER确认退出。记住安装路径：\n${HOME}/opt/Siemens/16.06.010-R8/STAR-CCM+16.06.010-R8\n和谐过程就免了，自行参考文档吧。支持正版，打击盗版。\n3. 编制SLURM脚本 下面编写SLURM脚本，提交算例简单测试一下。\n#!/bin/bash #SBATCH --job-name=carbin_tcm #SBATCH --partition=amd_256 #SBATCH --output=%j.out #SBATCH --error=%j.err #SBATCH -N 2 #SBATCH --ntasks-per-node=64 export MPI_TYPE=openmpi # intel platform openmpi export DIR=/***/home/***/opt/Siemens/16.06.010-R8/STAR-CCM+16.06.010-R8/star/bin export CDLMD_LICENSE_FILE=/***/home/***/opt/Siemens/license.dat export SIM_FILE=carbin_tcm.sim #export JAVA_FILE=carbin_tcm.java export MACHINEFILE=$SLURM_JOBID.node scontrol show hostnames $SLURM_JOB_NODELIST \u0026gt; $MACHINEFILE $DIR/starccm+ $SIM_FILE -batch $JAVA_FILE -np $SLURM_NPROCS -machinefile $MACHINEFILE -mpi $MPI_TYPE -rsh ssh -power --job-name指定的的是案例的名称，可以在squeue命令中显示的名称。\n--partition指定的是计算节点所在分区，这里调用的是amd_256分区中的计算节点。\n--output指定输出文件名称，%j或$SLURM_JOBID表示当前作业ID，由平台自行指定。\n--error指定输出错误文件名称。\n-N指定该案例调用的节点数，这里调用2个计算节点。\n--ntasks-per-node指定每节点进程数，这里指定每节点调用64线程。\n变量$SLURM_NPROCS表示总的计算进程数，可以根据以上两个参数自动计算，总的计算进程为128。\n参数MPI_TYPE指定调用mpi类型，推荐用intel或openmpi，platform高版本不再支持。\n参数DIR指定STAR-CCM+安装路径，就是starccm+文件所在的路径。\n参数CDLMD_LICENSE_FILE指定LICENSE的访问路径，可以是文件路径也可以是端口号@主机地址。\n参数SIM_FILE指定测试案例文件名。\n参数JAVA_FILE指定宏文件名。如果使用了宏，可以把这一行的注释去掉，把文件名改成调用的宏文件名即可。\n参数MACHINEFILE指定节点文件。\n命令scontrol show hostnames SLURM_JOB_NODELIST \u0026gt; MACHINEFILE用于输出主机名到节点文件。\n保存算例脚本文件为carbin_tcm.slurm，测试案例文件为carbin_tcm.sim，一共2个文件，上传。计算时间步长、迭代次数等要在.sim文件中先定义好，生成网格、配置好边界条件再行上传计算。\n如果算例文件比较大，可以压缩上传再解压。也可以清除掉网格再上传，通过调用宏重新生成网格、定义边界条件、计算。\n4. 提交计算任务 通过sbatch命令提交计算任务。\nsbatch carbin_tcm.slurm 提交后自动生成ID、排队，本算例ID号为899634。\n通过squeue命令查看计算任务队列。\n计算完成后，打包下载输出文件即可。\n","permalink":"https://andrewmoa.site/zh-cn/post/2022-04-08-deploy-starccm-on-scp/","tags":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/tags/cfd/"},{"LinkTitle":"Slurm","RelPermalink":"/zh-cn/tags/slurm/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/zh-cn/tags/star-ccm+/"}],"title":"超算平台部署STAR-CCM+"},{"categories":[{"LinkTitle":"Zhihu","RelPermalink":"/zh-cn/categories/zhihu/"}],"content":"先说结论：十沣科技产品QFlux和QMesh与自身宣传不符。QMesh并不是什么“自主开发的先进计算流体力学核心软件”；而QFlux功能根本就尚未完善，至少没有包含其宣传所拥有的功能，而且笔者有理由怀疑其求解器根本就是“借鉴”自OpenFOAM。\n1. 软件介绍及下载 废话不多说，关于十沣科技的宣传可以从其官网（http://www.tenfong.cn/、http://www.qfx-tech.com/）上窥知一二。\n该公司官网上的两个产品：QFlux和QMesh，前一个是CFD求解器、后一个是网格前处理器。感兴趣的同学可以在官网上注册然后登录下载。\n不知是不是浏览器的问题，点击下载按钮后并不会弹出下载信息确认窗口，而是直接在浏览器后台下载，用户需要等到下载完成后到临时文件夹（Windows：C:\\Users\\[用户名]]\\AppData\\Local\\Temp）里才能找到下载的程序安装文件。不仔细找找还真找不到，也不知道是官方是不是故意的呢。\n解压之后会得到QFlux_x64.exe及QMesh_x64.exe两个安装文件，分别安装之。安装过程没什么复杂的，这里就不细讲了。\n2. 前处理软件QMesh 按照惯例，先扒一扒前处理软件QMesh。\n可以看到QMesh网格显示方面是通过vtk库来实现的，界面则是通过Qt来实现。也不知道该公司有没有购买Qt的商业授权，要知道Qt的三种授权模式分别是GPL、LGPL和商业授权，笔者反正是没看到安装文件有声明引用Qt及其相关类库。\nQMesh网格生成器的核心是snappyHexMesh，熟悉OpenFOAM的朋友应该都知道，这是OpenFOAM自带的网格生成器。\nQMesh软件的帮助文件倒是很大方地承认了这一点。但是官网上宣传的不是“QMESH 是深圳十沣科技有限公司自主开发的先进计算流体力学核心软件”吗？核心的网格生成器引用别人的工具，这算哪门子的自主开发？合着就“自主”开发了一个GUI界面？然后还用了别人的类库？\n首次进入会提示试用期180天，软件界面中归中距。帮助文件关于里显示软件版本是2.1.3，版权方为“深圳清沣溪有限公司”。\n接下来用Fluent官方的算例elbow来试着进行建模及计算。上图是用ANSA导出的STL格式文件（ASCII格式），已经分好面。\nQMesh只支持导入STL格式曲面，但导入STL文件后无法识别已经分好的面，也无法对实体进行重新分区等操作。\nQMesh通过“区域”选项设置背景网格的区域，这点与OpenFOAM中的blockMesh类似，但不足的是不能调整背景网格的大小。需要指定材料点，也就是OpenFOAM中snappyHexMeshDict文件的locationInMesh关键字，熟悉OpenFOAM的同学应该都清楚。\nQMesh通过“控制”选项设置贴体网格及边界层的控制参数，基本对应OpenFOAM中snappyHexMeshDict文件的castellatedMeshControls、snapControls及addLayersControls等所包含的内容。\n点击“剖分网格”，在弹出会话框可以选择并行设置相关参数以及选择剖分体网格及边界层。\n通过任务管理器可以看到，QMesh实际上是通过调用snappyHexMesh进行网格生成操作的。\n由于是调用用snappyHexMesh进行网格生成操作的，因此不可避免地将snappyHexMesh的一些固有缺陷也带入其中。可以看到局部网格出现畸形，这其实是snappyHexMesh的缺陷所导致的，至今依然没有很好的解决办法。由于不能调整背景网格大小，因此当局部网格与背景网格相互重合时，有可能生成新的边界区域，就像上图中的y-minus区域一样。\n将QMesh生成的网格导出elbow.msh（Fluent Mesh格式）及elbow.cgns（CGNS通用格式），QMesh工程文件保存格式为.qmh。可以看到elbow.cgns大小只有4kB，明显CGNS格式保存得不成功。唉~，软件能做成这样，估计都没经过测试就直接发出来了。\n3. CFD求解器QFlux QFlux的程序文件和QMesh类似，底层是通过Qt和vtk来实现得，笔者依然没有看到相关引用类库的声明。\nQFlux核心求解器文件是qcore，这个留到后面再讲。\nQFlux软件界面，版本号显示是2.7.152.0，版权方变成了“深圳十沣科技有限公司”。\n试着导入前面QMesh生成的elbow.msh，结果导入失败，识别不了。真是大水冲了龙王庙——一家人不认一家人(～￣▽￣)～。至于QMesh生成的elbow.cgns，很明显保存失败，不用费劲巴拉去试了。\n导入ANSA生成的cgns文件，已经分好边界。\nQFlux导入CGNS网格之后依然没有识别出边界信息，边界条件里面只有一个边界，不能作边界分割的操作。（PS：依然是半成品呀~，这么草率就发出来吗？）\n安装包提供了OFoamToQFlux程序用于转换OpenFOAM网格，好在笔者用OpenFOAM计算过相关案例。打开CMD命令行窗口，执行以下命令转换OpenFOAM网格。\ncd /d D:\\opt\\tenfong\\QFlux #[QFlux安装路径] set path=%cd%;%path% cd /d F:\\tenfong\\solve #[OpenFOAM文件所在目录] OFoamToQFlux %cd%\\[OpenFOAM文件的Case目录]\\constant\\polyMesh out.qfx 这样即生成了QFlux的网格文件，并保存城QFlux格式(.qfx)。直接打开out.qfx文件即可。\n可以看到这回显示了边界信息，边界条件后面再慢慢改。\n计算案例取自Ansys Fluent的经典案例elbow，流体及边界参数设置如上图所示。\nQFlux的基本方程，貌似只能选不可压缩流求解器Incompressible（喂喂喂，宣传中的可压缩流求解器呢？），激活能量方程。\n湍流模型，没什么需要特别说明的。右边的湍流参数也都是可以在OpenFOAM的turbulenceProperties文件里修改的。唯一的亮点可能是湍流模型的近壁处理方面，总比OpenFOAM里一个一个地在k文件、nut文件里调试壁面函数要好。（PS：仅仅是加了一个GUI而已，同样的功能我用Qt包装一下也能实现。）\n数值方法，就是求解器、离散格式和松弛因子的选择，基本相当于OpenFOAM的controlDict、fvSchemes和fvSolution文件，只不过不用你一个个去调试了而已。貌似求解器还只能选择SIMPLE算法。\n材料库里面只有空气、水和结构钢三种材料。\n进出口边界条件，流动方向不能指定为表面法向，只能手动输入流动方向坐标。不过它的湍流参数定义方式与其他商业CFD软件比较接近，总比OpenFOAM中一个个去计算然后手动定义k文件、epsilon文件还有nut文件方便地多。貌似速度和温度的单位不能更改，所以如果用户用的是℃单位的话，那么还要自己转换温标。\n设置收敛条件，动量方程、能量方程、连续性方程及湍流粘度方程收敛判断公差均设置为1.0e-6。\n设置计算条件，计算最大步数1000步，可以设置并行核数，可以看到QFlux的Windows版下MPI库用的是Microsoft MPI库，这点和OpenFOAM的Windows版出奇地一致。计算调用的是qcore.exe程序。\n可以看到计算385步左右收敛，但当鼠标停留在收敛曲线上时，标签数字只能显示到小数点后面四位数，而且还不能用科学记数法显示标签数字。\n上图是QFlux计算结果输出的温度云图。\n上图是相同案例OpenFOAM计算结果输出的温度云图，相同的案例可以通过simpleFoam+scalarTransportFoam或者buoyantSimpleFoam实现。可以看到两者计算结果较为相似。\n4. QFlux求解器与OpenFOAM的对比 对比QFlux与OpenFOAM(v2012)的动态链接库文件，不难发现，两者在主要文件结构和命名方式上有不少相似之处。\n用\u0026quot;Dependency Walker\u0026quot;查看两者引用动态链接库文件的函数命名，可以发现在一些关键的链接库中，内部函数命名方式不敢说一模一样，只能说高度相似。\n由于OpenFOAM是开源软件，源代码网上都可以下载得到，很难说QFlux没有“借鉴”OpenFOAM源代码。至于“借鉴”了多少内容，则又很难说得清了。\n鉴于国内CFD等工业软件无比孱弱的现状，相比于自主开发，在开源平台上快速包装出易用的软件平台似乎更容易受到软件开发者和投资者的欢迎。对比OpenFOAM和QFlux两者功能实现、开发程度以及代码量的不同，OpenFOAM程序大小、代码量还有成熟度是要远远高于QFlux的。其实在OpenFOAM平台上开发相关界面、引入流程控制一直是业界研究的重点，国内外有不少公司都是围绕着OpenFOAM进行相关应用及二次开发。但是一边“借鉴”开源程序，一边宣称完全”自主开发“的，可能除了国内的互联网公司外都做不出来了，当然这其中”十沣科技“算是一个。\n其实对于OpenFOAM等开源平台来说，交流和创新是最重要的，新方法的实现远比工业化的包装以及简单易用重要的多，这也是OpenFOAM这么多年来坚持文本和命令行参数控制的原因之一。比”借鉴“本身更可怕的是麻木和惰性，当我们用惯了开源平台，只知道一味索取而不知道反哺，只知道重构别人家的代码而不知道从底层进行构建和完善，知其然而不知其所以然……当有一天失去了开源平台之后，我们还有没有办法坚持所谓的”自主开发“呢？\n","permalink":"https://andrewmoa.site/zh-cn/post/2021-09-08-tenfong/","tags":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/tags/cfd/"}],"title":"扒一扒十沣科技国产CFD软件——QFlux及QMesh"},{"categories":[{"LinkTitle":"Zhihu","RelPermalink":"/zh-cn/categories/zhihu/"}],"content":"本文采用不同CFD求解器，对层流流动问题进行验证。通过列举不同求解器的操作过程和输出结果的差异，验证各个求解器的精度。\n1. 问题描述 如下图，建立两个同心圆柱间的定常层流模型。流动由内筒以恒定角速度旋转而引起，而外筒保持静止。使用周期性边界，只需要对流域的一部分进行建模。物理模型和输入数据如下表所示。\n来源 Ansys验证算例 参考文献 F. M. White. Viscous Fluid Flow. Section 3-2.3. McGraw-Hill Book Co., Inc.. New York, NY. 1991. 物理模型 层流，旋转壁面 流体物性参数、几何尺寸和边界条件如下表所示。\n特征 单位 参数 流体密度 kg/m^3 1.0 流体粘度 kg/m-s 0.0002 内圆柱半径 mm 17.8 外圆柱半径 mm 46.28 内圆柱转速 rad/s 1.0 本文所用求解器和输入模型如下。\n求解器 输入模型 Ansys Fluent 2020R2 VMFL001_rot_conc_cyl_2D.msh Ansys CFX 2020R2 VMFL001_rot_conc_cyl_3D.msh Siemens STAR-CCM+ 2020.2.1(15.04.010-R8) VMFL001_rot_conc_cyl_2D.msh OpenFOAM v2006 VMFL001_rot_conc_cyl_2D.msh SU2 v7.1.1 VMFL001_rot_conc_cyl_2D.cgns 2. Fluent验证 2.1 求解器设置 打开Fluent，选择2D求解器。网格数量较少，只用1个处理器核心运行。\n导入网格模型VMFL001_rot_conc_cyl_2D.msh，如下所示。\n更改物理模型，选择层流。\n更改流体参数。将\u0026quot;air\u0026quot;重命名为\u0026quot;vmfl001\u0026quot;，密度1kg/m3，粘度0.0002kg/m-s。\n可以看到导入的periodic（id=10）和shadow_periodic（id=13）边界不是周期边界，需要在命令行窗口中输入/mesh/modify-zones/make-periodic命令，建立周期边界。\n\u0026gt; /mesh/modify-zones/make-periodic Periodic zone [()] 10 Shadow zone [()] 13 Rotationally periodic? (if no, translationally) [yes] yes Create periodic zones? [yes] yes 更改innerwall（id=11）边界为旋转壁面，设置如下。\n求解器选择SIMPLE，如下所示。\n收敛残差更改为1e-6，初始化。\n迭代步数800，求解并保存。\n2.2 后处理 在半径20~35mm处建立4个点，坐标如下表所示。\n名称 x (m) y (m) point-1 0.02 0 point-2 0.025 0 point-3 0.03 0 point-4 0.035 0 输出四个点的切向速度。\n新建一条水平方向的直线，输出不同半径下的切向速度分布曲线。\n将曲线点数据输出到文件中备用。\n层流方程与Fluent计算结果误差对比。\n半径 [mm] 切向速度（理论值） [m/s] 切向速度（Fluent仿真结果）[m/s] 误差 [%] 20 0.0151 0.015027723 0.48 25 0.0105 0.010464473 0.34 30 0.0072 0.0071237963 1.06 35 0.0046 0.0044973576 2.23 3. CFX验证 3.1 求解器设置 启动CFX-Pre，新建算例，导入fluent网格文件VMFL001_rot_conc_cyl_3D.msh。\n新建材料，命名为\u0026quot;vmfl001\u0026quot;。密度1 [kg m^-3]，动力粘度0.0002 [Pa s]。\n新建流体域（Domain），命名为FLUID，网格区域（Location）选择FLUID。材料选择刚刚新建的vmfl001。关闭传换热方程，湍流方程选择None(Laminar)。\n在此流体域中新建边界，所对应的网格边界如下表所示。\n流体边界 网格边界 边界类型 INNERWALL INNERWALL Wall OUTERWALL OUTERWALL Wall SYMM 1 SYMM 1 Symmetry SYMM 2 SYMM 2 Symmetry 其中，INNERWALL指定为旋转壁面，如下图所示，角速度 1 [radian s^-1]。\n新建交界面（Interfaces），命名为PERIODIC，如下图所示。\n新建全局初始化（Global Initialization），直接确认，采用默认值即可。\n更改求解器控制（Solver Control），最大迭代步数改为1000，残差类型改为MAX，收敛残差目标改为1e-6。\n保存算例，输出.def文件。打开CFS-Solver Manager载入.def文件，求解器类型选择双精度，开始求解。\n大概23步左右就收敛了。\n3.2 后处理 打开CFD-Post，载入求解完毕输出的结果文件。在半径20~35mm处建立4个点，如下表。\n名称 x (m) y (m) z (m) Point 1 0.02 0 0 Point 2 0.025 0 0 Point 3 0.03 0 0 Point 4 0.035 0 0 建立如下4个表达式（Expressions），输出4个点的切向速度。\n表达式名称 表达式定义 VelYOnPoint1 maxVal(Velocity v)@Point 1 VelYOnPoint2 maxVal(Velocity v)@Point 2 VelYOnPoint3 maxVal(Velocity v)@Point 3 VelYOnPoint4 maxVal(Velocity v)@Point 4 新建Table，将4个点的半径和速度列出。\n建立直线\u0026quot;Line 1\u0026quot;，如下图所示。\n建立曲线图（Chart），“Data Series\u0026quot;中的\u0026quot;Location\u0026quot;选择刚刚建立的\u0026quot;Line 1\u0026quot;，”X Axis\u0026quot;中的“Variable\u0026quot;选择\u0026quot;X\u0026quot;，”Y Axis\u0026quot;中的“Variable\u0026quot;选择\u0026quot;Velocity v\u0026quot;，输出曲线如下图所示。将曲线数据导出到.csv文件中备用。\n层流方程与CFX计算结果误差对比。\n半径 [mm] 切向速度（理论值） [m/s] 切向速度（CFX仿真结果）[m/s] 误差 [%] 20 0.0151 0.0150925 0.05 25 0.0105 0.0105287 0.27 30 0.0072 0.00718282 0.24 35 0.0046 0.00454726 1.15 4. STAR-CCM+验证 4.1 求解器设置 打开STAR-CCM+，由于网格数较少，进程选项可以选择串行，只用1个处理器核心进行求解。导入fluent网格文件VMFL001_rot_conc_cyl_2D.msh。\n选择求解器物理模型，如下图所示。\n将气体材料\u0026quot;Air\u0026quot;重命名为\u0026quot;vmfl001\u0026quot;，密度修改为1.0 kg/m^3，动力粘度修改为2.0E-4 Pa-s。\n展开流体区域中的\u0026quot;FLUID-7\u0026quot;，同时选中\u0026quot;PERIODIC\u0026quot;和\u0026quot;SHADOW_PERIODIC\u0026quot;边界，右键创建界面。新建的交界面拓扑改为\u0026quot;周期\u0026quot;，周期转换选择\u0026quot;旋转\u0026quot;，旋转轴指定改为\u0026quot;指定轴\u0026quot;，轴的方向为\u0026quot;[0.0, 0.0, 1.0]\u0026quot;。\n在\u0026quot;工具\u0026quot;、\u0026ldquo;运动\u0026quot;中新建旋转运动，旋转轴方向[0.0, 0.0, 1.0]，旋转速率1.0 radian/s。\n将INNERWALL边界的参考坐标系指定选项更改为\u0026quot;局部参考坐标系\u0026rdquo;，边界参考坐标系指定选择刚刚新建的旋转运动的坐标系。\n停止标准中的最大迭代步数默认为1000，运行求解并保存算例。\n4.2 后处理 在衍生零部件中新建4个探针点，如下表。\n名称 x (m) y (m) z (m) p1 0.02 0 0 p2 0.025 0 0 p3 0.03 0 0 p4 0.035 0 0 同样地，在衍生零部件中新建等值面\u0026quot;y=0\u0026quot;，如下图。\n在\u0026quot;工具\u0026quot;、\u0026ldquo;表\u0026quot;中新建\u0026quot;XYZ内部表\u0026rdquo;，标量选择\u0026quot;[Velocity[j]]\u0026quot;，零部件选择新建的p1~p4四个点，邮件提取然后制表，显示4个点的切向速度。\n在\u0026quot;绘图\u0026quot;中新建\u0026quot;XY绘图\u0026quot;，零部件选择新建的等值面\u0026quot;y=0\u0026quot;，\u0026ldquo;Y类型\u0026rdquo;、\u0026ldquo;Y Type 1\u0026quot;中的场函数选择\u0026rdquo;[Velocity[j]]\u0026quot;，曲线图如下。将曲线数据导出到.csv文件中备用。\n层流方程与STAR-CCM+计算结果误差对比。\n半径 [mm] 切向速度（理论值） [m/s] 切向速度（STAR-CCM+仿真结果）[m/s] 误差 [%] 20 0.0151 0.0153266630659337 1.50 25 0.0105 0.0106689558311781 1.61 30 0.0072 0.00671828465572254 6.69 35 0.0046 0.004432094526565 3.65 5. OpenFOAM验证 5.1 求解器设置 首先将官方自带案例中的simpleCar复制到当前目录下并重命名为vmfl001，这里用的是MSYS2作为命令行交互界面，执行以下命令。\ncp -r $FOAM_TUTORIALS/incompressible/simpleFoam/simpleCar . mv simpleCar vmfl001 cd vmfl001 将VMFL001_rot_conc_cyl_2D.msh文件复制到vmfl001文件夹中，执行以下命令，将fluent网格转换为OpenFOAM网格。\nfluentMeshToFoam VMFL001_rot_conc_cyl_2D.msh 打开constant/polyMesh/boundary文件，修改如下，将\u0026quot;PERIODIC\u0026quot;和\u0026quot;SHADOW_PERIODIC\u0026quot;边界由壁面改为周期边界。\nFoamFile { version 2.0; format ascii; class polyBoundaryMesh; location \u0026#34;constant/polyMesh\u0026#34;; object boundary; } 5 ( PERIODIC { type cyclicAMI; inGroups 1(wall); nFaces 20; startFace 2320; matchTolerance 0.0001; transform rotational; rotationAxis (0 0 1); rotationCentre (0 0 0); neighbourPatch SHADOW_PERIODIC; } INNERWALL { type wall; inGroups 1(wall); nFaces 60; startFace 2340; } OUTERWALL { type wall; inGroups 1(wall); nFaces 60; startFace 2400; } SHADOW_PERIODIC { type cyclicAMI; inGroups 1(wall); nFaces 20; startFace 2460; matchTolerance 0.0001; transform rotational; rotationAxis (0 0 1); rotationCentre (0 0 0); neighbourPatch PERIODIC; } frontAndBackPlanes { type empty; inGroups 1(empty); nFaces 2400; startFace 2480; } ) 删除vmfl001目录下的\u0026quot;Allrun\u0026quot;文件。删除vmfl001/0目录下的\u0026quot;epsilon\u0026quot;、\u0026ldquo;k\u0026quot;和\u0026quot;nut\u0026rdquo;，只保留\u0026quot;p\u0026quot;和\u0026quot;U\u0026quot;。\n修改\u0026quot;p\u0026quot;文件如下。\nFoamFile { version 2.0; format ascii; class volScalarField; object p; } dimensions [0 2 -2 0 0 0 0]; internalField uniform 0; boundaryField { \u0026#34;(INNERWALL|OUTERWALL)\u0026#34; { type zeroGradient; } frontAndBackPlanes { type empty; } \u0026#34;(PERIODIC|SHADOW_PERIODIC)\u0026#34; { type cyclicAMI; value $internalField; } } 修改\u0026quot;U\u0026quot;文件如下。\nFoamFile { version 2.0; format ascii; class volVectorField; object U; } dimensions [0 1 -1 0 0 0 0]; internalField uniform (0 0 0); boundaryField { INNERWALL { type rotatingWallVelocity; axis (0 0 1); origin (0 0 0); omega constant 1.0; value uniform (0 0 0); } OUTERWALL { type noSlip; } frontAndBackPlanes { type empty; } \u0026#34;(PERIODIC|SHADOW_PERIODIC)\u0026#34; { type cyclicAMI; value $internalField; } } 执行以下命令，检查网格边界条件设置。\ncheckMesh 修改constant目录下的\u0026quot;transportProperties\u0026quot;文件，将nu值改为0.0002。\nFoamFile { version 2.0; format ascii; class dictionary; location \u0026#34;constant\u0026#34;; object transportProperties; } transportModel Newtonian; nu 0.0002; 修改constant目录下的\u0026quot;turbulenceProperties\u0026quot;文件，采用层流模型。\nFoamFile { version 2.0; format ascii; class dictionary; object turbulenceProperties; } simulationType laminar; system目录下，只保留\u0026quot;controlDict\u0026quot;、\u0026ldquo;fvSchemes\u0026quot;和\u0026quot;fvSolution\u0026quot;三个文件，其他的全部删除。\n修改\u0026quot;controlDict\u0026quot;文件如下，求解程序simpleFoam，从保存的最后一步开始求解，迭代步数1000步，每隔100步写入求解结果。\nFoamFile { version 2.0; format ascii; class dictionary; location \u0026#34;system\u0026#34;; object controlDict; } application simpleFoam; startFrom latestTime; startTime 0; stopAt endTime; endTime 1000; deltaT 1; writeControl timeStep; writeInterval 100; purgeWrite 1; writeFormat ascii; writePrecision 8; writeCompression off; timeFormat general; timePrecision 8; runTimeModifiable true; 修改\u0026quot;fvSolution\u0026quot;文件如下。\nFoamFile { version 2.0; format ascii; class dictionary; location \u0026#34;system\u0026#34;; object fvSolution; } solvers { p { solver GAMG; smoother GaussSeidel; tolerance 1e-08; relTol 0.01; } U { solver smoothSolver; smoother symGaussSeidel; tolerance 1e-08; relTol 0.01; } } SIMPLE { nNonOrthogonalCorrectors 0; residualControl { p 1e-8; U 1e-8; } pRefCell 0; pRefValue 0; } relaxationFactors { fields { p 0.3; } equations { U 0.7; } } 回到vmfl001目录，执行以下命令（Windows下若用到tee命令，可以在Powershell里执行），运行求解并记录求解过程输出。\nsimpleFoam | tee simpleFoam.log 5.2 后处理 Linux下可以直接使用paraFoam启动ParaView进行后处理。在Windows下执行以下命令，在vmfl001目录中生成.foam文件，用ParaView打开.foam文件方能载入OpenFOAM的求解结果。\necho . \u0026gt; vmfl001.foam 以vmfl001.foam为基础，新建4个\u0026quot;Probe Location\u0026rdquo;，用于提取20~35mm处的切向速度。\n名称 Center Radius ProbeLocation1 (0.020,0,0) 0 ProbeLocation2 (0.025,0,0) 0 ProbeLocation3 (0.030,0,0) 0 ProbeLocation4 (0.035,0,0) 0 新建GroupDatasets，包含上述4个ProbeLocation。\n选中该GroupDatasets，可在右侧\u0026quot;SpreadSheetView\u0026quot;窗口中看到4个点位置的切向速度值。\n以vmfl001.foam为基础，建立\u0026quot;PlotOnIntersectionCurves\u0026quot;绘图。\n平面方向选择Y轴正向，如图所示。\nX轴显示变量选择\u0026quot;Points_X\u0026quot;，Y轴显示变量选择\u0026quot;U_Y\u0026quot;，如图所示。\n右侧\u0026quot;LineChartView\u0026quot;窗口可以查看生成的曲线。\n选择\u0026quot;File\u0026quot;、\u0026ldquo;Export Scene\u0026rdquo;，可以将曲线数据导出到csv文件中。\n层流方程与OpenFOAM计算结果误差对比。\n半径 [mm] 切向速度（理论值） [m/s] 切向速度（OpenFOAM仿真结果）[m/s] 误差 [%] 20 0.0151 0.0150 0.66 25 0.0105 0.0104366 0.60 30 0.0072 0.00712633 1.02 35 0.0046 0.00452779 1.57 6. SU2验证 6.1 求解器设置 SU2求解器配置文件vmfl001.cfg内容如下：\n% ------------- DIRECT, ADJOINT, AND LINEARIZED PROBLEM DEFINITION ------------% % % Physical governing equations (EULER, NAVIER_STOKES, % WAVE_EQUATION, HEAT_EQUATION, FEM_ELASTICITY, % POISSON_EQUATION) SOLVER= INC_NAVIER_STOKES % % Specify turbulence model (NONE, SA) KIND_TURB_MODEL= NONE % % Mathematical problem (DIRECT, CONTINUOUS_ADJOINT) MATH_PROBLEM= DIRECT % % Restart solution (NO, YES) RESTART_SOL= NO % ---------------- INCOMPRESSIBLE FLOW CONDITION DEFINITION -------------------% % % Density model within the incompressible flow solver. % Options are CONSTANT (default), BOUSSINESQ, or VARIABLE. If VARIABLE, % an appropriate fluid model must be selected. INC_DENSITY_MODEL= CONSTANT % % Solve the energy equation in the incompressible flow solver INC_ENERGY_EQUATION = YES % % Initial density for incompressible flows (1.2886 kg/m^3 by default) INC_DENSITY_INIT= 1.0 % % Initial temperature for incompressible flows that include the % energy equation (288.15 K by default). Value is ignored if % INC_ENERGY_EQUATION is false. INC_TEMPERATURE_INIT= 293.15 % % Initial velocity for incompressible flows (1.0,0,0 m/s by default) INC_VELOCITY_INIT= ( 1.0, 0.0, 0.0 ) % % ---- IDEAL GAS, POLYTROPIC, VAN DER WAALS AND PENG ROBINSON CONSTANTS -------% % % Fluid model (STANDARD_AIR, IDEAL_GAS, VW_GAS, PR_GAS, % CONSTANT_DENSITY, INC_IDEAL_GAS) FLUID_MODEL= CONSTANT_DENSITY % % Specific heat at constant pressure, Cp (1004.703 J/kg*K (air)). % Incompressible fluids with energy eqn. only (CONSTANT_DENSITY, INC_IDEAL_GAS). SPECIFIC_HEAT_CP= 1004.703 % --------------------------- VISCOSITY MODEL ---------------------------------% % % Viscosity model (SUTHERLAND, CONSTANT_VISCOSITY). VISCOSITY_MODEL= CONSTANT_VISCOSITY % % Molecular Viscosity that would be constant (1.716E-5 by default) MU_CONSTANT= 0.0002 % --------------------------- THERMAL CONDUCTIVITY MODEL ----------------------% % % Conductivity model (CONSTANT_CONDUCTIVITY, CONSTANT_PRANDTL). CONDUCTIVITY_MODEL= CONSTANT_PRANDTL % % Laminar Prandtl number (0.72 (air), only for CONSTANT_PRANDTL) PRANDTL_LAM= 0.72 % % Turbulent Prandtl number (0.9 (air), only for CONSTANT_PRANDTL) PRANDTL_TURB= 0.90 % ---------------------- REFERENCE VALUE DEFINITION ---------------------------% % % Reference origin for moment computation REF_ORIGIN_MOMENT_X = 0.25 REF_ORIGIN_MOMENT_Y = 0.00 REF_ORIGIN_MOMENT_Z = 0.00 % % Reference length for pitching, rolling, and yawing non-dimensional moment REF_LENGTH= 1.0 % % Reference area for force coefficients (0 implies automatic calculation) REF_AREA= 1.0 % ----------------------- DYNAMIC MESH DEFINITION -----------------------------% % % Type of dynamic surface movement (NONE, DEFORMING, % MOVING_WALL, FLUID_STRUCTURE, FLUID_STRUCTURE_STATIC, % AEROELASTIC, EXTERNAL, EXTERNAL_ROTATION, % AEROELASTIC_RIGID_MOTION) SURFACE_MOVEMENT= MOVING_WALL % % Moving wall boundary marker(s) (NONE = no marker, ignored for RIGID_MOTION) MARKER_MOVING= ( INNERWALL ) % % Coordinates of the motion origin SURFACE_MOTION_ORIGIN= 0.0 0.0 0.0 % % Angular velocity vector (rad/s) about the motion origin SURFACE_ROTATION_RATE = 0.0 0.0 1.0 % % -------------------- BOUNDARY CONDITION DEFINITION --------------------------% % % Navier-Stokes (no-slip), constant heat flux wall marker(s) (NONE = no marker) % Format: ( marker name, constant heat flux (J/m^2), ... ) MARKER_HEATFLUX= ( OUTERWALL, 0.0 ) % % Navier-Stokes (no-slip), isothermal wall marker(s) (NONE = no marker) % Format: ( marker name, constant wall temperature (K), ... ) MARKER_ISOTHERMAL= ( INNERWALL, 313.15 ) % % Periodic boundary marker(s) (NONE = no marker) % Format: ( periodic marker, donor marker, rotation_center_x, rotation_center_y, % rotation_center_z, rotation_angle_x-axis, rotation_angle_y-axis, % rotation_angle_z-axis, translation_x, translation_y, translation_z, ... ) % MARKER_PERIODIC= ( PERIODIC, SHADOW_PERIODIC, 0, 0, 0, 0, 0, 180, 0, 0, 0 ) % % Marker(s) of the surface to be plotted or designed MARKER_PLOTTING= ( INNERWALL, OUTERWALL ) % % Marker(s) of the surface where the functional (Cd, Cl, etc.) will be evaluated MARKER_MONITORING= ( NONE ) % ------------- COMMON PARAMETERS DEFINING THE NUMERICAL METHOD ---------------% % % Numerical method for spatial gradients (GREEN_GAUSS, WEIGHTED_LEAST_SQUARES) NUM_METHOD_GRAD= WEIGHTED_LEAST_SQUARES % % Courant-Friedrichs-Lewy condition of the finest grid CFL_NUMBER= 15.0 % % Adaptive CFL number (NO, YES) CFL_ADAPT= YES % % Parameters of the adaptive CFL number (factor down, factor up, CFL min value, % CFL max value ) CFL_ADAPT_PARAM= ( 0.1, 2.0, 1.0, 1e10 ) % % Number of total iterations ITER= 10000 % ------------------------ LINEAR SOLVER DEFINITION ---------------------------% % % Linear solver for the implicit (or discrete adjoint) formulation (BCGSTAB, FGMRES) LINEAR_SOLVER= FGMRES % % Preconditioner of the Krylov linear solver (JACOBI, LINELET, LU_SGS) LINEAR_SOLVER_PREC= ILU % % Linael solver ILU preconditioner fill-in level (0 by default) LINEAR_SOLVER_ILU_FILL_IN= 0 % % Min error of the linear solver for the implicit formulation LINEAR_SOLVER_ERROR= 1E-14 % % Max number of iterations of the linear solver for the implicit formulation LINEAR_SOLVER_ITER= 25 % -------------------------- MULTIGRID PARAMETERS -----------------------------% % % Multi-Grid Levels (0 = no multi-grid) MGLEVEL= 0 % % Multi-grid cycle (V_CYCLE, W_CYCLE, FULLMG_CYCLE) MGCYCLE= W_CYCLE % % Multi-grid pre-smoothing level MG_PRE_SMOOTH= ( 1, 2, 3, 3 ) % % Multi-grid post-smoothing level MG_POST_SMOOTH= ( 0, 0, 0, 0 ) % % Jacobi implicit smoothing of the correction MG_CORRECTION_SMOOTH= ( 0, 0, 0, 0 ) % % Damping factor for the residual restriction MG_DAMP_RESTRICTION= 0.8 % % Damping factor for the correction prolongation MG_DAMP_PROLONGATION= 0.8 % -------------------- FLOW NUMERICAL METHOD DEFINITION -----------------------% % % Convective numerical method (JST, LAX-FRIEDRICH, CUSP, ROE, AUSM, HLLC, % TURKEL_PREC, MSW) CONV_NUM_METHOD_FLOW= FDS % % Monotonic Upwind Scheme for Conservation Laws (TVD) in the flow equations. % Required for 2nd order upwind schemes (NO, YES) MUSCL_FLOW= YES % % Slope limiter (NONE, VENKATAKRISHNAN, VENKATAKRISHNAN_WANG, % BARTH_JESPERSEN, VAN_ALBADA_EDGE) SLOPE_LIMITER_FLOW= NONE % % Coefficient for the Venkat\u0026#39;s limiter (upwind scheme). A larger values decrease % the extent of limiting, values approaching zero cause % lower-order approximation to the solution (0.05 by default) VENKAT_LIMITER_COEFF= 0.05 % % Time discretization (RUNGE-KUTTA_EXPLICIT, EULER_IMPLICIT, EULER_EXPLICIT) TIME_DISCRE_FLOW= EULER_IMPLICIT % --------------------------- CONVERGENCE PARAMETERS --------------------------% % % Convergence criteria (CAUCHY, RESIDUAL) CONV_CRITERIA= RESIDUAL % % Min value of the residual (log10 of the residual) CONV_RESIDUAL_MINVAL= -14 % % Start convergence criteria at iteration number CONV_STARTITER= 10 % % Number of elements to apply the criteria CONV_CAUCHY_ELEMS= 100 % % Epsilon to control the series convergence CONV_CAUCHY_EPS= 1E-14 % ------------------------- INPUT/OUTPUT INFORMATION --------------------------% % % Mesh input file MESH_FILENAME= VMFL001_rot_conc_cyl_2D.cgns % % Mesh input file format (SU2, CGNS, NETCDF_ASCII) MESH_FORMAT= CGNS % % Mesh output file MESH_OUT_FILENAME= mesh_out.cgns % % Restart flow input file SOLUTION_FILENAME= solution_flow.dat % % Restart adjoint input file SOLUTION_ADJ_FILENAME= solution_adj.dat % % Output file format (PARAVIEW, TECPLOT, STL) TABULAR_FORMAT= CSV % % Output file convergence history (w/o extension) CONV_FILENAME= history % % Output file restart flow RESTART_FILENAME= restart_flow.dat % % Output file restart adjoint RESTART_ADJ_FILENAME= restart_adj.dat % % Output file flow (w/o extension) variables VOLUME_FILENAME= flow % % Output file adjoint (w/o extension) variables VOLUME_ADJ_FILENAME= adjoint % % Output objective function gradient (using continuous adjoint) GRAD_OBJFUNC_FILENAME= of_grad.dat % % Output file surface flow coefficient (w/o extension) SURFACE_FILENAME= surface_flow % % Output file surface adjoint coefficient (w/o extension) SURFACE_ADJ_FILENAME= surface_adjoint % % Writing solution file frequency OUTPUT_WRT_FREQ= 250 % % Writing convergence history frequency SCREEN_WRT_FREQ_INNER= 1 % % Screen output SCREEN_OUTPUT= (INNER_ITER, WALL_TIME, RMS_PRESSURE, RMS_VELOCITY-X, RMS_VELOCITY-Y) 确保vmfl001.cfg和网格文件VMFL001_rot_conc_cyl_2D.cgns在同一目录，执行以下命令（Windows下若用到tee命令，可以在Powershell里执行），运行求解并记录求解过程输出。\nSU2_CFD vmfl001.cfg | tee vmfl001.log 6.2 后处理 用ParaView打开求解输出的flow.vtu文件，处理过程同5.2。\n层流方程与SU2计算结果误差对比。\n半径 [mm] 切向速度（理论值） [m/s] 切向速度（SU2仿真结果）[m/s] 误差 [%] 20 0.0151 0.0150846 0.10 25 0.0105 0.0105093 0.09 30 0.0072 0.00716493 0.49 35 0.0046 0.00453409 1.43 7. 总结 横向对比来看，CFX求解精度最高；其次是SU2；然后是Fluent；OpenFOAM精度和Fluent相差无几，但求解速度更快；STAR-CCM+精度最差。\n半径 [mm] Fluent求解误差 [%] CFX求解误差 [%] STAR-CCM+求解误差 [%] OpenFOAM求解误差 [%] SU2求解误差 [%] 20 0.48 0.05 1.50 0.66 0.10 25 0.34 0.27 1.61 0.60 0.09 30 1.06 0.24 6.69 1.02 0.49 35 2.23 1.15 3.65 1.57 1.43 ","permalink":"https://andrewmoa.site/zh-cn/post/2021-08-14-solver-verification-comparison/","tags":[{"LinkTitle":"Cfd","RelPermalink":"/zh-cn/tags/cfd/"},{"LinkTitle":"Cfx","RelPermalink":"/zh-cn/tags/cfx/"},{"LinkTitle":"Fluent","RelPermalink":"/zh-cn/tags/fluent/"},{"LinkTitle":"Openfoam","RelPermalink":"/zh-cn/tags/openfoam/"},{"LinkTitle":"Star-Ccm+","RelPermalink":"/zh-cn/tags/star-ccm+/"},{"LinkTitle":"Su2","RelPermalink":"/zh-cn/tags/su2/"}],"title":"求解器验证对比——旋转和静止同心圆柱之间的层流"},{"categories":[{"LinkTitle":"Zhihu","RelPermalink":"/zh-cn/categories/zhihu/"}],"content":"1. 源代码下载 1.1 下载OpenFOAM源代码 在${HOME}目录下新建OpenFOAM目录：\ncd ${HOME} mkdir OpenFOAM \u0026amp;\u0026amp; cd OpenFOAM 从github下载OpenFOAM和ThirdParty的源代码，放到${HOME}/OpenFOAM目录中：\ngit clone https://github.com/OpenFOAM/OpenFOAM-dev --depth=1 git clone https://github.com/OpenFOAM/ThirdParty-dev --depth=1 1.2 下载Torque(PBS)源码包 本文用的是AUR中的Torque，CentOS、Debian和SUSE系的操作系统可以从Github上的OpenPBS下载现成的二进制包。采用OpenPBS或PBS Pro请跳过本文第3节，参照其他文档进行PBS配置。\n从AUR上下载Torque(PBS)源文件：\ngit clone https://aur.archlinux.org/torque.git cd torque wget http://wpfilebase.s3.amazonaws.com/torque/torque-6.1.1.1.tar.gz 2. 编译安装OpenFOAM 2.1 设置环境变量 编辑${HOME}/.bashrc文件，添加以下两行：\n#OpenFOAM source ${HOME}/OpenFOAM/OpenFOAM-dev/etc/bashrc WM_MPLIB=OPENMPI 末尾的WM_MPLIB=OPENMPI表示使用重新编译的OpenMPI库。\n更新环境变量：\nsource ${HOME}/.bashrc 验证环境变量是否正确：\necho $WM_PROJECT_DIR echo $WM_THIRD_PARTY_DIR 可以正确输出OpenFOAM编译目录，则表示环境变量设置正确。\n2.2 编译第三方库 进入ThirdParty-dev目录，编译第三方库：\ncd $WM_THIRD_PARTY_DIR wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.1.tar.gz tar -xvzf openmpi-2.1.1.tar.gz ./Allwmake 由于前面指定了WM_MPLIB=OPENMPI，此处需要手动下载OpenMPI源码文件，使用wget下载OpenMPI源码包并解压。Allwmake后面可以增加-jN选项开启多核并行编译，这里的N应替换成并行数，Allwmake会自动编译OpenMPI。\nwhich mpirun \u0026gt;${WM_THIRD_PARTY_DIR}/platforms/linux64Gcc/openmpi-2.1.1/bin/mpirun which mpicc \u0026gt;${WM_THIRD_PARTY_DIR}/platforms/linux64Gcc/openmpi-2.1.1/bin/mpicc 2.3 编译ParaView 继续在ThirdParty-dev目录里编译ParaView：\ncd $WM_THIRD_PARTY_DIR ./makeParaView -mpi wmRefresh 编译完成后运行wmRefresh刷新环境变量。\n2.4 编译OpenFOAM 切换到OpenFOAM-dev目录编译OpenFOAM：\ncd $WM_PROJECT_DIR ./Allwmake -jN OpenFOAM编译时间较长，建议开启多核心并行编译，这里的N应替换成核心数。\n2.5 测试 执行以下命令从OpenFOAM自带的实例文件中拷贝cavity文件夹到当前路径下：\nmkdir $FOAM_RUN \u0026amp;\u0026amp; cd $FOAM_RUN cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily . cd pitzDaily 生成网格：\nblockMesh 执行计算：\nsimpleFoam | tee simpleFoam.log 启动ParaView进行后处理：\nparaFoam 2.6 更新 使用git命令更新OpenFOAM-dev及ThirdParty-dev目录中的源代码文件，然后重新编译安装：\ngit pull wcleanPlatform ./Allwmake -jN 3. 编译安装Torque(PBS) 3.1 安装Torque 进入torque目录：\ncd ${HOME}/OpenFOAM/torque 编译Torque：\nmakepkg 安装Torque：\nsudo pacman -U torque-6.1.1.1-2-x86_64.pkg.tar.zst 启动服务（pbs_mom.service和pbs_sched.service在源代码的src/torque-6.1.1.1/contrib/systemd目录中，需要手动复制过来）：\nsudo systemctl enable pbs_server sudo systemctl enable trqauthd sudo systemctl start pbs_server sudo systemctl start trqauthd sudo cp pbs_mom.service /usr/lib/systemd/system sudo systemctl enable pbs_mom sudo systemctl start pbs_mom sudo cp pbs_sched.service /usr/lib/systemd/system sudo systemctl enable pbs_sched sudo systemctl start pbs_sched 3.2 服务器配置 参考archlinux wiki ，完成Torque配置。\n编辑/etc/hosts文件（示例），添加服务器节点和计算节点IP地址：\n192.168.100.101 master #192.168.100.102 cluster01 #192.168.100.103 cluster02 更改/var/spool/torque/server_name文件中的主机名为服务器节点主机名：\nmaster 执行以下命令，选择Y新建服务器端配置文件（只运行一次，提示会覆盖现有配置文件）：\nsudo pbs_server -t create 执行以下命令，运行服务器端守护进程：\nsudo trqauthd 初始化默认设置：\nsudo qmgr -c \u0026#34;set server acl_hosts = master\u0026#34; sudo qmgr -c \u0026#34;set server scheduling=true\u0026#34; sudo qmgr -c \u0026#34;create queue batch queue_type=execution\u0026#34; sudo qmgr -c \u0026#34;set queue batch started=true\u0026#34; sudo qmgr -c \u0026#34;set queue batch enabled=true\u0026#34; sudo qmgr -c \u0026#34;set queue batch resources_default.nodes=1\u0026#34; sudo qmgr -c \u0026#34;set queue batch resources_default.walltime=3600\u0026#34; sudo qmgr -c \u0026#34;set server default_queue=batch\u0026#34; sudo qmgr -c \u0026#34;set server keep_completed = 60\u0026#34; 验证设置：\nqmgr -c \u0026#39;p s\u0026#39; 编辑/var/spool/torque/server_priv/nodes文件，添加计算节点（服务器也可以是计算节点）。格式为HOSTNAME np=x gpus=y：\nmaster np=8 gpus=1 3.3 计算节点设置 编辑/var/spool/torque/mom_priv/config文件，添加以下信息：\npbsserver master # 服务器主机名，与nodes一致 logevent 255 # 日志记录事件数 生成并注册密钥文件，确保主机间ssh访问通畅：\ncd $HOME/.ssh ssh-keygen -t rsa cp id_rsa.pub authorized_keys 3.4 重启服务端进程 在服务器端执行以下命令，重启服务端进程。\nsudo killall -s 9 pbs_server sudo pbs_server 3.5 启动计算节点进程 执行以下命令，启动计算节点进程\nsudo pbs_mom 执行以下命令，显示计算节点状态：\npbsnodes -a 输出以下信息（示例），表示配置成功：\nmaster state = free power_state = Running np = 4 ntype = cluster mom_service_port = 15002 mom_manager_port = 15003 gpus = 1 若节点state显示为down，表示该节点不可用。可以用以下命令强制使节点释放：\nsudo qmgr -a -c \u0026#39;set node master state=free\u0026#39; 我们可以编写systemd服务脚本，自动释放节点。脚本内容如下，另存为/usr/lib/systemd/system/pbs_autofree.service文件：\n[Unit] Description=Auto free pbs_server Requires=network.target After=network.target trqauthd.service pbs_server.service pbs_mom.service StartLimitIntervalSec=0 [Service] Type=simple Restart=always RestartSec=30 User=root ExecStart=qmgr -a -c \u0026#39;set node master state=free\u0026#39; [Install] WantedBy=multi-user.target 启动服务：\nsudo systemctl enable pbs_autofree sudo systemctl start pbs_autofree 关闭服务以节省资源：\nsudo systemctl disable pbs_autofree sudo systemctl stop pbs_autofree 4. OpenFOAM并行化提交作业 4.1 准备算例 执行以下命令从OpenFOAM自带的实例文件中拷贝cavity文件夹到当前路径下：\ncd $FOAM_RUN mkdir cluster \u0026amp;\u0026amp; cd cluster cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily . cd pitzDaily 在system目录下新建decomposeParDict文件，用于网格分区，为并行化作准备。文件内容如下，网格分区数为2：\nFoamFile { version 2.0; format ascii; class dictionary; note \u0026#34;mesh decomposition control dictionary\u0026#34;; object decomposeParDict; } numberOfSubdomains 2; method scotch; 4.2 提交作业 新建脚本文件，命名为pitzDaily.pbs，用于提交并行作业。文件内容如下，使用1个节点、每节点2个CPU核心进行计算（nodes=1:ppn=2），线程数2（mpirun -np 2），这些与decomposeParDict文件中网格分区数目要保持一致：\n#!/bin/bash #PBS -l nodes=1:ppn=2 # #PBS -N pitzDaily #PBS -A OpenFOAM #PBS -o pitzDaily.out #PBS -e pitzDaily.err # source ${HOME}/OpenFOAM/OpenFOAM-dev/etc/bashrc WM_MPLIB=OPENMPI export RUN_DIR=${FOAM_RUN}/cluster/pitzDaily cd ${RUN_DIR} blockMesh decomposePar mpirun -np 2 simpleFoam -parallel | tee simpleFoam.log reconstructPar tar -Jcvf pitzDaily_results.tar.xz * 使用qsub提交作业：\nchmod 755 pitzDaily.pbs qsub pitzDaily.pbs 使用qstat检查作业状态：\nqstat -a 计算结束后，下载pitzDaily_results.tar.xz文件并解压缩，启动ParaView进行后处理：\nparaFoam ","permalink":"https://andrewmoa.site/zh-cn/post/2021-08-12-archlinux-openfoam-pbs/","tags":[{"LinkTitle":"Linux","RelPermalink":"/zh-cn/tags/linux/"},{"LinkTitle":"Pbs","RelPermalink":"/zh-cn/tags/pbs/"}],"title":"ArchLinux下OpenFOAM编译安装与PBS并行化"}]